{"meta":{"title":"有间博客","subtitle":"","description":"","author":"xxnjdg","url":"https://xxnjdg.github.io","root":"/"},"pages":[{"title":"tags","date":"2020-12-26T07:41:28.000Z","updated":"2020-12-26T07:41:41.998Z","comments":true,"path":"tags/index.html","permalink":"https://xxnjdg.github.io/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2020-12-26T07:40:34.000Z","updated":"2020-12-26T07:41:10.354Z","comments":true,"path":"categories/index.html","permalink":"https://xxnjdg.github.io/categories/index.html","excerpt":"","text":""},{"title":"search","date":"2020-12-26T07:43:50.000Z","updated":"2020-12-26T07:44:06.613Z","comments":true,"path":"search/index.html","permalink":"https://xxnjdg.github.io/search/index.html","excerpt":"","text":""}],"posts":[{"title":"elasticsearch-3","slug":"elasticsearch-3-1","date":"2021-06-20T14:34:24.000Z","updated":"2021-06-25T05:12:20.410Z","comments":true,"path":"2021/06/20/elasticsearch-3-1/","link":"","permalink":"https://xxnjdg.github.io/2021/06/20/elasticsearch-3-1/","excerpt":"","text":"timeout = {TimeValue@8576} “30s”masterNodeTimeout = {TimeValue@8463} “1m” 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172public class A&#123; public static void main(String[] args) &#123; return channel -&gt; client.index(indexRequest, new RestStatusToXContentListener&lt;&gt;(channel, r -&gt; r.getLocation(indexRequest.routing()))); &#125; ActionListener&lt;BulkResponse&gt; wrapBulkResponse(ActionListener&lt;Response&gt; listener) &#123; return ActionListener.wrap(bulkItemResponses -&gt; &#123; assert bulkItemResponses.getItems().length == 1 : &quot;expected only one item in bulk request&quot;; BulkItemResponse bulkItemResponse = bulkItemResponses.getItems()[0]; if (bulkItemResponse.isFailed() == false) &#123; final DocWriteResponse response = bulkItemResponse.getResponse(); listener.onResponse((Response) response); &#125; else &#123; listener.onFailure(bulkItemResponse.getFailure().getCause()); &#125; &#125;, listener::onFailure); &#125; void k()&#123; createIndex(index, bulkRequest.timeout(), new ActionListener&lt;CreateIndexResponse&gt;() &#123; @Override public void onResponse(CreateIndexResponse result) &#123; if (counter.decrementAndGet() == 0) &#123; executeBulk(task, bulkRequest, startTime, listener, responses, indicesThatCannotBeCreated); &#125; &#125; @Override public void onFailure(Exception e) &#123; if (!(ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException)) &#123; // fail all requests involving this index, if create didn&#x27;t work for (int i = 0; i &lt; bulkRequest.requests.size(); i++) &#123; DocWriteRequest request = bulkRequest.requests.get(i); if (request != null &amp;&amp; setResponseFailureIfIndexMatches(responses, i, request, index, e)) &#123; bulkRequest.requests.set(i, null); &#125; &#125; &#125; if (counter.decrementAndGet() == 0) &#123; executeBulk(task, bulkRequest, startTime, ActionListener.wrap(listener::onResponse, inner -&gt; &#123; inner.addSuppressed(e); listener.onFailure(inner); &#125;), responses, indicesThatCannotBeCreated); &#125; &#125; &#125;); &#125; ActionListener&lt;Response&gt; delegate = new ActionListener&lt;Response&gt;() &#123; @Override public void onResponse(Response response) &#123; listener.onResponse(response); &#125; @Override public void onFailure(Exception t) &#123; if (t instanceof Discovery.FailedToCommitClusterStateException || (t instanceof NotMasterException)) &#123; logger.debug((org.apache.logging.log4j.util.Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage(&quot;master could not publish cluster state or stepped down before publishing action [&#123;&#125;], scheduling a retry&quot;, actionName), t); retry(t, masterChangePredicate); &#125; else &#123; listener.onFailure(t); &#125; &#125; &#125;; //开启线程 void a()&#123; createIndexService.createIndex(updateRequest, ActionListener.wrap(response -&gt; listener.onResponse(new CreateIndexResponse(response.isAcknowledged(), response.isShardsAcked(), indexName)), listener::onFailure)); listenerDJR = onlyCreateIndex(request, ActionListener.wrap(response -&gt; &#123; if (response.isAcknowledged()) &#123; activeShardsObserver.waitForActiveShards(new String[]&#123;request.index()&#125;, request.waitForActiveShards(), request.ackTimeout(), shardsAcked -&gt; &#123; if (shardsAcked == false) &#123; logger.debug(&quot;[&#123;&#125;] index created, but the operation timed out while waiting for &quot; + &quot;enough shards to be started.&quot;, request.index()); &#125; listener.onResponse(new CreateIndexClusterStateUpdateResponse(response.isAcknowledged(), shardsAcked)); &#125;, listener::onFailure); &#125; else &#123; listener.onResponse(new CreateIndexClusterStateUpdateResponse(false, false)); &#125; &#125;, listener::onFailure)); clusterService.submitStateUpdateTask(&quot;create-index [&quot; + request.index() + &quot;], cause [&quot; + request.cause() + &quot;]&quot;, new IndexCreationTask(logger, allocationService, request, listenerDJR, indicesService, aliasValidator, xContentRegistry, settings, this::validate)); &#125; class UpdateTask extends BatchedTask &#123; //SafeAckedClusterStateTaskListener.listener = IndexCreationTask final ClusterStateTaskListener listener; UpdateTask(Priority priority, String source, Object task, ClusterStateTaskListener listener, ClusterStateTaskExecutor&lt;?&gt; executor) &#123; super(priority, source, executor, task); this.listener = listener; &#125; @Override public String describeTasks(List&lt;? extends BatchedTask&gt; tasks) &#123; return ((ClusterStateTaskExecutor&lt;Object&gt;) batchingKey).describeTasks( tasks.stream().map(BatchedTask::getTask).collect(Collectors.toList())); &#125; &#125; //线程池 //ackListener = DelegetingAckListener private static class DelegetingAckListener implements Discovery.AckListener &#123; ////ackedListener = (SafeAckedClusterStateTaskListener.listener = IndexCreationTask) //ackListeners.add(new AckCountDownListener(ackedListener, newClusterState.version(), newClusterState.nodes(),threadPool)); private final List&lt;Discovery.AckListener&gt; listeners; private DelegetingAckListener(List&lt;Discovery.AckListener&gt; listeners) &#123; this.listeners = listeners; &#125; @Override public void onNodeAck(DiscoveryNode node, @Nullable Exception e) &#123; for (Discovery.AckListener listener : listeners) &#123; listener.onNodeAck(node, e); &#125; &#125; @Override public void onTimeout() &#123; throw new UnsupportedOperationException(&quot;no timeout delegation&quot;); &#125; &#125; void aa()&#123; Supplier&lt;ThreadContext.StoredContext&gt; storedContextSupplier = threadPool.getThreadContext().newRestorableContext(true); TransportResponseHandler&lt;T&gt; responseHandler = new ContextRestoreResponseHandler&lt;&gt;(storedContextSupplier, handler); clientHandlers.put(requestId, new RequestHolder&lt;&gt;(responseHandler, connection, action, timeoutHandler)); FailedToCommitClusterStateException sendingController.getPublishResponseHandler().onResponse(node);sendingController.getPublishResponseHandler().onResponse(node); &#125; void aaaa()&#123; ShardStartedClusterStateTaskExecutor; clusterService.submitStateUpdateTask( &quot;shard-started &quot; + request, request, ClusterStateTaskConfig.build(Priority.URGENT), shardStartedClusterStateTaskExecutor, shardStartedClusterStateTaskExecutor); request = new StartRecoveryRequest( recoveryTarget.shardId(), recoveryTarget.indexShard().routingEntry().allocationId().getId(), recoveryTarget.sourceNode(), clusterService.localNode(), Store.MetadataSnapshot.EMPTY, recoveryTarget.state().getPrimary() = false, recoveryTarget.recoveryId(), SequenceNumbers.UNASSIGNED_SEQ_NO); &#125;&#125; return new Checkpoint(0, 0, 1, minSeqNo, maxSeqNo, -2, 1);return new Checkpoint(offset, 0, 1, minSeqNo, maxSeqNo, -2, 1); fields.add(new Field(NAME, id, fieldType)); public static SequenceIDFields emptySeqID() {return new SequenceIDFields(new LongPoint(NAME, SequenceNumbers.UNASSIGNED_SEQ_NO),new NumericDocValuesField(NAME, SequenceNumbers.UNASSIGNED_SEQ_NO),new NumericDocValuesField(PRIMARY_TERM_NAME, 0));context.seqID(seqID);fields.add(seqID.seqNo);fields.add(seqID.seqNoDocValue);fields.add(seqID.primaryTerm); fields.add(new StoredField(fieldType().name(), ref.bytes, ref.offset, ref.length)); final Field version = new NumericDocValuesField(NAME, -1L); builder = new TextFieldMapper.Builder(currentFieldName).addMultiField(new KeywordFieldMapper.Builder(“keyword”).ignoreAbove(256));","categories":[{"name":"spring cloud alibaba","slug":"spring-cloud-alibaba","permalink":"https://xxnjdg.github.io/categories/spring-cloud-alibaba/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://xxnjdg.github.io/tags/elasticsearch/"}]},{"title":"elasticsearch-3","slug":"elasticsearch-3","date":"2021-05-07T05:20:36.000Z","updated":"2021-06-25T10:48:26.275Z","comments":true,"path":"2021/05/07/elasticsearch-3/","link":"","permalink":"https://xxnjdg.github.io/2021/05/07/elasticsearch-3/","excerpt":"","text":"org.elasticsearch.action.bulk.TransportBulkAction#createIndex12345678910public class a&#123;//创建索引 void createIndex(String index, TimeValue timeout, ActionListener&lt;CreateIndexResponse&gt; listener) &#123; CreateIndexRequest createIndexRequest = new CreateIndexRequest(); createIndexRequest.index(index); createIndexRequest.cause(&quot;auto(bulk api)&quot;); createIndexRequest.masterNodeTimeout(timeout); createIndexAction.execute(createIndexRequest, listener); &#125;&#125; 索引创建请求发送到主节点处理 org.elasticsearch.action.admin.indices.create.TransportCreateIndexAction#masterOperation1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class TransportCreateIndexAction extends TransportMasterNodeAction&lt;CreateIndexRequest, CreateIndexResponse&gt; &#123; @Override protected void masterOperation(final CreateIndexRequest request, final ClusterState state, final ActionListener&lt;CreateIndexResponse&gt; listener) &#123; String cause = request.cause(); if (cause.length() == 0) &#123; cause = &quot;api&quot;; &#125; //索引名 final String indexName = indexNameExpressionResolver.resolveDateMathExpression(request.index()); final CreateIndexClusterStateUpdateRequest updateRequest = new CreateIndexClusterStateUpdateRequest(request, cause, indexName, request.index(), request.updateAllTypes()) .ackTimeout(request.timeout()).masterNodeTimeout(request.masterNodeTimeout()) .settings(request.settings()).mappings(request.mappings()) .aliases(request.aliases()).customs(request.customs()) .waitForActiveShards(request.waitForActiveShards()); createIndexService.createIndex(updateRequest, ActionListener.wrap(response -&gt; listener.onResponse(new CreateIndexResponse(response.isAcknowledged(), response.isShardsAcked(), indexName)), listener::onFailure)); &#125;&#125;public class MetaDataCreateIndexService extends AbstractComponent &#123; public void createIndex(final CreateIndexClusterStateUpdateRequest request, final ActionListener&lt;CreateIndexClusterStateUpdateResponse&gt; listener) &#123; onlyCreateIndex(request, ActionListener.wrap(response -&gt; &#123; if (response.isAcknowledged()) &#123; activeShardsObserver.waitForActiveShards(new String[]&#123;request.index()&#125;, request.waitForActiveShards(), request.ackTimeout(), shardsAcked -&gt; &#123; if (shardsAcked == false) &#123; logger.debug(&quot;[&#123;&#125;] index created, but the operation timed out while waiting for &quot; + &quot;enough shards to be started.&quot;, request.index()); &#125; listener.onResponse(new CreateIndexClusterStateUpdateResponse(response.isAcknowledged(), shardsAcked)); &#125;, listener::onFailure); &#125; else &#123; listener.onResponse(new CreateIndexClusterStateUpdateResponse(false, false)); &#125; &#125;, listener::onFailure)); &#125; private void onlyCreateIndex(final CreateIndexClusterStateUpdateRequest request, final ActionListener&lt;ClusterStateUpdateResponse&gt; listener) &#123; Settings.Builder updatedSettingsBuilder = Settings.builder(); Settings build = updatedSettingsBuilder.put(request.settings()).normalizePrefix(IndexMetaData.INDEX_SETTING_PREFIX).build(); indexScopedSettings.validate(build, true); // we do validate here - index setting must be consistent request.settings(build); //提交任务 clusterService.submitStateUpdateTask(&quot;create-index [&quot; + request.index() + &quot;], cause [&quot; + request.cause() + &quot;]&quot;, new IndexCreationTask(logger, allocationService, request, listener, indicesService, aliasValidator, xContentRegistry, settings, this::validate)); &#125;&#125; IndexCreationTask SafeAckedClusterStateTaskListenerTieBreakingPrioritizedRunnable Index 相关 API 12345678910111213141516171819202122232425262728#查看索引相关信息GET kibana_sample_data_ecommerce#查看索引的文档总数GET kibana_sample_data_ecommerce&#x2F;_count#查看前10条文档，了解文档格式POST kibana_sample_data_ecommerce&#x2F;_search&#123;&#125;#_cat indices API#查看indicesGET &#x2F;_cat&#x2F;indices&#x2F;kibana*?v&amp;s&#x3D;index#查看状态为绿的索引GET &#x2F;_cat&#x2F;indices?v&amp;health&#x3D;green#按照文档个数排序GET &#x2F;_cat&#x2F;indices?v&amp;s&#x3D;docs.count:desc#查看具体的字段GET &#x2F;_cat&#x2F;indices&#x2F;kibana*?pri&amp;v&amp;h&#x3D;health,index,pri,rep,docs.count,mt#How much memory is used per index?GET &#x2F;_cat&#x2F;indices?v&amp;h&#x3D;i,tm&amp;s&#x3D;tm:desc 123456789101112131415161718192021get _cat&#x2F;nodes?vGET &#x2F;_nodes&#x2F;es7_01,es7_02GET &#x2F;_cat&#x2F;nodes?vGET &#x2F;_cat&#x2F;nodes?v&amp;h&#x3D;id,ip,port,v,mGET _cluster&#x2F;healthGET _cluster&#x2F;health?level&#x3D;shardsGET &#x2F;_cluster&#x2F;health&#x2F;kibana_sample_data_ecommerce,kibana_sample_data_flightsGET &#x2F;_cluster&#x2F;health&#x2F;kibana_sample_data_flights?level&#x3D;shards#### cluster stateThe cluster state API allows access to metadata representing the state of the whole cluster. This includes information such asGET &#x2F;_cluster&#x2F;state#cluster get settingsGET &#x2F;_cluster&#x2F;settingsGET &#x2F;_cluster&#x2F;settings?include_defaults&#x3D;trueGET _cat&#x2F;shardsGET _cat&#x2F;shards?h&#x3D;index,shard,prirep,state,unassigned.reason final CreateIndexClusterStateUpdateRequest updateRequest = new CreateIndexClusterStateUpdateRequest(request, cause, indexName, request.index(), request.updateAllTypes()).ackTimeout(request.timeout()).masterNodeTimeout(request.masterNodeTimeout()).settings(request.settings()).mappings(request.mappings()).aliases(request.aliases()).customs(request.customs()).waitForActiveShards(request.waitForActiveShards()); createIndexService.createIndex(updateRequest, ActionListener.wrap(response -&gt; listener.onResponse(new CreateIndexResponse(response.isAcknowledged(), response.isShardsAcked(), indexName)), listener::onFailure)); new IndexCreationTask(logger, allocationService, request, listener, indicesService, aliasValidator, xContentRegistry, settings,this::validate)) 1 从集群状态元数据找出 templates2 从请求中解析出mappings3 设置索引setting,优先使用templates，如果没有使用默认setting4 构造了新 indexMetaData5 把 indexMetaData 设置进 MetaData，生成新 MetaData6 把 MetaData 设置进 ClusterState，生成新 ClusterState7 重新设置路由表，把 RoutingTable 设置进 ClusterState8 返回新状态，得到新集群状态后，开始给节点分配分片，刚开始只会把主分片分给节点，副分片不动，主分片状态由UNASSIGNED 转成 INITIALIZING9 开始把新的集群状态发送给集群所有节点10 org.elasticsearch.cluster.service.ClusterApplierService#callClusterStateAppliers 方法开始对新集群状态设置org.elasticsearch.indices.cluster.IndicesClusterStateService#applyClusterState 11 接受到状态后，查找该节点分配到的分片，创建分片IndexShard创建_state和index文件夹，_state下写入state-0.st文件的是否为主分片，索引id,分片idtranslog translog.ckp translog-1.tlog org.elasticsearch.index.engine.InternalEngine#getIndexWriterConfig 创建 IndexWriterConfig org.elasticsearch.index.engine.InternalEngine#createWriter(org.apache.lucene.store.Directory, org.apache.lucene.index.IndexWriterConfig) //创建 IndexWriter seqNoStats = new SeqNoStats(SequenceNumbers.NO_OPS_PERFORMED,SequenceNumbers.NO_OPS_PERFORMED,SequenceNumbers.UNASSIGNED_SEQ_NO); seqNoService.getGlobalCheckpoint() final long minSeqNo = SequenceNumbers.NO_OPS_PERFORMED;final long maxSeqNo = SequenceNumbers.NO_OPS_PERFORMED; Translog.ckp return new Checkpoint(offset = 0, numOps=0, generation=1, minSeqNo=-1, maxSeqNo=-1, globalCheckpoint=-2, minTranslogGeneration=1); Checkpoint数据写入Translog.ckp文件 translog-1.tlog CodecUtil.writeHeader(out, TRANSLOG_CODEC, VERSION);translogUUID ref=translogUUIDreturn new Checkpoint(offset = getHeaderLength(ref.length), numOps=0, generation=1, minSeqNo=-1, maxSeqNo=-1, globalCheckpoint=-2, minTranslogGeneration=1); 段提交 开始生成 StandardDirectoryReader,并生成IndexSearcherinternalSearcherManager.addListener(versionMap);//new RefreshMetricUpdater(refreshMetric)this.internalSearcherManager.addListener(listener);//buildRefreshListeners()this.externalSearcherManager.addListener(listener); refresh * 2 主分片状态由INITIALIZING 转成 STARTED给主节点发送请求开始为副分片分配节点从新得到新的集群状态，发布新状态，得到新的状态节点开始更新 主分片更新 IndexNotFoundException FileNotFoundException IOException 副分片创建IndexShared,后开始索引恢复，发送请求给主分片节点 主分片节点接受到请求后，传送主分片给副分片，副分片创建 Engine,副分片恢复成功 副分片更新，只是换了状态 nonFailedTasks.forEach(task -&gt; task.listener.clusterStateProcessed(task.source(), previousClusterState, newClusterState));taskInputs.executor.clusterStatePublished(clusterChangedEvent); SubReaderWrapper onTragicEvent tragicEvent @Overridepublic IndexSearcher newSearcher(IndexReader reader, IndexReader previousReader) throws IOException {IndexSearcher searcher = super.newSearcher(reader, previousReader);searcher.setQueryCache(engineConfig.getQueryCache());searcher.setQueryCachingPolicy(engineConfig.getQueryCachingPolicy());searcher.setSimilarity(engineConfig.getSimilarity());return searcher;} (searcher) -&gt; { IndexShard shard = getShardOrNull(shardId.getId()); if (shard != null) { warmer.warm(searcher, shard, IndexService.this.indexSettings);}} org.elasticsearch.index.shard.IndexShard#newEngineConfig DirectoryReader.openIfChanged((DirectoryReader) r); 专家：返回只读读取器，涵盖对索引的所有已提交和未提交的更改。 这提供了“近乎实时”的搜索，因为在 IndexWriter 会话期间所做的更改可以快速用于搜索，而无需关闭 writer 或调用 {@link #commit}。 请注意，这在功能上等同于调用 {flush} 然后打开一个新的阅读器。 但是这种方法的周转时间应该更快，因为它避免了潜在的代价高昂的 {@link #commit}。 一旦使用完毕，您必须关闭此方法返回的 {@link IndexReader}。 它接近实时，因为没有硬性保证在使用 IndexWriter 进行更改后您能多快获得新阅读器。 您必须根据自己的情况进行试验，以确定它是否足够快。 由于这是一个新的实验性功能，请报告您的发现，以便我们可以学习、改进和迭代。 生成的阅读器支持 {@link DirectoryReader#openIfChanged}，但该调用将简单地转发回此方法（尽管将来可能会改变）。 第一次调用此方法时，此写入器实例将尽一切努力将其打开的读取器池化以进行合并、应用删除等。这意味着额外的资源（RAM、文件描述符、CPU 时间）将被占用 消耗。 为了降低重新打开阅读器的延迟，您应该调用 {@link IndexWriterConfig#setMergedSegmentWarmer} 在新合并的段提交到索引之前对其进行预热。 这对于在大型合并后最小化索引到搜索的延迟很重要。 如果 addIndexes* 调用正在另一个线程中运行，那么这个读取器将只从外部索引中搜索到目前为止已成功复制的那些段。 注意：一旦写入器关闭，任何未完成的读取器可能会继续使用。但是，如果您尝试重新打开这些读取器中的任何一个，您将遇到 {@link AlreadyClosedException}。 索引中有多少文档，或者正在添加（保留）的过程中。 例如，像 addIndexes 这样的操作将在实际更改索引之前首先保留添加 N 个文档的权利，就像酒店如何在您的信用卡上放置“授权保留”以确保他们稍后可以在您退房时向您收费。 final IndicesService indicesService = new IndicesService(settings, pluginsService, nodeEnvironment, xContentRegistry,analysisModule.getAnalysisRegistry(),clusterModule.getIndexNameExpressionResolver(), indicesModule.getMapperRegistry(), namedWriteableRegistry,threadPool, settingsModule.getIndexScopedSettings(), circuitBreakerService, bigArrays, scriptModule.getScriptService(),client, metaStateService); this.buildInIndexListener =Arrays.asList(peerRecoverySourceService,recoveryTargetService,searchService,syncedFlushService,snapshotShardsService); shardStartedClusterStateTaskExecutorthreadPool.schedule(activityTimeout, ThreadPool.Names.GENERIC,new RecoveryMonitor(recoveryTarget.recoveryId(), recoveryTarget.lastAccessTime(), activityTimeout)); org.elasticsearch.indices.recovery.PeerRecoveryTargetService#doRecovery 当节点异常重启时， 写入磁盘的数据先到文件系统的缓冲， 未必来得及刷盘， 如果不通过某种方式将未刷盘的数据找回来， 则会丢失一些数据， 这是保持数据完整性的体现；另一方面， 由于写入操作在多个分片副本上没有来得及全部执行， 副分片需要同步成和主分片完全一致， 这是数据副 本一致性的体现 主分片从translog中自我恢复， 尚未执行flush到磁盘的Lucene分段可以从translog中重建；· 副分片需要从主分片中拉取Lucene分段和translog进行恢复。 但是有机会跳过拉取Lucene分段的过程。索引恢复的触发条件包括从快照备份恢复、 节点加入和离开、 索引的_open操作等 org.apache.lucene.codecs.lucene90.Lucene90Codecorg.apache.lucene.codecs.lucene90.Lucene90CodecLucene90CompressingStoredFieldsWriter CharTermAttributePackedTokenAttributeImpl this.attributes = input.attributes;this.attributeImpls = input.attributeImpls;this.currentState = input.currentState;this.factory = input.factory; return new TokenStreamComponents(r -&gt; {src.setMaxTokenLength(StandardAnalyzer.this.maxTokenLength);src.setReader(r);},tok); schema.reset(docId);invertState.reset();stream.reset(); Lucene90NormsProducer final SegmentInfos sis = new SegmentInfos(config.getIndexCreatedVersionMajor()); final SegmentWriteState flushState =new SegmentWriteState(infoStream,directory,segmentInfo,fieldInfos.finish(),pendingUpdates,new IOContext(new FlushInfo(numDocsInRAM, lastCommittedBytesUsed))); org.apache.lucene.index.DocumentsWriter#flushAllThreads加锁 org.apache.lucene.index.DocumentsWriterFlushControl#markForFullFlush 加锁换DocumentsWriterDeleteQueue DeltaPackedLongValues Lucene90PostingsWriter new FreqProxTerms(perField) FreqProxTermsEnum termsEnum = new FreqProxTermsEnum(terms); posEnum = new FreqProxPostingsEnum(terms, postingsArray);","categories":[{"name":"spring cloud alibaba","slug":"spring-cloud-alibaba","permalink":"https://xxnjdg.github.io/categories/spring-cloud-alibaba/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://xxnjdg.github.io/tags/elasticsearch/"}]},{"title":"elasticsearch数据写入","slug":"elasticsearch-2","date":"2021-05-04T11:42:00.000Z","updated":"2021-06-22T20:46:03.729Z","comments":true,"path":"2021/05/04/elasticsearch-2/","link":"","permalink":"https://xxnjdg.github.io/2021/05/04/elasticsearch-2/","excerpt":"","text":"https://elasticsearch.cn/article/13533 写入入口 RestIndexAction 参数 简介 version 设置文档版本号。主要用于实现乐观锁 version type 默认为internal，请求参数指定的版本号与存储的文档版本号相同则写入。其他可选值有external等类型，为external类型时，如果当前存储的文档版本号小于请求参数指定的版本号，则写入数据。version type主要控制版本号的比较机制，用于对文档进行并发更新操作时同步数据 op type 可设置为create，代表仅在文档不存在时才写入。如果文档已存在，则写请求将失败 routing ES默认使用文档ID进行路由，指定routing可使用routing值进行路由 wait for active shards 用于控制写一致性，当指定数量的分片副本可用时才执行写入，否则重试直至超时。默认为1，主分片可用即执行写入 refresh 写入完毕后执行refresh,使其对搜索可见 timeout 请求超时时间，默认为1分钟 pipeline 指定事先创建好的pipeline名称 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class RestIndexAction extends BaseRestHandler &#123; public RestIndexAction(Settings settings, RestController controller) &#123; super(settings); //1 写入文档，自动生成id, 注册 post 请求 /&#123;index&#125;/&#123;type&#125; 对应的处理函数 this controller.registerHandler(POST, &quot;/&#123;index&#125;/&#123;type&#125;&quot;, this); // auto id creation controller.registerHandler(PUT, &quot;/&#123;index&#125;/&#123;type&#125;/&#123;id&#125;&quot;, this); controller.registerHandler(POST, &quot;/&#123;index&#125;/&#123;type&#125;/&#123;id&#125;&quot;, this); CreateHandler createHandler = new CreateHandler(settings); controller.registerHandler(PUT, &quot;/&#123;index&#125;/&#123;type&#125;/&#123;id&#125;/_create&quot;, createHandler); controller.registerHandler(POST, &quot;/&#123;index&#125;/&#123;type&#125;/&#123;id&#125;/_create&quot;, createHandler); &#125; @Override public String getName() &#123; return &quot;document_index_action&quot;; &#125; final class CreateHandler extends BaseRestHandler &#123; protected CreateHandler(Settings settings) &#123; super(settings); &#125; @Override public String getName() &#123; return &quot;document_create_action&quot;; &#125; @Override public RestChannelConsumer prepareRequest(RestRequest request, final NodeClient client) throws IOException &#123; request.params().put(&quot;op_type&quot;, &quot;create&quot;); return RestIndexAction.this.prepareRequest(request, client); &#125; &#125; //2 对应处理器方法执行 @Override public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException &#123; //解析各种请求参数,请求体 IndexRequest indexRequest = new IndexRequest(request.param(&quot;index&quot;), request.param(&quot;type&quot;), request.param(&quot;id&quot;)); //默认id路由 indexRequest.routing(request.param(&quot;routing&quot;)); indexRequest.parent(request.param(&quot;parent&quot;)); indexRequest.setPipeline(request.param(&quot;pipeline&quot;)); //请求体，即文档内容 indexRequest.source(request.requiredContent(), request.getXContentType()); //默认1分钟超时 indexRequest.timeout(request.paramAsTime(&quot;timeout&quot;, IndexRequest.DEFAULT_TIMEOUT)); //默认不刷新 indexRequest.setRefreshPolicy(request.param(&quot;refresh&quot;)); //设置版本号，用于乐观锁 indexRequest.version(RestActions.parseVersion(request)); indexRequest.versionType(VersionType.fromString(request.param(&quot;version_type&quot;), indexRequest.versionType())); String sOpType = request.param(&quot;op_type&quot;); String waitForActiveShards = request.param(&quot;wait_for_active_shards&quot;); if (waitForActiveShards != null) &#123; indexRequest.waitForActiveShards(ActiveShardCount.parseString(waitForActiveShards)); &#125; if (sOpType != null) &#123; indexRequest.opType(sOpType); &#125; //3 执行这个方法 return channel -&gt; client.index(indexRequest, new RestStatusToXContentListener&lt;&gt;(channel, r -&gt; r.getLocation(indexRequest.routing()))); &#125;&#125; 进入这个方法 org.elasticsearch.action.bulk.TransportBulkAction#doExecute(org.elasticsearch.tasks.Task, org.elasticsearch.action.bulk.BulkRequest, org.elasticsearch.action.ActionListener&lt;org.elasticsearch.action.bulk.BulkResponse&gt;)这个方法做了以下事情 1. ingest pipeline查看该请求是否符合某个ingest pipeline的pattern, 如果符合则执行pipeline中的逻辑，一般是对文档进行各种预处理，如格式调整，增加字段等。如果当前节点没有ingest角色，则需要将请求转发给有ingest角色的节点执行。 2. 自动创建索引判断索引是否存在，如果开启了自动创建则交给主节点自动创建，否则报错 3. 设置routing获取请求URL或mapping中的_routing，如果没有则使用_id, 如果没有指定_id则ES会自动生成一个全局唯一ID。该_routing字段用于决定文档分配在索引的哪个shard上。 4. 构建BulkShardRequest由于Bulk Request中包含多种(Index/Update/Delete)请求，这些请求分别需要到不同的shard上执行，因此协调节点，会将请求按照shard分开，同一个shard上的请求聚合到一起，构建BulkShardRequest 5. 将请求发送给primary shard因为当前执行的是写操作，因此只能在primary上完成，所以需要把请求路由到primary shard所在节点 6. 等待primary shard返回123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344public class TransportBulkAction extends HandledTransportAction&lt;BulkRequest, BulkResponse&gt; &#123; @Override protected void doExecute(Task task, BulkRequest bulkRequest, ActionListener&lt;BulkResponse&gt; listener) &#123; //1 处理pipeline请求 if (bulkRequest.hasIndexRequestsWithPipelines()) &#123; if (clusterService.localNode().isIngestNode()) &#123; processBulkIndexIngestRequest(task, bulkRequest, listener); &#125; else &#123; ingestForwarder.forwardIngestRequest(BulkAction.INSTANCE, bulkRequest, listener); &#125; return; &#125; final long startTime = relativeTime(); final AtomicArray&lt;BulkItemResponse&gt; responses = new AtomicArray&lt;&gt;(bulkRequest.requests.size()); //默认为true if (needToCheck()) &#123; // Attempt to create all the indices that we&#x27;re going to need during the bulk before we start. // Step 1: collect all the indices in the request //获取索引名字 final Set&lt;String&gt; indices = bulkRequest.requests.stream() // delete requests should not attempt to create the index (if the index does not // exists), unless an external versioning is used .filter(request -&gt; request.opType() != DocWriteRequest.OpType.DELETE || request.versionType() == VersionType.EXTERNAL || request.versionType() == VersionType.EXTERNAL_GTE) .map(DocWriteRequest::index) .collect(Collectors.toSet()); /* Step 2: filter that to indices that don&#x27;t exist and we can create. At the same time build a map of indices we can&#x27;t create * that we&#x27;ll use when we try to run the requests. */ final Map&lt;String, IndexNotFoundException&gt; indicesThatCannotBeCreated = new HashMap&lt;&gt;(); Set&lt;String&gt; autoCreateIndices = new HashSet&lt;&gt;(); ClusterState state = clusterService.state(); for (String index : indices) &#123; boolean shouldAutoCreate; try &#123; //是否自动创建索引,从路由表查询索引是否存在，如果不存在那么就自动创建索引,路由表在集群状态中保存 shouldAutoCreate = shouldAutoCreate(index, state); &#125; catch (IndexNotFoundException e) &#123; shouldAutoCreate = false; indicesThatCannotBeCreated.put(index, e); &#125; if (shouldAutoCreate) &#123; autoCreateIndices.add(index); &#125; &#125; // Step 3: create all the indices that are missing, if there are any missing. start the bulk after all the creates come back. if (autoCreateIndices.isEmpty()) &#123; executeBulk(task, bulkRequest, startTime, listener, responses, indicesThatCannotBeCreated); &#125; else &#123; //需要自动创建索引 final AtomicInteger counter = new AtomicInteger(autoCreateIndices.size()); for (String index : autoCreateIndices) &#123; //2 这个方法就是创建索引 createIndex(index, bulkRequest.timeout(), new ActionListener&lt;CreateIndexResponse&gt;() &#123; @Override public void onResponse(CreateIndexResponse result) &#123; if (counter.decrementAndGet() == 0) &#123; //创建完所有索引后执行 executeBulk(task, bulkRequest, startTime, listener, responses, indicesThatCannotBeCreated); &#125; &#125; @Override public void onFailure(Exception e) &#123; if (!(ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException)) &#123; // fail all requests involving this index, if create didn&#x27;t work for (int i = 0; i &lt; bulkRequest.requests.size(); i++) &#123; DocWriteRequest request = bulkRequest.requests.get(i); if (request != null &amp;&amp; setResponseFailureIfIndexMatches(responses, i, request, index, e)) &#123; bulkRequest.requests.set(i, null); &#125; &#125; &#125; if (counter.decrementAndGet() == 0) &#123; executeBulk(task, bulkRequest, startTime, ActionListener.wrap(listener::onResponse, inner -&gt; &#123; inner.addSuppressed(e); listener.onFailure(inner); &#125;), responses, indicesThatCannotBeCreated); &#125; &#125; &#125;); &#125; &#125; &#125; else &#123; executeBulk(task, bulkRequest, startTime, listener, responses, emptyMap()); &#125; &#125; void executeBulk(Task task, final BulkRequest bulkRequest, final long startTimeNanos, final ActionListener&lt;BulkResponse&gt; listener, final AtomicArray&lt;BulkItemResponse&gt; responses, Map&lt;String, IndexNotFoundException&gt; indicesThatCannotBeCreated) &#123; new BulkOperation(task, bulkRequest, listener, responses, startTimeNanos, indicesThatCannotBeCreated).run(); &#125; private final class BulkOperation extends AbstractRunnable &#123; @Override protected void doRun() throws Exception &#123; final ClusterState clusterState = observer.setAndGetObservedState(); if (handleBlockExceptions(clusterState)) &#123; return; &#125; final ConcreteIndices concreteIndices = new ConcreteIndices(clusterState, indexNameExpressionResolver); MetaData metaData = clusterState.metaData(); //遍历所有请求 for (int i = 0; i &lt; bulkRequest.requests.size(); i++) &#123; DocWriteRequest docWriteRequest = bulkRequest.requests.get(i); //the request can only be null because we set it to null in the previous step, so it gets ignored if (docWriteRequest == null) &#123; continue; &#125; //检查索引状态是否可用 if (addFailureIfIndexIsUnavailable(docWriteRequest, i, concreteIndices, metaData)) &#123; continue; &#125; Index concreteIndex = concreteIndices.resolveIfAbsent(docWriteRequest); try &#123; //opType 默认 INDEX switch (docWriteRequest.opType()) &#123; case CREATE: case INDEX: IndexRequest indexRequest = (IndexRequest) docWriteRequest; final IndexMetaData indexMetaData = metaData.index(concreteIndex); MappingMetaData mappingMd = indexMetaData.mappingOrDefault(indexRequest.type()); Version indexCreated = indexMetaData.getCreationVersion(); //3 设置 routing，获取请求URL或mapping中的_routing，如果没有则使用_id indexRequest.resolveRouting(metaData); //如果id为空，生成id indexRequest.process(indexCreated, mappingMd, concreteIndex.getName()); break; case UPDATE: TransportUpdateAction.resolveAndValidateRouting(metaData, concreteIndex.getName(), (UpdateRequest) docWriteRequest); break; case DELETE: docWriteRequest.routing(metaData.resolveIndexRouting(docWriteRequest.parent(), docWriteRequest.routing(), docWriteRequest.index())); // check if routing is required, if so, throw error if routing wasn&#x27;t specified if (docWriteRequest.routing() == null &amp;&amp; metaData.routingRequired(concreteIndex.getName(), docWriteRequest.type())) &#123; throw new RoutingMissingException(concreteIndex.getName(), docWriteRequest.type(), docWriteRequest.id()); &#125; break; default: throw new AssertionError(&quot;request type not supported: [&quot; + docWriteRequest.opType() + &quot;]&quot;); &#125; &#125; catch (ElasticsearchParseException | IllegalArgumentException | RoutingMissingException e) &#123; BulkItemResponse.Failure failure = new BulkItemResponse.Failure(concreteIndex.getName(), docWriteRequest.type(), docWriteRequest.id(), e); BulkItemResponse bulkItemResponse = new BulkItemResponse(i, docWriteRequest.opType(), failure); responses.set(i, bulkItemResponse); // make sure the request gets never processed again bulkRequest.requests.set(i, null); &#125; &#125; // first, go over all the requests and create a ShardId -&gt; Operations mapping Map&lt;ShardId, List&lt;BulkItemRequest&gt;&gt; requestsByShard = new HashMap&lt;&gt;(); for (int i = 0; i &lt; bulkRequest.requests.size(); i++) &#123; DocWriteRequest request = bulkRequest.requests.get(i); if (request == null) &#123; continue; &#125; //获取索引名 String concreteIndex = concreteIndices.getConcreteIndex(request.index()).getName(); //根据id 或 _routing 算出要写入到的分片id ShardId shardId = clusterService.operationRouting().indexShards(clusterState, concreteIndex, request.id(), request.routing()).shardId(); List&lt;BulkItemRequest&gt; shardRequests = requestsByShard.computeIfAbsent(shardId, shard -&gt; new ArrayList&lt;&gt;()); //把同一分片的请求合并一起 shardRequests.add(new BulkItemRequest(i, request)); &#125; if (requestsByShard.isEmpty()) &#123; listener.onResponse(new BulkResponse(responses.toArray(new BulkItemResponse[responses.length()]), buildTookInMillis(startTimeNanos))); return; &#125; final AtomicInteger counter = new AtomicInteger(requestsByShard.size()); String nodeId = clusterService.localNode().getId(); //遍历每个分片请求 for (Map.Entry&lt;ShardId, List&lt;BulkItemRequest&gt;&gt; entry : requestsByShard.entrySet()) &#123; final ShardId shardId = entry.getKey(); final List&lt;BulkItemRequest&gt; requests = entry.getValue(); //4 构建BulkShardRequest BulkShardRequest bulkShardRequest = new BulkShardRequest(shardId, bulkRequest.getRefreshPolicy(), requests.toArray(new BulkItemRequest[requests.size()])); bulkShardRequest.waitForActiveShards(bulkRequest.waitForActiveShards()); bulkShardRequest.timeout(bulkRequest.timeout()); if (task != null) &#123; bulkShardRequest.setParentTask(nodeId, task.getId()); &#125; shardBulkAction.execute(bulkShardRequest, new ActionListener&lt;BulkShardResponse&gt;() &#123; @Override public void onResponse(BulkShardResponse bulkShardResponse) &#123; for (BulkItemResponse bulkItemResponse : bulkShardResponse.getResponses()) &#123; // we may have no response if item failed if (bulkItemResponse.getResponse() != null) &#123; bulkItemResponse.getResponse().setShardInfo(bulkShardResponse.getShardInfo()); &#125; responses.set(bulkItemResponse.getItemId(), bulkItemResponse); &#125; if (counter.decrementAndGet() == 0) &#123; finishHim(); &#125; &#125; @Override public void onFailure(Exception e) &#123; // create failures for all relevant requests for (BulkItemRequest request : requests) &#123; final String indexName = concreteIndices.getConcreteIndex(request.index()).getName(); DocWriteRequest docWriteRequest = request.request(); responses.set(request.id(), new BulkItemResponse(request.id(), docWriteRequest.opType(), new BulkItemResponse.Failure(indexName, docWriteRequest.type(), docWriteRequest.id(), e))); &#125; if (counter.decrementAndGet() == 0) &#123; finishHim(); &#125; &#125; private void finishHim() &#123; listener.onResponse(new BulkResponse(responses.toArray(new BulkItemResponse[responses.length()]), buildTookInMillis(startTimeNanos))); &#125; &#125;); &#125; &#125; &#125;&#125;public abstract class TransportReplicationAction&lt; Request extends ReplicationRequest&lt;Request&gt;, ReplicaRequest extends ReplicationRequest&lt;ReplicaRequest&gt;, Response extends ReplicationResponse &gt; extends TransportAction&lt;Request, Response&gt; &#123; @Override protected void doExecute(Task task, Request request, ActionListener&lt;Response&gt; listener) &#123; new ReroutePhase((ReplicationTask) task, request, listener).run(); &#125; final class ReroutePhase extends AbstractRunnable &#123; private final ActionListener&lt;Response&gt; listener; private final Request request; private final ReplicationTask task; private final ClusterStateObserver observer; private final AtomicBoolean finished = new AtomicBoolean(); @Override public void onFailure(Exception e) &#123; finishWithUnexpectedFailure(e); &#125; @Override protected void doRun() &#123; setPhase(task, &quot;routing&quot;); final ClusterState state = observer.setAndGetObservedState(); if (handleBlockExceptions(state)) &#123; return; &#125; // request does not have a shardId yet, we need to pass the concrete index to resolve shardId final String concreteIndex = concreteIndex(state); final IndexMetaData indexMetaData = state.metaData().index(concreteIndex); if (indexMetaData == null) &#123; retry(new IndexNotFoundException(concreteIndex)); return; &#125; if (indexMetaData.getState() == IndexMetaData.State.CLOSE) &#123; throw new IndexClosedException(indexMetaData.getIndex()); &#125; // resolve all derived request fields, so we can route and apply it resolveRequest(indexMetaData, request); assert request.shardId() != null : &quot;request shardId must be set in resolveRequest&quot;; assert request.waitForActiveShards() != ActiveShardCount.DEFAULT : &quot;request waitForActiveShards must be set in resolveRequest&quot;; //根据请求分片id获取主分片 final ShardRouting primary = primary(state); if (retryIfUnavailable(state, primary)) &#123; return; &#125; //获取主分片所在的节点 final DiscoveryNode node = state.nodes().get(primary.currentNodeId()); //5 因为当前执行的是写操作，因此只能在primary上完成，所以需要把请求路由到primary shard所在节点 if (primary.currentNodeId().equals(state.nodes().getLocalNodeId())) &#123; //主分片在当前节点 performLocalAction(state, primary, node, indexMetaData); &#125; else &#123; //在其他节点 performRemoteAction(state, primary, node); &#125; &#125; private void performLocalAction(ClusterState state, ShardRouting primary, DiscoveryNode node, IndexMetaData indexMetaData) &#123; setPhase(task, &quot;waiting_on_primary&quot;); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;send action [&#123;&#125;] to local primary [&#123;&#125;] for request [&#123;&#125;] with cluster state version [&#123;&#125;] to [&#123;&#125;] &quot;, transportPrimaryAction, request.shardId(), request, state.version(), primary.currentNodeId()); &#125; performAction(node, transportPrimaryAction, true, new ConcreteShardRequest&lt;&gt;(request, primary.allocationId().getId(), indexMetaData.primaryTerm(primary.id()))); &#125; private void performAction(final DiscoveryNode node, final String action, final boolean isPrimaryAction, final TransportRequest requestToPerform) &#123; transportService.sendRequest(node, action, requestToPerform, transportOptions, new TransportResponseHandler&lt;Response&gt;() &#123; @Override public Response newInstance() &#123; return newResponseInstance(); &#125; @Override public String executor() &#123; return ThreadPool.Names.SAME; &#125; @Override public void handleResponse(Response response) &#123; finishOnSuccess(response); &#125; @Override public void handleException(TransportException exp) &#123; try &#123; // if we got disconnected from the node, or the node / shard is not in the right state (being closed) final Throwable cause = exp.unwrapCause(); if (cause instanceof ConnectTransportException || cause instanceof NodeClosedException || (isPrimaryAction &amp;&amp; retryPrimaryException(cause))) &#123; logger.trace( (org.apache.logging.log4j.util.Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage( &quot;received an error from node [&#123;&#125;] for request [&#123;&#125;], scheduling a retry&quot;, node.getId(), requestToPerform), exp); retry(exp); &#125; else &#123; finishAsFailed(exp); &#125; &#125; catch (Exception e) &#123; e.addSuppressed(exp); finishWithUnexpectedFailure(e); &#125; &#125; &#125;); &#125; &#125;&#125; TransportShardBulkAction primary shardPrimary请求的入口是PrimaryOperationTransportHandler的MessageReceived, 当接收到请求时，执行的逻辑如下 1. 判断操作类型遍历bulk请求中的各子请求，根据不同的操作类型跳转到不同的处理逻辑 2. 将update操作转换为Index和Delete操作获取文档的当前内容，与update内容合并生成新文档，然后将update请求转换成index请求，此处文档设置一个version v1 3. Parse Doc解析文档的各字段，并添加如_uid等ES相关的一些系统字段 4. 更新mapping对于新增字段会根据dynamic mapping或dynamic template生成对应的mapping，如果mapping中有dynamic mapping相关设置则按设置处理，如忽略或抛出异常 5. 获取sequence Id和Version从SequcenceNumberService获取一个sequenceID和Version。SequcenID用于初始化LocalCheckPoint， verion是根据当前Versoin+1用于防止并发写导致数据不一致。 6. 写入lucene这一步开始会对文档uid加锁，然后判断uid对应的version v2和之前update转换时的versoin v1是否一致，不一致则返回第二步重新执行。 如果version一致，如果同id的doc已经存在，则调用lucene的updateDocument接口，如果是新文档则调用lucene的addDoucument. 这里有个问题，如何保证Delete-Then-Add的原子性，ES是通过在Delete之前会加上已refresh锁，禁止被refresh，只有等待Add完成后释放了Refresh Lock, 这样就保证了这个操作的原子性。 7. 写入translog写入Lucene的Segment后，会以key value的形式写Translog， Key是Id, Value是Doc的内容。当查询的时候，如果请求的是GetDocById则可以直接根据_id从translog中获取。满足nosql场景的实时性。 8. 重构bulk request因为primary shard已经将update操作转换为index操作或delete操作，因此要对之前的bulkrequest进行调整，只包含index或delete操作，不需要再进行update的处理操作。 9. flush translog默认情况下，translog要在此处落盘完成，如果对可靠性要求不高，可以设置translog异步，那么translog的fsync将会异步执行，但是落盘前的数据有丢失风险。 10. 发送请求给replicas将构造好的bulkrequest并发发送给各replicas，等待replica返回，这里需要等待所有的replicas返回，响应请求给协调节点。如果某个shard执行失败，则primary会给master发请求remove该shard。这里会同时把sequenceID， primaryTerm, GlobalCheckPoint等传递给replica。 11. 等待replica响应123456789101112131415161718192021public abstract class TransportReplicationAction&lt; Request extends ReplicationRequest&lt;Request&gt;, ReplicaRequest extends ReplicationRequest&lt;ReplicaRequest&gt;, Response extends ReplicationResponse &gt; extends TransportAction&lt;Request, Response&gt; &#123; protected class PrimaryOperationTransportHandler implements TransportRequestHandler&lt;ConcreteShardRequest&lt;Request&gt;&gt; &#123; @Override public void messageReceived(ConcreteShardRequest&lt;Request&gt; request, TransportChannel channel, Task task) &#123; new AsyncPrimaryAction(request.request, request.targetAllocationID, request.primaryTerm, channel, (ReplicationTask) task).run(); &#125; &#125; class AsyncPrimaryAction extends AbstractRunnable implements ActionListener&lt;PrimaryShardReference&gt; &#123; @Override protected void doRun() throws Exception &#123; acquirePrimaryShardReference(request.shardId(), targetAllocationID, primaryTerm, this); &#125; &#125;&#125; 当所有的replica返回请求时，更细primary shard的LocalCheckPoint。 Defaults.FIELD_TYPEFIELD_TYPE.setIndexOptions(IndexOptions.DOCS);FIELD_TYPE.setTokenized(false);FIELD_TYPE.setStored(true);FIELD_TYPE.setOmitNorms(true);FIELD_TYPE.setIndexAnalyzer(Lucene.KEYWORD_ANALYZER);FIELD_TYPE.setSearchAnalyzer(Lucene.KEYWORD_ANALYZER);FIELD_TYPE.setName(NAME);FIELD_TYPE.freeze();defaultFieldType.setIndexOptions(IndexOptions.NONE);defaultFieldType.setStored(false); org.apache.lucene.index.DefaultIndexingChain#processField id context.seqID(seqID); fields.add(seqID.seqNo); fields.add(seqID.seqNoDocValue); fields.add(seqID.primaryTerm); sourceversionfirst jsonField field = new Field(fieldType().name(), binaryValue, fieldType());fields.add(new SortedSetDocValuesField(fieldType().name(), binaryValue)); return new KeywordFieldMapper(name, fieldType, defaultFieldType, ignoreAbove, includeInAll,context.indexSettings(), multiFieldsBuilder.build(this, context), copyTo); return new TextFieldMapper( name, fieldType, defaultFieldType, positionIncrementGap, includeInAll, context.indexSettings(), multiFieldsBuilder.build(this, context), copyTo); return new IndexingStrategy(true, false, true, seqNoForIndexing, 1, null);return new IndexResult(plan.versionForIndexing, plan.seqNoForIndexing, plan.currentNotFoundOrDeleted); out.writeVInt(SERIALIZATION_FORMAT);out.writeString(id);out.writeString(type);out.writeBytesReference(source);out.writeOptionalString(routing);out.writeOptionalString(parent);out.writeLong(version); out.writeByte(versionType.getValue()); out.writeLong(autoGeneratedIdTimestamp); out.writeLong(seqNo); out.writeLong(primaryTerm); BulkItemResponse primaryResponse = new BulkItemResponse(replicaRequest.id(), opType, response);// set a blank ShardInfo so we can safely send it to the replicas. We won’t use it in the real response though.primaryResponse.getResponse().setShardInfo(new ShardInfo());return primaryResponse; return new WritePrimaryResult&lt;&gt;(request, response, location, null, primary, logger); 1createIndexService.createIndex(updateRequest, ActionListener.wrap(response -&gt;listener.onResponse(new CreateIndexResponse(response.isAcknowledged(), response.isShardsAcked(), indexName)),listener::onFailure)); 2ActionListener.wrap(response -&gt; {if (response.isAcknowledged()) {activeShardsObserver.waitForActiveShards(new String[]{request.index()}, request.waitForActiveShards(), request.ackTimeout(),shardsAcked -&gt; {if (shardsAcked == false) {logger.debug(“[{}] index created, but the operation timed out while waiting for “ +“enough shards to be started.”, request.index());}listener.onResponse(new CreateIndexClusterStateUpdateResponse(response.isAcknowledged(), shardsAcked));}, listener::onFailure);} else {listener.onResponse(new CreateIndexClusterStateUpdateResponse(false, false));}}, listener::onFailure) SafeAckedClusterStateTaskListener 任务 AckCountDownListener","categories":[{"name":"spring cloud alibaba","slug":"spring-cloud-alibaba","permalink":"https://xxnjdg.github.io/categories/spring-cloud-alibaba/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://xxnjdg.github.io/tags/elasticsearch/"}]},{"title":"调试elasticsearch源码笔记","slug":"elasticsearch-1","date":"2021-03-22T18:21:43.000Z","updated":"2021-06-25T10:42:19.602Z","comments":true,"path":"2021/03/23/elasticsearch-1/","link":"","permalink":"https://xxnjdg.github.io/2021/03/23/elasticsearch-1/","excerpt":"","text":"环境window10 版本7.11.2 源码下载https://github.com/elastic/elasticsearch.git &#103;&#x69;&#116;&#64;&#103;&#x69;&#x74;&#x68;&#x75;&#98;&#46;&#x63;&#111;&#x6d;:elastic/elasticsearch.git 二进制包下载https://www.elastic.co/cn/downloads/elasticsearch 下载jdk不同版本的es要求对应的jdk版本不一样，当前版本至少要jdk15，可参考以下编译文档 https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md gradle 配置可以参考网上教程，我偷懒不想配置了，打开源码让他自行下载gradle，包下载嫌慢下载路径可以改到阿里云，都不配的情况下，可能要等1，2小时 build打开idea，要求2020.1版本，版本要求同样参考上面的编译文档，我是使用 project from version control,能直接识别是gradle项目 如果是直接git clone,再用idea打开,选择 build.gradle 也能识别是gradle项目，接着等待idea帮我们build run入口函数如下，直接运行 org.elasticsearch.bootstrap.Elasticsearch#main(java.lang.String[]) 会直接提示你有错误 1234&quot;C:\\Program Files\\Java\\jdk-15.0.2\\bin\\java.exe&quot; ...ERROR: the system property [es.path.conf] must be setProcess finished with exit code 78 添加 -Des.networkaddress.cache.ttl=60-Des.networkaddress.cache.negative.ttl=10-Djava.awt.headless=true-Dfile.encoding=UTF-8-Djna.nosys=true-Dio.netty.noUnsafe=true-Dio.netty.noKeySetOptimization=true-Dio.netty.recycler.maxCapacityPerThread=0-Dio.netty.allocator.numDirectArenas=0-Dlog4j.shutdownHookEnabled=false-Dlog4j2.disable.jmx=true-Djava.locale.providers=SPI,COMPAT–add-opens=java.base/java.io=ALL-UNNAMED-Delasticsearch-Des.distribution.flavor=default-Des.distribution.type=zip-Des.bundled_jdk=true-Des.path.home=D:\\a\\elasticsearch-7.12.0-Des.path.conf=D:\\a\\elasticsearch-7.12.0\\config-Djava.io.tmpdir=D:\\a\\tmp-Xms512m-Xms512m-cp D:\\a\\elasticsearch-7.12.0\\lib* “cluster.name” -&gt; “elasticsearch”“node.name” -&gt; null“path.home” -&gt; “D:\\a\\elasticsearch-7.11.2\\elasticsearch-7.11.2”“path.logs” -&gt; “D:\\a\\elasticsearch-7.11.2\\elasticsearch-7.11.2\\logs” https://blog.csdn.net/weixin_36146690/article/details/114086786 https://www.colabug.com/2021/0307/8044055/ interface org.elasticsearch.painless.spi.PainlessExtension “x-pack-autoscaling” -&gt; {ArrayList@4353} size = 1“lang-painless” -&gt; {ArrayList@4050} size = 10“x-pack-ql” -&gt; {ArrayList@4377} size = 2“x-pack-core” -&gt; {ArrayList@4390} size = 37 org.elasticsearch.search.aggregations.matrix.MatrixAggregationPluginorg.elasticsearch.analysis.common.CommonAnalysisPluginorg.elasticsearch.xpack.constantkeyword.ConstantKeywordMapperPlugin 1org.elasticsearch.xpack.flattened.FlattenedMapperPluginorg.elasticsearch.xpack.frozen.FrozenIndicesorg.elasticsearch.ingest.common.IngestCommonPluginorg.elasticsearch.ingest.geoip.IngestGeoIpPluginorg.elasticsearch.ingest.useragent.IngestUserAgentPluginorg.elasticsearch.kibana.KibanaPluginorg.elasticsearch.script.expression.ExpressionPluginorg.elasticsearch.script.mustache.MustachePluginorg.elasticsearch.painless.PainlessPlugin 1org.elasticsearch.index.mapper.MapperExtrasPluginorg.elasticsearch.xpack.versionfield.VersionFieldPluginorg.elasticsearch.join.ParentJoinPluginorg.elasticsearch.percolator.PercolatorPluginorg.elasticsearch.index.rankeval.RankEvalPluginorg.elasticsearch.index.reindex.ReindexPluginorg.elasticsearch.xpack.repositories.metering.RepositoriesMeteringPluginorg.elasticsearch.repositories.encrypted.EncryptedRepositoryPlugin 1org.elasticsearch.plugin.repository.url.URLRepositoryPluginorg.elasticsearch.xpack.searchablesnapshots.SearchableSnapshots 1org.elasticsearch.xpack.searchbusinessrules.SearchBusinessRulesorg.elasticsearch.repositories.blobstore.testkit.SnapshotRepositoryTestKitorg.elasticsearch.xpack.spatial.SpatialPluginorg.elasticsearch.xpack.transform.Transform 1org.elasticsearch.transport.Netty4Pluginorg.elasticsearch.xpack.unsignedlong.UnsignedLongMapperPluginorg.elasticsearch.xpack.vectors.Vectorsorg.elasticsearch.xpack.wildcard.Wildcardorg.elasticsearch.xpack.aggregatemetric.AggregateMetricMapperPlugin 1org.elasticsearch.xpack.analytics.AnalyticsPlugin 1org.elasticsearch.xpack.async.AsyncResultsIndexPlugin 1org.elasticsearch.xpack.search.AsyncSearchorg.elasticsearch.xpack.autoscaling.Autoscaling 1org.elasticsearch.xpack.ccr.Ccr 1org.elasticsearch.xpack.core.XPackPlugin 1org.elasticsearch.xpack.datastreams.DataStreamsPlugin 1org.elasticsearch.xpack.deprecation.Deprecationorg.elasticsearch.xpack.enrich.EnrichPlugin 1org.elasticsearch.xpack.eql.plugin.EqlPluginorg.elasticsearch.xpack.fleet.Fleetorg.elasticsearch.xpack.graph.Graph 1org.elasticsearch.xpack.idp.IdentityProviderPluginorg.elasticsearch.xpack.ilm.IndexLifecycle 1org.elasticsearch.xpack.ingest.IngestPluginorg.elasticsearch.xpack.logstash.Logstashorg.elasticsearch.xpack.ml.MachineLearning 1org.elasticsearch.xpack.monitoring.Monitoring 1org.elasticsearch.xpack.ql.plugin.QlPluginorg.elasticsearch.xpack.rollup.Rollup 1org.elasticsearch.xpack.runtimefields.RuntimeFields 1org.elasticsearch.xpack.security.Security 1org.elasticsearch.xpack.sql.plugin.SqlPluginorg.elasticsearch.xpack.stack.StackPluginorg.elasticsearch.xpack.textstructure.TextStructurePluginorg.elasticsearch.cluster.coordination.VotingOnlyNodePlugin 1org.elasticsearch.xpack.watcher.Watcher 1 “client.type” -&gt; “node”“cluster.name” -&gt; “elasticsearch”“node.name” -&gt; null“path.home” -&gt; “D:\\a\\elasticsearch-7.12.0”“path.logs” -&gt; “D:\\a\\elasticsearch-7.12.0\\logs” org.elasticsearch.xpack.core.XPackPlugin 调度任务 lowFuture = threadPool.scheduleWithFixedDelay(lowMonitor, lowMonitor.interval, Names.SAME);mediumFuture = threadPool.scheduleWithFixedDelay(mediumMonitor, mediumMonitor.interval, Names.SAME);highFuture = threadPool.scheduleWithFixedDelay(highMonitor, highMonitor.interval, Names.SAME); 线程this.cachedTimeThread = new CachedTimeThread(EsExecutors.threadName(settings, “[timer]”), estimatedTimeInterval.millis());this.cachedTimeThread.start(); -Xms256m-Xmx256m-Djava.awt.headless=true-Dfile.encoding=UTF-8-Djna.nosys=true-Dio.netty.noUnsafe=true-Dio.netty.noKeySetOptimization=true-Dio.netty.recycler.maxCapacityPerThread=0-Dlog4j.shutdownHookEnabled=false-Dlog4j2.disable.jmx=true-Delasticsearch-Des.path.home=D:\\a\\612\\elasticsearch-6.1.2\\elasticsearch-6.1.2-Des.path.conf=D:\\a\\612\\elasticsearch-6.1.2\\elasticsearch-6.1.2\\config-cp D:\\a\\612\\elasticsearch-6.1.2\\elasticsearch-6.1.2\\lib* request header 长度 intrequest header 内容response header 长度 intresponse header 内容action 长度 intaction 内容一个字符站一个字节nodeId 长度 output.writeByte((byte)’E’);output.writeByte((byte)’S’);// write the size, the size indicates the remaining message size, not including the size intoutput.writeInt(messageSize + REQUEST_ID_SIZE + STATUS_SIZE + VERSION_ID_SIZE);output.writeLong(requestId);output.writeByte(status);output.writeInt(version.id); transportService.registerRequestHandler(ACTION_NAME, UnicastPingRequest::new, ThreadPool.Names.SAME,new UnicastPingRequestHandler()); transportService.sendRequest(node, SEND_ACTION_NAME,new BytesTransportRequest(bytes, node.getVersion()),options,new EmptyTransportResponseHandler(ThreadPool.Names.SAME) { @Override public void handleResponse(TransportResponse.Empty response) &#123; if (sendingController.getPublishingTimedOut()) &#123; logger.debug(&quot;node &#123;&#125; responded for cluster state [&#123;&#125;] (took longer than [&#123;&#125;])&quot;, node, clusterState.version(), publishTimeout); &#125; sendingController.onNodeSendAck(node); &#125; @Override public void handleException(TransportException exp) &#123; if (sendDiffs &amp;&amp; exp.unwrapCause() instanceof IncompatibleClusterStateVersionException) &#123; logger.debug(&quot;resending full cluster state to node &#123;&#125; reason &#123;&#125;&quot;, node, exp.getDetailedMessage()); sendFullClusterState(clusterState, serializedStates, node, publishTimeout, sendingController); &#125; else &#123; logger.debug((org.apache.logging.log4j.util.Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage(&quot;failed to send cluster state to &#123;&#125;&quot;, node), exp); sendingController.onNodeSendFailed(node, exp); &#125; &#125; &#125;); transportService.registerRequestHandler(SEND_ACTION_NAME, BytesTransportRequest::new, ThreadPool.Names.SAME, false, false,new SendClusterStateRequestHandler()); tasks.put(BECOME_MASTER_TASK, (source1, e) -&gt; {}); // noop listener, the election finished listener determines resulttasks.put(FINISH_ELECTION_TASK, electionFinishedListener); STATE_NOT_RECOVERED_BLOCK customSupplier.put(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress::new);customSupplier.put(RestoreInProgress.TYPE, RestoreInProgress::new);customSupplier.put(SnapshotsInProgress.TYPE, SnapshotsInProgress::new); IndexGraveyard.TYPE, new IndexGraveyard(tombstones) httpRequest = new Netty4HttpRequest(serverTransport.xContentRegistry, copy, ctx.channel()); final Netty4HttpChannel channel =new Netty4HttpChannel(serverTransport, httpRequest, pipelinedRequest, detailedErrorsEnabled, threadContext); return channel -&gt;client.index(indexRequest, new RestStatusToXContentListener&lt;&gt;(channel, r -&gt; r.getLocation(indexRequest.routing()))); execute(IndexAction.INSTANCE, request, listener); ActionHandler actions.register(IndexAction.INSTANCE, TransportIndexAction.class); execute(task, request, new ActionListener() {@Overridepublic void onResponse(Response response) {taskManager.unregister(task);listener.onResponse(response);} @Override public void onFailure(Exception e) &#123; taskManager.unregister(task); listener.onFailure(e); &#125; &#125;); doExecute protected abstract void doExecute(Request request, ActionListener listener); this.action.doExecute(task, request, listener); return channel -&gt;client.index(indexRequest, new RestStatusToXContentListener&lt;&gt;(channel, r -&gt; r.getLocation(indexRequest.routing())));IndexRequestTransportIndexAction execute(task, request, new ActionListener() {@Overridepublic void onResponse(Response response) {taskManager.unregister(task);listener.onResponse(response);} @Override public void onFailure(Exception e) &#123; taskManager.unregister(task); listener.onFailure(e); &#125; &#125;); bulkAction.execute(task, toSingleItemBulkRequest(request), wrapBulkResponse(listener)); this.action.doExecute(task, request, listener); createIndex(index, bulkRequest.timeout(), new ActionListener() {@Overridepublic void onResponse(CreateIndexResponse result) {if (counter.decrementAndGet() == 0) {executeBulk(task, bulkRequest, startTime, listener, responses, indicesThatCannotBeCreated);}} @Override public void onFailure(Exception e) &#123; if (!(ExceptionsHelper.unwrapCause(e) instanceof ResourceAlreadyExistsException)) &#123; // fail all requests involving this index, if create didn&#39;t work for (int i = 0; i &lt; bulkRequest.requests.size(); i++) &#123; DocWriteRequest request = bulkRequest.requests.get(i); if (request != null &amp;&amp; setResponseFailureIfIndexMatches(responses, i, request, index, e)) &#123; bulkRequest.requests.set(i, null); &#125; &#125; &#125; if (counter.decrementAndGet() == 0) &#123; executeBulk(task, bulkRequest, startTime, ActionListener.wrap(listener::onResponse, inner -&gt; &#123; inner.addSuppressed(e); listener.onFailure(inner); &#125;), responses, indicesThatCannotBeCreated); &#125; &#125; &#125;); CreateIndexRequest createIndexRequest = new CreateIndexRequest();createIndexRequest.index(index);createIndexRequest.cause(“auto(bulk api)”);createIndexRequest.masterNodeTimeout(timeout);createIndexAction.execute(createIndexRequest, listener); execute(task, request, new ActionListener() {@Overridepublic void onResponse(Response response) {taskManager.unregister(task);listener.onResponse(response);} @Override public void onFailure(Exception e) &#123; taskManager.unregister(task); listener.onFailure(e); &#125; &#125;); TransportCreateIndexActiondoExecute ActionListener delegate = new ActionListener() {@Overridepublic void onResponse(Response response) {listener.onResponse(response);} @Override public void onFailure(Exception t) &#123; if (t instanceof Discovery.FailedToCommitClusterStateException || (t instanceof NotMasterException)) &#123; logger.debug((org.apache.logging.log4j.util.Supplier&lt;?&gt;) () -&gt; new ParameterizedMessage(&quot;master could not publish cluster state or stepped down before publishing action [&#123;&#125;], scheduling a retry&quot;, actionName), t); retry(t, masterChangePredicate); &#125; else &#123; listener.onFailure(t); &#125; &#125; &#125;; masterOperation(task, request, clusterState, delegate);AckedClusterStateTaskListenernew IndexCreationTask(logger, allocationService, request, listener, indicesService, aliasValidator, xContentRegistry, settings,this::validate) finalListeners.add(onStoreClose);finalListeners.add(oldShardsStats); PreConfiguredCharFilter PreConfiguredTokenFilter PreConfiguredTokenizer PreBuiltAnalyzersanalyzerProviderFactories.put(name, new PreBuiltAnalyzerProviderFactory(name, AnalyzerScope.INDICES, preBuiltAnalyzerEnum.getAnalyzer(Version.CURRENT)));final Analyzer a = new StandardAnalyzer(CharArraySet.EMPTY_SET);PreBuiltAnalyzerProvideranalyzers.register(“default”, StandardAnalyzerProvider::new);analyzers.register(“standard”, StandardAnalyzerProvider::new);analyzers.register(“standard_html_strip”, StandardHtmlStripAnalyzerProvider::new);analyzers.register(“simple”, SimpleAnalyzerProvider::new);analyzers.register(“stop”, StopAnalyzerProvider::new);analyzers.register(“whitespace”, WhitespaceAnalyzerProvider::new);analyzers.register(“keyword”, KeywordAnalyzerProvider::new);analyzers.register(“pattern”, PatternAnalyzerProvider::new);analyzers.register(“snowball”, SnowballAnalyzerProvider::new);analyzers.register(“arabic”, ArabicAnalyzerProvider::new);analyzers.register(“armenian”, ArmenianAnalyzerProvider::new);analyzers.register(“basque”, BasqueAnalyzerProvider::new);analyzers.register(“bengali”, BengaliAnalyzerProvider::new);analyzers.register(“brazilian”, BrazilianAnalyzerProvider::new);analyzers.register(“bulgarian”, BulgarianAnalyzerProvider::new);analyzers.register(“catalan”, CatalanAnalyzerProvider::new);analyzers.register(“chinese”, ChineseAnalyzerProvider::new);analyzers.register(“cjk”, CjkAnalyzerProvider::new);analyzers.register(“czech”, CzechAnalyzerProvider::new);analyzers.register(“danish”, DanishAnalyzerProvider::new);analyzers.register(“dutch”, DutchAnalyzerProvider::new);analyzers.register(“english”, EnglishAnalyzerProvider::new);analyzers.register(“finnish”, FinnishAnalyzerProvider::new);analyzers.register(“french”, FrenchAnalyzerProvider::new);analyzers.register(“galician”, GalicianAnalyzerProvider::new);analyzers.register(“german”, GermanAnalyzerProvider::new);analyzers.register(“greek”, GreekAnalyzerProvider::new);analyzers.register(“hindi”, HindiAnalyzerProvider::new);analyzers.register(“hungarian”, HungarianAnalyzerProvider::new);analyzers.register(“indonesian”, IndonesianAnalyzerProvider::new);analyzers.register(“irish”, IrishAnalyzerProvider::new);analyzers.register(“italian”, ItalianAnalyzerProvider::new);analyzers.register(“latvian”, LatvianAnalyzerProvider::new);analyzers.register(“lithuanian”, LithuanianAnalyzerProvider::new);analyzers.register(“norwegian”, NorwegianAnalyzerProvider::new);analyzers.register(“persian”, PersianAnalyzerProvider::new);analyzers.register(“portuguese”, PortugueseAnalyzerProvider::new);analyzers.register(“romanian”, RomanianAnalyzerProvider::new);analyzers.register(“russian”, RussianAnalyzerProvider::new);analyzers.register(“sorani”, SoraniAnalyzerProvider::new);analyzers.register(“spanish”, SpanishAnalyzerProvider::new);analyzers.register(“swedish”, SwedishAnalyzerProvider::new);analyzers.register(“turkish”, TurkishAnalyzerProvider::new);analyzers.register(“thai”, ThaiAnalyzerProvider::new);analyzers.register(“fingerprint”, FingerprintAnalyzerProvider::new);analyzers.extractAndRegister(plugins, AnalysisPlugin::getAnalyzers); canAllocate addAllocationDecider(deciders, new MaxRetryAllocationDecider(settings));addAllocationDecider(deciders, new ResizeAllocationDecider(settings));addAllocationDecider(deciders, new ReplicaAfterPrimaryActiveAllocationDecider(settings));addAllocationDecider(deciders, new RebalanceOnlyWhenActiveAllocationDecider(settings));addAllocationDecider(deciders, new ClusterRebalanceAllocationDecider(settings, clusterSettings));addAllocationDecider(deciders, new ConcurrentRebalanceAllocationDecider(settings, clusterSettings));addAllocationDecider(deciders, new EnableAllocationDecider(settings, clusterSettings)); 3addAllocationDecider(deciders, new NodeVersionAllocationDecider(settings));addAllocationDecider(deciders, new SnapshotInProgressAllocationDecider(settings));addAllocationDecider(deciders, new RestoreInProgressAllocationDecider(settings));addAllocationDecider(deciders, new FilterAllocationDecider(settings, clusterSettings));addAllocationDecider(deciders, new SameShardAllocationDecider(settings, clusterSettings));addAllocationDecider(deciders, new DiskThresholdDecider(settings, clusterSettings));addAllocationDecider(deciders, new ThrottlingAllocationDecider(settings, clusterSettings));addAllocationDecider(deciders, new ShardsLimitAllocationDecider(settings, clusterSettings));addAllocationDecider(deciders, new AwarenessAllocationDecider(settings, clusterSettings)); private final IndexMetaDataUpdater indexMetaDataUpdater = new IndexMetaDataUpdater();private final RoutingNodesChangedObserver nodesChangedObserver = new RoutingNodesChangedObserver();private final RestoreInProgressUpdater restoreInProgressUpdater = new RestoreInProgressUpdater(); cachedDecisions.put(AllocationStatus.DECIDERS_NO,new AllocateUnassignedDecision(AllocationStatus.DECIDERS_NO, null, null, null, false, 0L, 0L)); TransportShardBulkAction 1 文档元素据 元数据，用于标注文档的相关信息_index一文档所属的索引名_type一文档所属的类型名_id一文档唯一Id_source:文档的原始Json数据_all:整合所有字段内容到该字段，已被废除_version:文档的版本信息_score:相关性打分 索引 Index一索引是文档的容器，是一类文档的结合 Index体现了逻辑空间的概念:每个索引都有 自己的Mapping定义，用于定义包含的文档 的字段名和字段类型 Shard体现了物理空间的概念:索引中的数据 分散在Shard上索引的Mapping与SettingsMapping定义文档字段的类型Setting定义不同的数据分布 Master-eligible nodes和Master Node 每个节点启动后，默认就是一个Master eligible节点 可以设置node.master: false禁止Master-eligible节点可以参加选主流程，成为Master节点当第一个节点启动时候，它会将自己选举成Master节点每个节点上都保存了集群的状态，只有Master节点才能修改集群的状态信息 集群状态(Cluster State)，维护了一个集群中，必要的信息 所有的节点信息 所有的索引和其相关的Mapping与Setti ng信息 分片的路由信息 任意节点都能修改信息会导致数据的不一致性 Data Node&amp;Coordinating NodeData Node 可以保存数据的节点，叫做Data Node。负责保存分片数据。在数据扩展上起到了至关重要的作用Coordinating Node负责接受Client的请求，将请求分发到合适的节点，最终把结果汇集到一起每个节点默认都起到了Coordinating Node的职责 .Hot&amp;Warm Node不同硬件配置的Data Node，用来实现Hot&amp;Warm架构，降低集群部署的成本.Machine Learning Node负责跑机器学习的Job，用来做异常检测Tribe Node(5.3开始使用Cross Cluster Serarch)tribe Node连接到不同的elasticsearch集群并且支持将这些集群当成一个单独的集群处理 配置节点类型开发环境中一个节点可以承担多种角色生产环境中，应该设置单一的角色的节点(dedicated node) 分片(Primary Shard&amp;Replica Shard) 主分片，用以解决数据水平扩展的问题。通过主分片，可以将数据分布到集群内的所有节点之上一个分片是一个运行的Lucene的实例主分片数在索引创建时指定，后续不允许修改，除非Reindex副本，用以解决数据高可用的问题。分片是主分片的拷贝副本分片数，可以动态题调整增加副本数，还可以在一定程度上提高服务的可用性(读取的吞吐) 对于生产环境中分片的设定，需要提前做好容量规划分片数设置过小导致后续无法增加节点实现水品扩展单个分片的数据量太大，导致数据重新分配耗时分片数设置过大，7.0开始，默认主分片设置成1，解决了over-sha rd i ng的问题影响搜索结果的相关’}生打分，影响统计结果的准确’}生单个节点上过多的分片，会导致资源浪费，同时也会影响’}生能 Green一主分片与副本都正常分配yellow一主分片全部正常分配，有副本分片未能正常分配Red一有主分片未能分配 例如，当服务器的磁盘容量超过85%时 去创建了一个新的索引 type名，约定都用_docCreate一如果ID已经存在，会失败Index一如果ID不存在，创建新的文档。否则，先删除现有的文档再创建新的文档，版本会增加Update一文档必须已经存在，更新只会对相应字段做增量修改 具体操作看 3.3-文档的基本CRUD与批量操作 Bulk API 支持在一次API调用中，对不同的索引进行操作支持四种类型操作IndexCreateUpdateDelete可以再URI中指定Index，也可以在请求的Payload中进行操作中单条操作失败，并不会影响其他操作返回结果包括了每一条操作执行的结果 具体操作看 3.3-文档的基本CRUD与批量操作 批量读取文档一mget批量操作，可以减少网络连接所产生的开销，提高性能 具体操作看 3.3-文档的基本CRUD与批量操作 批量查询一msearch 具体操作看 3.3-文档的基本CRUD与批量操作 倒排索引的核心组成 倒排索引包含两个部分单词词典(term Dictionary)，记录所有文档的单词，记录单词到倒排列表的关联关系单词词典一般比较大，可以通过B+树或哈希拉链法实现，以满足高性能的插入与查询倒排列表(Posting List)一记录了单词对应的文档结合，由倒排索引项组成倒排索引项(Posting).文档ID.词频TF一该单词在文档中出现的次数，用于相关性评分·位置(Position)一单词在文档中分词的位置。用于语句搜索(phrase query).偏移(Offset)一记录单词的开始结束位置，实现高亮显示 Elasticsearch的JSON文档中的每个字段，都有自己的倒排索引可以指定对某些字段不做索引优点:节省存储空间缺点:字段无法被搜索 Analysis与Analyzer Analysis一文本分析是把全文本转换一系列单词(term / token)的过程，也叫分词Analysis是通过Analyzer来实现的可使用Elasticsearch内置的分析器/或者按需定制化分析器除了在数据写入时转换词条，匹配Query语句时候也需要用相同的分析器对查询语句进行分析 分词器是专门处理分词的组件，Analyzer由三部分组成 Character Filters(针对原始文本处理，例如去除html)/tokenizer(按照规则切分为单词)/token Filter(将切分的的单词进行加工，小写，删除stopwords增加同义词) 具体例子 3.5-通过分析器进行分词 什么是Mapping Mapping类似数据库中的schema的定义，作用如下 定义索引中的字段的名称 定义字段的数据类型，例如字符串，数字，布尔…… 字段，倒排索引的相关配置，(Analyzed or Not Analyzed, Analyzer)Mapping会把JSON文档映射成Lucene所需要的扁平格式一个Mapping属于一个索引的type 每个文档都属于一个type 一个type有一个Mapping定义 7.0开始，不需要在Mapping定义中指定type信息 字段的数据类型 简单类型Text/KeywordDateInteger/FloatingBooleanIPv4&amp;IPv6复杂类型一对象和嵌套对象对象类型/嵌套类型特殊类型geo_point&amp;geo_ shape/percolator 什么是Dynamic Mapping 在写入文档时候，如果索引不存在会自动创建索引Dynamic Mapping的机制，使得我们无需手动定义MappingsoElasticsearch会自动根据文档信息，推算出字段的类型但是有时候会推算的不对，例如地理位置信息当类型如果设置不对时，会导致一些功能无法正常运行，例如Range查询 类型的自动识别 JSON类型 Elasticsearch类型 字符串 匹配日期格式，设置成Date 配置数字设置为float或者Long，该选项默认关闭 设置为text，并且增加keyword子字段 布尔值 Boolean 浮点数 float 整数 Long 对象 Object 数组 由第一个非空数值的类型所决定 空值 忽略 Term 是表达语意的最⼩小单位。搜索和利利⽤用统计语⾔言模型进⾏行行⾃自然语⾔言处理理都需要处理理 Term 在 ES 中， Term 查询，对输⼊不做分词。会将输⼊作为一个整体，在倒排索引中查找准确的词项， 并且使⽤用相关度算分公式为每个包含该词项的⽂文档进⾏行行相关度算分 – 例例如“Apple Store”可以通过 Constant Score 将查询转换成⼀一个 Filtering，避免算分，并利利⽤用缓存，提⾼高性能 索引和搜索时都会进行分词，查询字符串先传递到⼀个合适的分词器，然后生成一个供查询的词项列表● 查询时候，先会对输入的查询进⾏分词，然后每个词项逐个进行底层的查询，最终将结果进行合并。并为每个⽂档⽣生成⼀个算分。 -例例如查 “Matrix reloaded”，会查到包括 Matrix 或者 reload的所有结果。","categories":[{"name":"spring cloud alibaba","slug":"spring-cloud-alibaba","permalink":"https://xxnjdg.github.io/categories/spring-cloud-alibaba/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://xxnjdg.github.io/tags/elasticsearch/"}]},{"title":"极客时间-Redis核心技术与实战笔记","slug":"redis-1","date":"2021-03-13T06:42:38.000Z","updated":"2021-03-16T04:54:23.941Z","comments":true,"path":"2021/03/13/redis-1/","link":"","permalink":"https://xxnjdg.github.io/2021/03/13/redis-1/","excerpt":"","text":"服务器初始化主流程（见图9-2） 可以简要分为7个步骤： ①初始化配置， 包括用户可配置的参数， 以及命令表的初始化； ②加载并解析配置文件； ③初始化服务端内部变量， 其中就包括数据库； ④创建事件循环eventLoop； ⑤创建socket并启动监听； ⑥创建文件事件与时间事件；⑦开启事件循环。 下面详细介绍步骤①～步骤④， 至于步骤⑤～步骤⑦将会在9.2.2节介绍 高性能主线，包括线程模型、数据结构、持久化、网络框架；高可靠主线，包括主从复制、哨兵机制；高可扩展主线，包括数据分片、负载均衡 Redis 能够在实际业务场景中得到广泛的应用，就是得益于支持多样化类型的 value 一般而言，内存键值数据库（例如 Redis）采用哈希表作为索引，很大一部分原因在于，其键值数据基本都是保存在内存中的，而内存的高性能随机访问特性可以很好地与哈希表O(1) 的操作复杂度相匹配。 一方面，这是因为它是内存数据库，所有操作都在内存上完成，内存的访问速度本身就很快。另一方面，这要归功于它的数据结构 为了实现从键到值的快速访问，Redis 使用了一个哈希表来保存所有键值对 因为这个哈希表保存了所有的键值对，所以，我也把它称为全局哈希表。哈希表的最大好处很明显，就是让我们可以用 O(1) 的时间复杂度来快速查找到键值对——我们只需要计算键的哈希值，就可以知道它所对应的哈希桶位置，然后就可以访问相应的 entry 元素 当你往 Redis 中写入大量数据后，就可能发现操作有时候会突然变慢了。这其实是因为你忽略了一个潜在的风险点，那就是哈希表的冲突问题和 rehash 可能带来的操作阻塞 Redis 解决哈希冲突的方式，就是链式哈希。链式哈希也很容易理解，就是指同一个哈希桶中的多个元素用一个链表来保存，它们之间依次用指针连接。 哈希冲突链上的元素只能通过指针逐一查找再操作。如果哈希表里写入的数据越来越多，哈希冲突可能也会越来越多，这就会导致某些哈希冲突链过长，进而导致这个链上的元素查找耗时长，效率降低 Redis 会对哈希表做 rehash 操作。rehash 也就是增加现有的哈希桶数量，让逐渐增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个桶中的冲突 Redis 开始执行 rehash，这个过程分为三步： 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍； 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中； 释放哈希表 1 的空间 但是第二步涉及大量的数据拷贝，如果一次性把哈希表 1 中的数据都迁移完，会造成 Redis 线程阻塞，无法服务其他请求 Redis 采用了渐进式 rehash。简单来说就是在第二步拷贝数据时，Redis 仍然正常处理客户端请求，每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中；等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的entries 压缩列表实际上类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同的是，压缩列表在表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量和列表中的 entry 个数；压缩列表在表尾还有一个 zlend，表示列表结束 在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段的长度直接定位，复杂度是 O(1)。而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O(N) 了 有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。具体来说，跳表在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位 可以看到，这个查找过程就是在多级索引上跳来跳去，最后定位到元素。这也正好符合“跳”表的叫法。当数据量很大时，跳表的查找复杂度就是 O(logN) 第一，单元素操作，是指每一种集合类型对单个数据实现的增删改查操作。例如，Hash 类型的 HGET、HSET 和 HDEL，Set 类型的 SADD、SREM、SRANDMEMBER 等。这些操作的复杂度由集合采用的数据结构决定，例如，HGET、HSET 和 HDEL 是对哈希表做操作，所以它们的复杂度都是 O(1)；Set 类型用哈希表作为底层数据结构时，它的 SADD、SREM、SRANDMEMBER 复杂度也是 O(1) 第二，范围操作，是指集合类型中的遍历操作，可以返回集合中的所有数据，比如 Hash类型的 HGETALL 和 Set 类型的 SMEMBERS，或者返回一个范围内的部分数据，比如 List类型的 LRANGE 和 ZSet 类型的 ZRANGE。这类操作的复杂度一般是 O(N)，比较耗时，我们应该尽量避免 不过，Redis 从 2.8 版本开始提供了 SCAN 系列操作（包括 HSCAN，SSCAN 和ZSCAN），这类操作实现了渐进式遍历，每次只返回有限数量的数据。这样一来，相比于HGETALL、SMEMBERS 这类操作来说，就避免了一次性返回所有元素而导致的 Redis 阻塞 第三，统计操作，是指集合类型对集合中所有元素个数的记录，例如 LLEN 和 SCARD。这类操作复杂度只有 O(1)，这是因为当集合类型采用压缩列表、双向链表、整数数组这些数据结构时，这些结构中专门记录了元素的个数统计，因此可以高效地完成相关操作 第四，例外情况，是指某些数据结构的特殊记录，例如压缩列表和双向链表都会记录表头和表尾的偏移量。这样一来，对于 List 类型的 LPOP、RPOP、LPUSH、RPUSH 这四个操作来说，它们是在列表的头尾增删元素，这就可以通过偏移量直接定位，所以它们的复杂度也只有 O(1)，可以实现快速操作集合类型的范围操作，因为要遍历底层数据结构，复杂度通常是 O(N)。这里，我的建议是：用其他命令来替代，例如可以用 SCAN 来代替，避免在 Redis 内部产生费时的全集合遍历操作双向链表和压缩列表的操作复杂度都是 O(N)。因此，我的建议是：因地制宜地使用 List 类型。例如，既然它的 POP/PUSH 效率很高，那么就将它主要用于 FIFO 队列场景，而不是作为一个可以随机读写的集合 至于问题答案，采用压缩列表或者是整数集合，都是数据量比较小的情况，所以一次能够分配到足够大的内存，而压缩列表和整数集合本身的数据结构也是线性的，对cpu的缓存更友好一些，所以真正的执行的时间因为高速缓存的关系，速度更快 我们通常说，Redis 是单线程，主要是指 Redis 的网络 IO和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程。但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的 Redis 为什么用单线程 系统中通常会存在被多线程同时访问的共享资源，比如一个共享的数据结构。当有多个线程要修改这个共享资源时，为了保证共享资源的正确性，就需要有额外的机制进行保证，而这个额外的机制，就会带来额外的开销 而且，采用多线程开发一般会引入同步原语来保护共享资源的并发访问，这也会降低系统代码的易调试性和可维护性。为了避免这些问题，Redis 直接采用了单线程模式 单线程 Redis 为什么那么快？ 一方面，Redis 的大部分操作在内存上完成，再加上它采用了高效的数据结构，例如哈希表和跳表，这是它实现高性能的一个重要原因。另一方面，就是 Redis 采用了多路复用机制，使其在网络 IO 操作中能并发处理大量的客户端请求，实现高吞吐率。接下来，我们就重点学习下多路复用机制 Redis 的持久化主要有两大机制，即 AOF 日志和 RDB 快照 AOF 日志正好相反，它是写后日志，“写后”的意思是 Redis 是先执行命令，把数据写入内存，然后才记录日志 而 AOF 里记录的是 Redis 收到的每一条命令，这些命令是以文本形式保存的 而写后日志这种方式，就是先让系统执行命令，只有命令能执行成功，才会被记录到日志中，否则，系统就会直接向客户端报错。所以，Redis 使用写后日志这一方式的一大好处是，可以避免出现记录错误命令的情况 AOF 还有一个好处：它是在命令执行后才记录日志，所以不会阻塞当前的写操作。 那么这个命令和相应的数 据就有丢失的风险 但可能会给下一个操作带来阻塞风险。这是因 为，AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就会导致写盘很慢 到这里，我们就可以根据系统对高性能和高可靠性的要求，来选择使用哪种写回策略了。总结一下就是：想要获得高性能，就选择 No 策略；如果想要得到高可靠性保证，就选择Always 策略；如果允许数据有一点丢失，又希望性能别受太大影响的话，那么就选择Everysec 策略 这里的“性能问题”，主要在于以下三个方面：一是，文件系统本身对文件大小有限制，无法保存过大的文件；二是，如果文件太大，之后再往里面追加命令记录的话，效率也会变低；三是，如果发生宕机，AOF 中记录的命令要一个个被重新执行，用于故障恢复，如果日志文件太大，整个恢复过程就会非常缓慢，这就会影响到 Redis 的正常使用 Redis 根据数据库的现状创建一个新的 AOF 文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入 和 AOF 日志由主线程写回不同，重写过程是由后台线程 bgrewriteaof 来完成的，这也是为了避免阻塞主线程，导致数据库性能下降 “一个拷贝”就是指，每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此时，fork 会把主线程的内存拷贝一份给 bgrewriteaof 子进程，这里面就包含了数据库的最新数据。然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志 因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的 AOF 日志，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复。而第二处日志，就是指新的 AOF 重写日志。这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以用新的 AOF 文件替代旧文件了 但是，也正因为记录的是操作命令，而不是实际的数据，所以，用 AOF 方法进行故障恢复的时候，需要逐一把操作日志都执行一遍。如果操作日志非常多，Redis 就会恢复得很缓慢，影响到正常使用 所谓内存快照，就是指内存中的数据在某一个时刻的状态记录。这就类似于照片，当你给朋友拍照时，一张照片就能把朋友一瞬间的形象完全记下来 和 AOF 相比，RDB 记录的是某一时刻的数据，并不是操作，所以，在做数据恢复时，我们可以直接把 RDB 文件读入内存，很快地完成恢复。听起来好像很不错，但内存快照也并不是最优选项。为什么这么说呢对哪些数据做快照？这关系到快照的执行效率问题；做快照时，数据还能被增删改吗？这关系到 Redis 是否被阻塞，能否同时正常处理请求Redis 的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是全量快照，也就是说，把内存中的所有数据都记录到磁盘中 Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsavesave：在主线程中执行，会导致阻塞；bgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是Redis RDB 文件生成的默认配置 所以这个时候，Redis 就会借助操作系统提供的写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作 简单来说，bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。此时，如果主线程对这些数据也都是读操作（例如图中的键值对 A），那么，主线程和bgsave 子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对 C），那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据 虽然 bgsave 执行时不阻塞主线程，但是，如果频繁地执行全量快照，也会带来两方面的开销 一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环 另一方面，bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了 虽然跟 AOF 相比，快照的恢复速度快，但是，快照的频率不好把握，如果频率太低，两次快照间一旦宕机，就可能有比较多的数据丢失。如果频率太高，又会产生额外开销，那么，还有什么方法既能利用 RDB 的快速恢复，又能以较小的开销做到尽量少丢数据呢 Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作 这样一来，快照不用很频繁地执行，这就避免了频繁 fork 对主线程的影响。而且，AOF日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销 实际上，Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分 离的方式读操作：主库、从库都可以接收；写操作：首先到主库执行，然后，主库将写操作同步给从库 并集SUNIONSTORE user:id user:id user:id:20200803差集SDIFFSTORE user:new user:id:20200804 user:id交集SINTERSTORE user:id:rem user:id:20200803 user:id:20200804 Set 的差集、并集和交集的计算复杂度较高，在数据量较大的情况下，如果直接执行这些计算，会导致 Redis 实例阻塞。所以，我给你分享一个小建议：你可以从主从集群中选择一个从库，让它专门负责聚合计算，或者是把数据读取到客户端，在客户端来完成聚合统计，这样就可以规避阻塞主库实例和其他从库实例的风险了 所以，在面对需要展示最新列表、排行榜等场景时，如果数据更新频繁或者需要分页显示，建议你优先考虑使用 Sorted Set 16 | 异步机制：如何避免单线程模型的阻塞？Redis 实例有哪些阻塞点？ 客户端：网络 IO，键值对增删改查操作，数据库操作磁盘：生成 RDB 快照，记录 AOF 日志，AOF 日志重写主从节点：主库生成、传输 RDB 文件，从库接收 RDB 文件、清空数据库、加载 RDB 文件切片集群实例：向其他实例传输哈希槽信息，数据迁移 和客户端交互时的阻塞点Redis 使用了 IO 多路复用机制，网络 IO 不是导致 Redis 阻塞的因素 复杂度高的增删改查操作肯定会阻塞 Redis 这里有一个最基本的标准，就是看操作的复杂度是否为 O(N) Redis 中涉及集合的操作复杂度通常为 O(N)，我们要在使用时重视起来。例如集合元素全量查询操作 HGETALL、SMEMBERS，以及集合的聚合统计操作，例如求交、并和差集。这些操作可以作为 Redis 的第一个阻塞点：集合全量查询和聚合操作 其实，删除操作的本质是要释放键值对占用的内存空间。你可不要小瞧内存的释放过程。释放内存只是第一步，为了更加高效地管理内存空间，在应用程序释放内存时，操作系统需要把释放掉的内存块插入一个空闲内存块的链表，以便后续进行管理和再分配。这个过程本身需要一定时间，而且会阻塞当前释放内存的应用程序，所以，如果一下子释放了大量内存，空闲内存块链表操作时间就会增加，相应地就会造成 Redis 主线程的阻塞 那么，什么时候会释放大量内存呢？其实就是在删除大量键值对数据的时候，最典型的就是删除包含了大量元素的集合，也称为 bigkey 删除 bigkey 删除操作就是 Redis 的第二个阻塞点 清空数据库（例如 FLUSHDB 和 FLUSHALL 操作）必然也是一个潜在的阻塞风险，因为它涉及到删除和释放所有的键值对。所以，这就是 Redis 的第三个阻塞点：清空数据库 和磁盘交互时的阻塞点 Redis 直接记录 AOF 日志时，会根据不同的写回策略对数据做落盘保存。一个同步写磁盘的操作的耗时大约是 1～2ms，如果有大量的写操作需要记录在 AOF 日志中，并同步写回的话，就会阻塞主线程了。这就得到了 Redis 的第四个阻塞点了：AOF 日志同步写。 主从节点交互时的阻塞点 在主从集群中，主库需要生成 RDB 文件，并传输给从库。主库在复制的过程中，创建和传输 RDB 文件都是由子进程来完成的，不会阻塞主线程。但是，对于从库来说，它在接收了RDB 文件后，需要使用 FLUSHDB 命令清空当前数据库，这就正好撞上了刚才我们分析的第三个阻塞点。此外，从库在清空当前数据库后，还需要把 RDB 文件加载到内存，这个过程的快慢和RDB 文件的大小密切相关，RDB 文件越大，加载过程越慢，所以，加载 RDB 文件就成为了 Redis 的第五个阻塞点。 切片集群实例交互时的阻塞点 不过，哈希槽的信息量不大，而数据迁移是渐进式执行的，所以，一般来说，这两类操作对 Redis 主线程的阻塞风险不大 当没有 bigkey 时，切片集群的各实例在进行交互时不会阻塞主线程，就可以了 集合全量查询和聚合操作；bigkey 删除；清空数据库；AOF 日志同步写；从库加载 RDB 文件 所谓的异步线程机制，就是指，Redis 会启动一些子线程，然后把一些任务交给这些子线程，让它们在后台完成，而不再由主线程来执行这些任务。使用异步线程机制执行操作，可以避免阻塞主线程 哪些阻塞点可以异步执行？ 如果一个操作能被异步执行，就意味着，它并不是 Redis 主线程的关键路径上的操作 对于 Redis 来说，读操作是典型的关键路径操作，因为客户端发送了读操作之后，就会等待读取的数据返回，以便进行后续的数据处理。而 Redis 的第一个阻塞点“集合全量查询和聚合操作”都涉及到了读操作，所以，它们是不能进行异步操作了 我们再来看看删除操作。删除操作并不需要给客户端返回具体的数据结果，所以不算是关键路径操作。而我们刚才总结的第二个阻塞点“bigkey 删除”，和第三个阻塞点“清空数据库”，都是对数据做删除，并不在关键路径上。因此，我们可以使用后台子线程来异步执行删除操作。 对于第四个阻塞点“AOF 日志同步写”来说，为了保证数据可靠性，Redis 实例需要保证AOF 日志中的操作记录已经落盘，这个操作虽然需要实例等待，但它并不会返回具体的数据结果给实例。所以，我们也可以启动一个子线程来执行 AOF 日志的同步写，而不用让主线程等待 AOF 日志的写完成 最后，我们再来看下“从库加载 RDB 文件”这个阻塞点。从库要想对客户端提供数据存取服务，就必须把 RDB 文件加载完成。所以，这个操作也属于关键路径上的操作，我们必须让从库的主线程来执行 异步的子线程机制会使用操作系统提供的 pthread_create 函数创建 3 个子线程，分别由它们负责 AOF 日志写操作、键值对删除以及文件关闭的异步执行 主线程通过一个链表形式的任务队列和子线程进行交互。当收到键值对删除和清空数据库的操作时，主线程会把这个操作封装成一个任务，放入到任务队列中，然后给客户端返回一个完成信息，表明删除已经完成 但实际上，这个时候删除还没有执行，等到后台子线程从任务队列中读取任务后，才开始实际删除键值对，并释放相应的内存空间。因此，我们把这种异步删除也称为惰性删除（lazy free） 和惰性删除类似，当 AOF 日志配置成 everysec 选项后，主线程会把 AOF 写日志操作封装成一个任务，也放到任务队列中。后台子线程读取任务后，开始自行写入 AOF 日志，这样主线程就不用一直等待 AOF 日志写完了 这里有个地方需要你注意一下，异步的键值对删除和数据库清空操作是 Redis 4.0 后提供的功能，Redis 也提供了新的命令来执行这两个操作 键值对删除：当你的集合类型中有大量元素（例如有百万级别或千万级别元素）需要删除时，我建议你使用 UNLINK 命令。清空数据库：可以在 FLUSHDB 和 FLUSHALL 命令后加上 ASYNC 选项，这样就可以让后台子线程异步地清空数据库 不过，异步删除操作是 Redis 4.0 以后才有的功能，如果你使用的是 4.0 之前的版本，当你遇到 bigkey 删除时，我给你个小建议：先使用集合类型提供的 SCAN 命令读取数据，然后再进行删除。因为用 SCAN 命令可以每次只读取一部分数据并进行删除，这样可以避免一次性删除大量 key 给主线程带来的阻塞。例如，对于 Hash 类型的 bigkey 删除，你可以使用 HSCAN 命令，每次从 Hash 集合中获取一部分键值对（例如 200 个），再使用 HDEL 删除这些键值对，这样就可以把删除压力分摊到多次操作中，那么，每次删除操作的耗时就不会太长，也就不会阻塞主线程了。最后，我想再提一下，集合全量查询和聚合操作、从库加载 RDB 文件是在关键路径上，无法使用异步操作来完成。对于这两个阻塞点，我也给你两个小建议 集合全量查询和聚合操作：可以使用 SCAN 命令，分批读取数据，再在客户端进行聚合计算；从库加载 RDB 文件：把主库的数据量大小控制在 2~4GB 左右，以保证 RDB 文件能以较快的速度加载 17 | 为什么CPU结构也会影响Redis的性能？L1、L2 缓存中的指令和数据的访问速度很快，所以，充分利用 L1、L2 缓存，可以有效缩短应用程序的执行时间；在 NUMA 架构下，如果应用程序从一个 Socket 上调度到另一个 Socket 上，就可能会出现远端内存访问的情况，这会直接增加应用程序的执行时间 我们先简单地总结下刚刚学习的内容。在 CPU 多核的场景下，用 taskset 命令把 Redis 实例和一个核绑定，可以减少 Redis 实例在不同核上被来回调度执行的开销，避免较高的尾延迟；在多 CPU 的 NUMA 架构下，如果你对网络中断程序做了绑核操作，建议你同时把Redis 实例和网络中断程序绑在同一个 CPU Socket 的不同核上，这样可以避免 Redis 跨Socket 访问内存中的网络数据的时间开销。不过，“硬币都是有两面的”，绑核也存在一定的风险。接下来，我们就来了解下它的潜在风险点和解决方案","categories":[{"name":"cache","slug":"cache","permalink":"https://xxnjdg.github.io/categories/cache/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://xxnjdg.github.io/tags/redis/"}]},{"title":"namesrv笔记","slug":"rocketmq-1","date":"2021-02-15T07:27:27.000Z","updated":"2021-03-03T12:19:17.496Z","comments":true,"path":"2021/02/15/rocketmq-1/","link":"","permalink":"https://xxnjdg.github.io/2021/02/15/rocketmq-1/","excerpt":"","text":"org.apache.rocketmq.namesrv.NamesrvStartup程序入口 定时任务1234567891011121314151617//定时扫描不可用的broker，同时删除不可用的broker，同时打印相关日志 this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.routeInfoManager.scanNotActiveBroker(); &#125; &#125;, 5, 10, TimeUnit.SECONDS); //定时将kv的配置信息输出到info日志中 this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.kvConfigManager.printAllPeriodically(); &#125; &#125;, 1, 10, TimeUnit.MINUTES); requestHeader.setBrokerAddr(brokerAddr);requestHeader.setBrokerId(brokerId);requestHeader.setBrokerName(brokerName);requestHeader.setClusterName(clusterName);requestHeader.setHaServerAddr(haServerAddr);requestHeader.setCompressed(compressed);requestHeader.setBodyCrc32(bodyCrc32); requestBody.setTopicConfigSerializeWrapper(topicConfigWrapper);requestBody.setFilterServerList(filterServerList); Broker定时任务123456789101112//每30秒循环 Broker 发送心跳包this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; try &#123; BrokerController.this.registerBrokerAll(true, false, brokerConfig.isForceRegister()); &#125; catch (Throwable e) &#123; log.error(&quot;registerBrokerAll Exception&quot;, e); &#125; &#125;&#125;, 1000 * 10, Math.max(10000, Math.min(brokerConfig.getRegisterNameServerPeriod(), 60000)), TimeUnit.MILLISECONDS); 消息队列如何进行负载 ？消息发送如何实现高可用 ？批量消息发送如何实现一致性？2 ）消息队列负载机制消息生产者在发送消息时，如果本地路由表中未缓存 topic 的路由信息，向 NameServer 发送获取路由信息请求，更新本地路由信息表，并且消息生产者每隔 30s 从 NameServer 更新路由表 。3 ）消息发送异常机制消息发送高可用主要通过两个手段 ： 重试与 Broker 规避 。 Brok巳r 规避就是在一次消息发送过程中发现错误，在某一时间段内，消息生产者不会选择该 Broker（消息服务器）上的消息队列，提高发送消息的成功率 。4 ）批量消息发送RocketMQ 支持将 同一主题下 的多条消息一次性发送到消息服务端 。 SendMessageRequestHeader requestHeader = new SendMessageRequestHeader();//消息发送组requestHeader.setProducerGroup(this.defaultMQProducer.getProducerGroup());//消息的topicrequestHeader.setTopic(msg.getTopic());//消息的默认topicrequestHeader.setDefaultTopic(this.defaultMQProducer.getCreateTopicKey());//消息的默认queue数量requestHeader.setDefaultTopicQueueNums(this.defaultMQProducer.getDefaultTopicQueueNums());//消息发送的queueidrequestHeader.setQueueId(mq.getQueueId());//特殊标识requestHeader.setSysFlag(sysFlag);//消息创建的时间戳requestHeader.setBornTimestamp(System.currentTimeMillis());//标识requestHeader.setFlag(msg.getFlag());//消息的扩展属性requestHeader.setProperties(MessageDecoder.messageProperties2String(msg.getProperties()));//消息的消费requestHeader.setReconsumeTimes(0);//模型requestHeader.setUnitMode(this.isUnitMode());//批量requestHeader.setBatch(msg instanceof MessageBatch); response.setOpaque(request.getOpaque());response.addExtField(MessageConst.PROPERTY_MSG_REGION, this.brokerController.getBrokerConfig().getRegionId());response.addExtField(MessageConst.PROPERTY_TRACE_SWITCH, String.valueOf(this.brokerController.getBrokerConfig().isTraceOn()));response.setCode(ResponseCode.SUCCESS);response.setRemark(null);responseHeader.setMsgId(putMessageResult.getAppendMessageResult().getMsgId());responseHeader.setQueueId(queueIdInt);responseHeader.setQueueOffset(putMessageResult.getAppendMessageResult().getLogicsOffset()); MessageAccessor.putProperty(msgInner, MessageConst.PROPERTY_CLUSTER, clusterName); MessageAccessor.putProperty(msgExt, MessageConst.PROPERTY_RETRY_TOPIC, msgExt.getTopic());this.putProperty(MessageConst.PROPERTY_WAIT_STORE_MSG_OK, false);this.putProperty(MessageConst.PROPERTY_DELAY_TIME_LEVEL, 3);putProperty(msg, MessageConst.PROPERTY_ORIGIN_MESSAGE_ID, msgExt.getMsgId());MessageAccessor.putProperty(msg, MessageConst.PROPERTY_REAL_TOPIC, msg.getTopic());MessageAccessor.putProperty(msg, MessageConst.PROPERTY_REAL_QUEUE_ID, String.valueOf(msg.getQueueId()));topic = TopicValidator.RMQ_SYS_SCHEDULE_TOPIC;queueId = ScheduleMessageService.delayLevel2QueueId(msg.getDelayTimeLevel()); 1 ) TOTALSIZE ： 该消息条目总长度 ， 4 字节 。2 ) MAGICCODE ： 魔数， 4 字节 。 固定值 Oxdaa320a7 。3 ) BODYCRC ： 消息体 ere 校验码， 4 字节 。4 ) QUEUEID ： 消息消费队列 ID , 4 字节 。5 ) FLAG ： 消息 FLAG , RocketMQ 不做处理 ， 供应用程序使用，默认 4 字节 。6 ) QUEUEOFFSET ：消息在消息消费队列的偏移量 ， 8 字节 。7 ) PHYSICALOFFSET ： 消息在 CommitLog 文件中的偏移量 ， 8 字节 。8 ) SYSFLAG ： 消息系统 Flag ，例如是否压缩 、 是否是事务消息等 ， 4 字节 。9 ) BORNTIMESTAMP ： 消息生产者调用消息发送 API 的时间戳， 8 字节 。10 ) BORNHOST ：消息发送者 IP 、端 口 号， 8 字节 。11 ) STORETIMESTAMP ： 消息存储时间戳， 8 字节 。12 ) STOREHOSTADDRESS: Broker 服务器 IP＋ 端 口 号， 8 字节 。13 ） 阻CONSUMETIMES ： 消息重试次数， 4 字节 。14 ) Prepared Transaction Offset ： 事务消息物理偏移量 ， 8 字节 。15 ) BodyLength ：消息体长度， 4 字节 。16 ) Body ： 消息体内容，长度为 bodyLenth 中存储的值。17 ) TopieLength ： 主题存储长度， 1 字节 ，表示主题名称不能超过 255 个字符 。18) Topie ： 主题，长度为 TopieLength 中存储的值。 19 ) PropertiesLength ： 消息属性长度 ， 2 字节 ， 表示消息属性长度不能超过 6 553 6 个 字符 。 20 ) Properties ： 消息属性，长度为 PropertiesLength 中存储的值 。 DLedgerMmapFileStore.AppendHook appendHook = (entry, buffer, bodyOffset) -&gt; {assert bodyOffset == DLedgerEntry.BODY_OFFSET;buffer.position(buffer.position() + bodyOffset + MessageDecoder.PHY_POS_POSITION);buffer.putLong(entry.getPos() + bodyOffset);};dLedgerFileStore.addAppendHook(appendHook); org.apache.rocketmq.common.namesrv.NamesrvConfig 配置项 key 默认值 说明 rocketmq 主目录 rocketmqHome System.getProperty(MixAll.ROCKETMQ_HOME_PROPERTY, System.getenv(MixAll.ROCKETMQ_HOME_ENV)) 可以通过 -Drocketmq.home.dir=path 或通过设置环境变量 ROCKETMQ_HOME 来配置 RocketMQ 的主目录 NameServer存储 KV 配置属性的持久化路径 kvConfigPath System.getProperty(\"user.home\") + File.separator + \"namesrv\" + File.separator + \"kvConfig.json\" NameServer 存储 KV 配置属性的持久化路径 NameServer 默认配置文件路径 configStorePath System.getProperty(\"user.home\") + File.separator + \"namesrv\" + File.separator + \"namesrv.properties\" NameServer 默认配置文件路径,不生效. NameServer 启动时如果要通过配置文件配置 NameServer 启动属性的话，请使用 -c 选项 productEnvName center 集群测试 clusterTest false 是否开启集群测试 顺序消息 orderMessageEnable false 是否支持顺序消息，默认是不支持 org.apache.rocketmq.remoting.netty.NettyServerConfig 配置项 默认值类型 默认值 说明 listenPort int 8888 服务端监听端口，NameServer 监昕端口，该值默认会被初始化为 9876 serverWorkerThreads int 8 Netty 业务线程池线程个数 serverCallbackExecutorThreads int 0 Netty public 任务线程池线程个数，Netty 网络设计，根据业务类型会创建不同的线程池，比如处理消息发送、消息消费、心跳检测等。如果该业务类型（RequestCode）未注册线程池，则由 public 线程池执行 serverSelectorThreads int 3 IO线程池线程个数，主要是 NameServer 、 Broker 端解析请求、返回相应的线程个数，这类线程主要是处理网络请求的，解析请求包，然后转发到各个业务线程池完成具体的业务操作，然后将结果再返回调用方 serverOnewaySemaphoreValue int 256 send oneway 消息请求井发度（ Broker 端参数） serverAsyncSemaphoreValue int 64 异步消息发送最大并发度（ Broker 端参数） serverChannelMaxIdleTimeSeconds int 120 网络连接最大空闲时间，默认120s。如果连接空闲时间超过该参数设置的值，连接将被关闭 serverSocketSndBufSize int 65535 网络 socket 发送缓存区大小，默认64k serverSocketRcvBufSize int 65535 网络 socket 接收缓存区大小，默认64k serverPooledByteBufAllocatorEnable boolean true ByteBuffer 是否开启缓存，建议开启 useEpollNativeSelector boolean false 是否启用 EpollIO 模型，Linux 环境建议开启 rocketmqHome class java.lang.String System.getProperty(MixAll.ROCKETMQ_HOME_PROPERTY, System.getenv(MixAll.ROCKETMQ_HOME_ENV)) namesrvAddr class java.lang.String System.getProperty(MixAll.NAMESRV_ADDR_PROPERTY, System.getenv(MixAll.NAMESRV_ADDR_ENV)) brokerIP1 class java.lang.String RemotingUtil.getLocalAddress() brokerIP2 class java.lang.String RemotingUtil.getLocalAddress() brokerName class java.lang.String localHostName() brokerClusterName class java.lang.String DefaultCluster brokerId long 1 brokerPermission int 1 defaultTopicQueueNums int 8 autoCreateTopicEnable boolean true clusterTopicEnable boolean true brokerTopicEnable boolean true autoCreateSubscriptionGroup boolean true messageStorePlugIn class java.lang.String msgTraceTopicName class java.lang.String TopicValidator.RMQ_SYS_TRACE_TOPIC traceTopicEnable boolean false sendMessageThreadPoolNums int 1 pullMessageThreadPoolNums int 32 processReplyMessageThreadPoolNums int 32 queryMessageThreadPoolNums int 16 adminBrokerThreadPoolNums int 16 clientManageThreadPoolNums int 32 consumerManageThreadPoolNums int 32 heartbeatThreadPoolNums int 8 endTransactionThreadPoolNums int 24 flushConsumerOffsetInterval int 5000 flushConsumerOffsetHistoryInterval int 60000 rejectTransactionMessage boolean false fetchNamesrvAddrByAddressServer boolean false sendThreadPoolQueueCapacity int 10000 pullThreadPoolQueueCapacity int 100000 replyThreadPoolQueueCapacity int 10000 queryThreadPoolQueueCapacity int 20000 clientManagerThreadPoolQueueCapacity int 1000000 consumerManagerThreadPoolQueueCapacity int 1000000 heartbeatThreadPoolQueueCapacity int 50000 endTransactionPoolQueueCapacity int 100000 filterServerNums int 0 longPollingEnable boolean true shortPollingTimeMills long 1000 notifyConsumerIdsChangedEnable boolean true highSpeedMode boolean false commercialEnable boolean true commercialTimerCount int 1 commercialTransCount int 1 commercialBigCount int 1 commercialBaseCount int 1 transferMsgByHeap boolean true maxDelayTime int 40 regionId class java.lang.String MixAll.DEFAULT_TRACE_REGION_ID registerBrokerTimeoutMills int 6000 slaveReadEnable boolean false disableConsumeIfConsumerReadSlowly boolean false consumerFallbehindThreshold long 17179869184 brokerFastFailureEnable boolean true waitTimeMillsInSendQueue long 200 waitTimeMillsInPullQueue long 5000 waitTimeMillsInHeartbeatQueue long 31000 waitTimeMillsInTransactionQueue long 3000 startAcceptSendRequestTimeStamp long 0 traceOn boolean true enableCalcFilterBitMap boolean false expectConsumerNumUseFilter int 32 maxErrorRateOfBloomFilter int 20 filterDataCleanTimeSpan long 86400000 filterSupportRetry boolean false enablePropertyFilter boolean false compressedRegister boolean false forceRegister boolean true registerNameServerPeriod int 30000 transactionTimeOut long 6000 transactionCheckMax int 15 transactionCheckInterval long 60000 aclEnable boolean false storeReplyMessageEnable boolean true autoDeleteUnusedStats boolean false","categories":[{"name":"mq","slug":"mq","permalink":"https://xxnjdg.github.io/categories/mq/"}],"tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"https://xxnjdg.github.io/tags/rocketmq/"}]},{"title":"nacos注册中心集群笔记2","slug":"nacos-5","date":"2021-02-10T03:10:57.000Z","updated":"2021-02-14T14:07:40.099Z","comments":true,"path":"2021/02/10/nacos-5/","link":"","permalink":"https://xxnjdg.github.io/2021/02/10/nacos-5/","excerpt":"","text":"nacos 版本 1.4.1 DistroProtocol12345678910111213141516@Componentpublic class DistroProtocol &#123; private void startDistroTask() &#123; if (EnvUtil.getStandaloneMode()) &#123; isInitialized = true; return; &#125; startVerifyTask(); startLoadTask(); &#125; private void startVerifyTask() &#123; GlobalExecutor.schedulePartitionDataTimedSync(new DistroVerifyTask(memberManager, distroComponentHolder), distroConfig.getVerifyIntervalMillis()); &#125;&#125; 定时任务每5秒循环执行DistroVerifyTask 每5秒广播向其他Server发送 Datum checksum，其他 Server 如果发现你的 checksum 和我的不一样，就会请求源 Server 获取新数据，获取成功后更新本地 Datum 和本地实例表 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class DistroVerifyTask implements Runnable &#123; private final ServerMemberManager serverMemberManager; private final DistroComponentHolder distroComponentHolder; public DistroVerifyTask(ServerMemberManager serverMemberManager, DistroComponentHolder distroComponentHolder) &#123; this.serverMemberManager = serverMemberManager; this.distroComponentHolder = distroComponentHolder; &#125; @Override public void run() &#123; try &#123; List&lt;Member&gt; targetServer = serverMemberManager.allMembersWithoutSelf(); if (Loggers.DISTRO.isDebugEnabled()) &#123; Loggers.DISTRO.debug(&quot;server list is: &#123;&#125;&quot;, targetServer); &#125; for (String each : distroComponentHolder.getDataStorageTypes()) &#123; verifyForDataStorage(each, targetServer); &#125; &#125; catch (Exception e) &#123; Loggers.DISTRO.error(&quot;[DISTRO-FAILED] verify task failed.&quot;, e); &#125; &#125; private void verifyForDataStorage(String type, List&lt;Member&gt; targetServer) &#123; DistroData distroData = distroComponentHolder.findDataStorage(type).getVerifyData(); if (null == distroData) &#123; return; &#125; distroData.setType(DataOperation.VERIFY); //广播 for (Member member : targetServer) &#123; try &#123; distroComponentHolder.findTransportAgent(type).syncVerifyData(distroData, member.getAddress()); &#125; catch (Exception e) &#123; Loggers.DISTRO.error(String .format(&quot;[DISTRO-FAILED] verify data for type %s to %s failed.&quot;, type, member.getAddress()), e); &#125; &#125; &#125;&#125;public class DistroDataStorageImpl implements DistroDataStorage &#123; @Override public DistroData getVerifyData() &#123; Map&lt;String, String&gt; keyChecksums = new HashMap&lt;&gt;(64); for (String key : dataStore.keys()) &#123; //不处理不属于当前Server的Service if (!distroMapper.responsible(KeyBuilder.getServiceName(key))) &#123; continue; &#125; Datum datum = dataStore.get(key); if (datum == null) &#123; continue; &#125; //计算 Datum checksum keyChecksums.put(key, datum.value.getChecksum()); &#125; if (keyChecksums.isEmpty()) &#123; return null; &#125; DistroKey distroKey = new DistroKey(&quot;checksum&quot;, KeyBuilder.INSTANCE_LIST_KEY_PREFIX); return new DistroData(distroKey, ApplicationUtils.getBean(Serializer.class).serialize(keyChecksums)); &#125;&#125;public class DistroHttpAgent implements DistroTransportAgent &#123; @Override public boolean syncVerifyData(DistroData verifyData, String targetServer) &#123; if (!memberManager.hasMember(targetServer)) &#123; return true; &#125; // put 请求 /v1/ns/distro/checksum NamingProxy.syncCheckSums(verifyData.getContent(), targetServer); return true; &#125;&#125; DistroConsistencyServiceImpl默认情况下，延时1秒后把数据广播同步到其他Server，如果发送给某了Server失败了,那么会重试，延时设置为5秒后 key = 代表Service key,在这1秒内，无论这个Service下实例发生了什么变化，也都会等到1秒后延时执行同步，并不是实例发生了改变就立即同步 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177@DependsOn(&quot;ProtocolManager&quot;)@org.springframework.stereotype.Service(&quot;distroConsistencyService&quot;)public class DistroConsistencyServiceImpl implements EphemeralConsistencyService, DistroDataProcessor &#123; @Override public void put(String key, Record value) throws NacosException &#123; //将注册实例更新到内存注册表中 onPut(key, value); //复制新数据到服务集群中 distroProtocol.sync(new DistroKey(key, KeyBuilder.INSTANCE_LIST_KEY_PREFIX), DataOperation.CHANGE, globalConfig.getTaskDispatchPeriod() / 2); &#125;&#125;@Componentpublic class DistroProtocol &#123; public void sync(DistroKey distroKey, DataOperation action, long delay) &#123; //广播 for (Member each : memberManager.allMembersWithoutSelf()) &#123; DistroKey distroKeyWithTarget = new DistroKey(distroKey.getResourceKey(), distroKey.getResourceType(), each.getAddress()); //构造 DistroDelayTask DistroDelayTask distroDelayTask = new DistroDelayTask(distroKeyWithTarget, action, delay); //加入 distroTaskEngineHolder 中 distroTaskEngineHolder.getDelayTaskExecuteEngine().addTask(distroKeyWithTarget, distroDelayTask); if (Loggers.DISTRO.isDebugEnabled()) &#123; Loggers.DISTRO.debug(&quot;[DISTRO-SCHEDULE] &#123;&#125; to &#123;&#125;&quot;, distroKey, each.getAddress()); &#125; &#125; &#125;&#125;public class NacosDelayTaskExecuteEngine extends AbstractNacosTaskExecuteEngine&lt;AbstractDelayTask&gt; &#123; @Override public void addTask(Object key, AbstractDelayTask newTask) &#123; lock.lock(); try &#123; AbstractDelayTask existTask = tasks.get(key); if (null != existTask) &#123; //任务还在延时中 newTask.merge(existTask); &#125; //放入 tasks tasks.put(key, newTask); &#125; finally &#123; lock.unlock(); &#125; &#125; protected void processTasks() &#123; Collection&lt;Object&gt; keys = getAllTaskKeys(); for (Object taskKey : keys) &#123; AbstractDelayTask task = removeTask(taskKey); if (null == task) &#123; continue; &#125; NacosTaskProcessor processor = getProcessor(taskKey); if (null == processor) &#123; getEngineLog().error(&quot;processor not found for task, so discarded. &quot; + task); continue; &#125; try &#123; // ReAdd task if process failed if (!processor.process(task)) &#123; retryFailedTask(taskKey, task); &#125; &#125; catch (Throwable e) &#123; getEngineLog().error(&quot;Nacos task execute error : &quot; + e.toString(), e); retryFailedTask(taskKey, task); &#125; &#125; &#125; @Override public AbstractDelayTask removeTask(Object key) &#123; lock.lock(); try &#123; AbstractDelayTask task = tasks.get(key); //shouldProcess 会延迟指定时间间隔才能移除 if (null != task &amp;&amp; task.shouldProcess()) &#123; return tasks.remove(key); &#125; else &#123; return null; &#125; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;public class DistroDelayTask extends AbstractDelayTask &#123; @Override public void merge(AbstractDelayTask task) &#123; if (!(task instanceof DistroDelayTask)) &#123; return; &#125; DistroDelayTask newTask = (DistroDelayTask) task; if (!action.equals(newTask.getAction()) &amp;&amp; createTime &lt; newTask.getCreateTime()) &#123; action = newTask.getAction(); createTime = newTask.getCreateTime(); &#125; //修改 lastProcessTime setLastProcessTime(newTask.getLastProcessTime()); &#125;&#125;public class DistroDelayTaskProcessor implements NacosTaskProcessor &#123; @Override public boolean process(NacosTask task) &#123; if (!(task instanceof DistroDelayTask)) &#123; return true; &#125; DistroDelayTask distroDelayTask = (DistroDelayTask) task; DistroKey distroKey = distroDelayTask.getDistroKey(); if (DataOperation.CHANGE.equals(distroDelayTask.getAction())) &#123; DistroSyncChangeTask syncChangeTask = new DistroSyncChangeTask(distroKey, distroComponentHolder); distroTaskEngineHolder.getExecuteWorkersManager().addTask(distroKey, syncChangeTask); return true; &#125; return false; &#125;&#125;public class DistroSyncChangeTask extends AbstractDistroExecuteTask &#123; @Override public void run() &#123; Loggers.DISTRO.info(&quot;[DISTRO-START] &#123;&#125;&quot;, toString()); try &#123; String type = getDistroKey().getResourceType(); //从 DataStore 获取数据 DistroData distroData = distroComponentHolder.findDataStorage(type).getDistroData(getDistroKey()); distroData.setType(DataOperation.CHANGE); boolean result = distroComponentHolder.findTransportAgent(type).syncData(distroData, getDistroKey().getTargetServer()); if (!result) &#123; //如果失败 handleFailedTask(); &#125; Loggers.DISTRO.info(&quot;[DISTRO-END] &#123;&#125; result: &#123;&#125;&quot;, toString(), result); &#125; catch (Exception e) &#123; Loggers.DISTRO.warn(&quot;[DISTRO] Sync data change failed.&quot;, e); //如果失败 handleFailedTask(); &#125; &#125; private void handleFailedTask() &#123; String type = getDistroKey().getResourceType(); DistroFailedTaskHandler failedTaskHandler = distroComponentHolder.findFailedTaskHandler(type); if (null == failedTaskHandler) &#123; Loggers.DISTRO.warn(&quot;[DISTRO] Can&#x27;t find failed task for type &#123;&#125;, so discarded&quot;, type); return; &#125; failedTaskHandler.retry(getDistroKey(), DataOperation.CHANGE); &#125;&#125;public class DistroHttpAgent implements DistroTransportAgent &#123; @Override public boolean syncData(DistroData data, String targetServer) &#123; if (!memberManager.hasMember(targetServer)) &#123; return true; &#125; byte[] dataContent = data.getContent(); // put 请求 /v1/ns/distro/datum return NamingProxy.syncData(dataContent, data.getDistroKey().getTargetServer()); &#125;&#125;public class DistroHttpCombinedKeyTaskFailedHandler implements DistroFailedTaskHandler &#123; @Override public void retry(DistroKey distroKey, DataOperation action) &#123; DistroHttpCombinedKey combinedKey = (DistroHttpCombinedKey) distroKey; for (String each : combinedKey.getActualResourceTypes()) &#123; DistroKey newKey = new DistroKey(each, KeyBuilder.INSTANCE_LIST_KEY_PREFIX, distroKey.getTargetServer()); //默认设置延时5秒后进行集群同步 DistroDelayTask newTask = new DistroDelayTask(newKey, action, globalConfig.getSyncRetryDelay()); distroTaskEngineHolder.getDelayTaskExecuteEngine().addTask(newKey, newTask); &#125; &#125;&#125; /v1/ns/distro/checksum123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224@RestController@RequestMapping(UtilsAndCommons.NACOS_NAMING_CONTEXT + &quot;/distro&quot;)public class DistroController &#123;@PutMapping(&quot;/checksum&quot;) public ResponseEntity syncChecksum(@RequestParam String source, @RequestBody Map&lt;String, String&gt; dataMap) &#123; DistroHttpData distroHttpData = new DistroHttpData(createDistroKey(source), dataMap); distroProtocol.onVerify(distroHttpData); return ResponseEntity.ok(&quot;ok&quot;); &#125;&#125;@Componentpublic class DistroProtocol &#123; public boolean onReceive(DistroData distroData) &#123; String resourceType = distroData.getDistroKey().getResourceType(); DistroDataProcessor dataProcessor = distroComponentHolder.findDataProcessor(resourceType); if (null == dataProcessor) &#123; Loggers.DISTRO.warn(&quot;[DISTRO] Can&#x27;t find data process for received data &#123;&#125;&quot;, resourceType); return false; &#125; return dataProcessor.processData(distroData); &#125;&#125;@DependsOn(&quot;ProtocolManager&quot;)@org.springframework.stereotype.Service(&quot;distroConsistencyService&quot;)public class DistroConsistencyServiceImpl implements EphemeralConsistencyService, DistroDataProcessor &#123; @Override public boolean processVerifyData(DistroData distroData) &#123; DistroHttpData distroHttpData = (DistroHttpData) distroData; //源Server地址 String sourceServer = distroData.getDistroKey().getResourceKey(); //Datum checksum Map&lt;String, String&gt; verifyData = (Map&lt;String, String&gt;) distroHttpData.getDeserializedContent(); onReceiveChecksums(verifyData, sourceServer); return true; &#125; /** * Check sum when receive checksums request. * * @param checksumMap map of checksum * @param server source server request checksum */ public void onReceiveChecksums(Map&lt;String, String&gt; checksumMap, String server) &#123; if (syncChecksumTasks.containsKey(server)) &#123; // Already in process of this server: Loggers.DISTRO.warn(&quot;sync checksum task already in process with &#123;&#125;&quot;, server); return; &#125; syncChecksumTasks.put(server, &quot;1&quot;); try &#123; List&lt;String&gt; toUpdateKeys = new ArrayList&lt;&gt;(); List&lt;String&gt; toRemoveKeys = new ArrayList&lt;&gt;(); for (Map.Entry&lt;String, String&gt; entry : checksumMap.entrySet()) &#123; //不应该处理属于当前Server处理的key if (distroMapper.responsible(KeyBuilder.getServiceName(entry.getKey()))) &#123; // this key should not be sent from remote server: Loggers.DISTRO.error(&quot;receive responsible key timestamp of &quot; + entry.getKey() + &quot; from &quot; + server); // abort the procedure: return; &#125; //本地 dataStore 不含这个key //本地 dataStore 对应 key 没有实例数据 //本地 dataStore 对应 key 的实例数据发送变化 if (!dataStore.contains(entry.getKey()) || dataStore.get(entry.getKey()).value == null || !dataStore .get(entry.getKey()).value.getChecksum().equals(entry.getValue())) &#123; toUpdateKeys.add(entry.getKey()); &#125; &#125; for (String key : dataStore.keys()) &#123; if (!server.equals(distroMapper.mapSrv(KeyBuilder.getServiceName(key)))) &#123; continue; &#125; //找到属于源 server 的 key //key 不在 checksumMap，说明源数据删除了 if (!checksumMap.containsKey(key)) &#123; toRemoveKeys.add(key); &#125; &#125; Loggers.DISTRO .info(&quot;to remove keys: &#123;&#125;, to update keys: &#123;&#125;, source: &#123;&#125;&quot;, toRemoveKeys, toUpdateKeys, server); for (String key : toRemoveKeys) &#123; //当前Server也要跟着删除 onRemove(key); &#125; //没有数据更新 if (toUpdateKeys.isEmpty()) &#123; return; &#125; try &#123; DistroHttpCombinedKey distroKey = new DistroHttpCombinedKey(KeyBuilder.INSTANCE_LIST_KEY_PREFIX, server); distroKey.getActualResourceTypes().addAll(toUpdateKeys); DistroData remoteData = distroProtocol.queryFromRemote(distroKey); if (null != remoteData) &#123; processData(remoteData.getContent()); &#125; &#125; catch (Exception e) &#123; Loggers.DISTRO.error(&quot;get data from &quot; + server + &quot; failed!&quot;, e); &#125; &#125; finally &#123; // Remove this &#x27;in process&#x27; flag: syncChecksumTasks.remove(server); &#125; &#125;&#125;@Componentpublic class DistroProtocol &#123; public DistroData queryFromRemote(DistroKey distroKey) &#123; if (null == distroKey.getTargetServer()) &#123; Loggers.DISTRO.warn(&quot;[DISTRO] Can&#x27;t query data from empty server&quot;); return null; &#125; String resourceType = distroKey.getResourceType(); DistroTransportAgent transportAgent = distroComponentHolder.findTransportAgent(resourceType); if (null == transportAgent) &#123; Loggers.DISTRO.warn(&quot;[DISTRO] Can&#x27;t find transport agent for key &#123;&#125;&quot;, resourceType); return null; &#125; return transportAgent.getData(distroKey, distroKey.getTargetServer()); &#125;&#125;public class DistroHttpAgent implements DistroTransportAgent &#123; @Override public DistroData getData(DistroKey key, String targetServer) &#123; try &#123; List&lt;String&gt; toUpdateKeys = null; if (key instanceof DistroHttpCombinedKey) &#123; toUpdateKeys = ((DistroHttpCombinedKey) key).getActualResourceTypes(); &#125; else &#123; toUpdateKeys = new ArrayList&lt;&gt;(1); toUpdateKeys.add(key.getResourceKey()); &#125; //get 请求 /v1/ns/distro/datum 获取数据 byte[] queriedData = NamingProxy.getData(toUpdateKeys, key.getTargetServer()); return new DistroData(key, queriedData); &#125; catch (Exception e) &#123; throw new DistroException(String.format(&quot;Get data from %s failed.&quot;, key.getTargetServer()), e); &#125; &#125;&#125;@DependsOn(&quot;ProtocolManager&quot;)@org.springframework.stereotype.Service(&quot;distroConsistencyService&quot;)public class DistroConsistencyServiceImpl implements EphemeralConsistencyService, DistroDataProcessor &#123; //data = 从源 Server 获取 Datum 数据 private boolean processData(byte[] data) throws Exception &#123; if (data.length &gt; 0) &#123; Map&lt;String, Datum&lt;Instances&gt;&gt; datumMap = serializer.deserializeMap(data, Instances.class); for (Map.Entry&lt;String, Datum&lt;Instances&gt;&gt; entry : datumMap.entrySet()) &#123; //把新 Datum 数据 存入 dataStore dataStore.put(entry.getKey(), entry.getValue()); //listeners 不包含 key if (!listeners.containsKey(entry.getKey())) &#123; // pretty sure the service not exist: if (switchDomain.isDefaultInstanceEphemeral()) &#123; // create empty service //创建空 service Loggers.DISTRO.info(&quot;creating service &#123;&#125;&quot;, entry.getKey()); Service service = new Service(); String serviceName = KeyBuilder.getServiceName(entry.getKey()); String namespaceId = KeyBuilder.getNamespace(entry.getKey()); service.setName(serviceName); service.setNamespaceId(namespaceId); service.setGroupName(Constants.DEFAULT_GROUP); // now validate the service. if failed, exception will be thrown service.setLastModifiedMillis(System.currentTimeMillis()); service.recalculateChecksum(); // The Listener corresponding to the key value must not be empty //与键值对应的监听器不能为空 RecordListener listener = listeners.get(KeyBuilder.SERVICE_META_KEY_PREFIX).peek(); if (Objects.isNull(listener)) &#123; return false; &#125; //这里 listener 是 ServiceManager listener.onChange(KeyBuilder.buildServiceMetaKey(namespaceId, serviceName), service); &#125; &#125; &#125; for (Map.Entry&lt;String, Datum&lt;Instances&gt;&gt; entry : datumMap.entrySet()) &#123; //不应该存在 if (!listeners.containsKey(entry.getKey())) &#123; // Should not happen: Loggers.DISTRO.warn(&quot;listener of &#123;&#125; not found.&quot;, entry.getKey()); continue; &#125; try &#123; for (RecordListener listener : listeners.get(entry.getKey())) &#123; //listener 就是 Service listener.onChange(entry.getKey(), entry.getValue().value); &#125; &#125; catch (Exception e) &#123; Loggers.DISTRO.error(&quot;[NACOS-DISTRO] error while execute listener of key: &#123;&#125;&quot;, entry.getKey(), e); continue; &#125; // Update data store if listener executed successfully: dataStore.put(entry.getKey(), entry.getValue()); &#125; &#125; return true; &#125;&#125; GET /v1/ns/distro/datum12345678910111213141516171819202122232425262728293031323334353637383940414243444546@RestController@RequestMapping(UtilsAndCommons.NACOS_NAMING_CONTEXT + &quot;/distro&quot;)public class DistroController &#123; @GetMapping(&quot;/datum&quot;) public ResponseEntity get(@RequestBody String body) throws Exception &#123; JsonNode bodyNode = JacksonUtils.toObj(body); String keys = bodyNode.get(&quot;keys&quot;).asText(); String keySplitter = &quot;,&quot;; DistroHttpCombinedKey distroKey = new DistroHttpCombinedKey(KeyBuilder.INSTANCE_LIST_KEY_PREFIX, &quot;&quot;); for (String key : keys.split(keySplitter)) &#123; distroKey.getActualResourceTypes().add(key); &#125; DistroData distroData = distroProtocol.onQuery(distroKey); return ResponseEntity.ok(distroData.getContent()); &#125;&#125;@Componentpublic class DistroProtocol &#123; public DistroData onQuery(DistroKey distroKey) &#123; String resourceType = distroKey.getResourceType(); DistroDataStorage distroDataStorage = distroComponentHolder.findDataStorage(resourceType); if (null == distroDataStorage) &#123; Loggers.DISTRO.warn(&quot;[DISTRO] Can&#x27;t find data storage for received key &#123;&#125;&quot;, resourceType); return new DistroData(distroKey, new byte[0]); &#125; return distroDataStorage.getDistroData(distroKey); &#125;&#125;public class DistroDataStorageImpl implements DistroDataStorage &#123; @Override public DistroData getDistroData(DistroKey distroKey) &#123; Map&lt;String, Datum&gt; result = new HashMap&lt;&gt;(1); if (distroKey instanceof DistroHttpCombinedKey) &#123; //批量获取 Datum result = dataStore.batchGet(((DistroHttpCombinedKey) distroKey).getActualResourceTypes()); &#125; else &#123; Datum datum = dataStore.get(distroKey.getResourceKey()); result.put(distroKey.getResourceKey(), datum); &#125; byte[] dataContent = ApplicationUtils.getBean(Serializer.class).serialize(result); return new DistroData(distroKey, dataContent); &#125;&#125; PUT /v1/ns/distro/datum12345678910111213141516171819202122232425262728293031323334353637383940@RestController@RequestMapping(UtilsAndCommons.NACOS_NAMING_CONTEXT + &quot;/distro&quot;)public class DistroController &#123;@PutMapping(&quot;/datum&quot;) public ResponseEntity onSyncDatum(@RequestBody Map&lt;String, Datum&lt;Instances&gt;&gt; dataMap) throws Exception &#123; if (dataMap.isEmpty()) &#123; Loggers.DISTRO.error(&quot;[onSync] receive empty entity!&quot;); throw new NacosException(NacosException.INVALID_PARAM, &quot;receive empty entity!&quot;); &#125; for (Map.Entry&lt;String, Datum&lt;Instances&gt;&gt; entry : dataMap.entrySet()) &#123; if (KeyBuilder.matchEphemeralInstanceListKey(entry.getKey())) &#123; String namespaceId = KeyBuilder.getNamespace(entry.getKey()); String serviceName = KeyBuilder.getServiceName(entry.getKey()); if (!serviceManager.containService(namespaceId, serviceName) &amp;&amp; switchDomain .isDefaultInstanceEphemeral()) &#123; //如果没有，创建新 Service serviceManager.createEmptyService(namespaceId, serviceName, true); &#125; DistroHttpData distroHttpData = new DistroHttpData(createDistroKey(entry.getKey()), entry.getValue()); distroProtocol.onReceive(distroHttpData); &#125; &#125; return ResponseEntity.ok(&quot;ok&quot;); &#125;&#125;@DependsOn(&quot;ProtocolManager&quot;)@org.springframework.stereotype.Service(&quot;distroConsistencyService&quot;)public class DistroConsistencyServiceImpl implements EphemeralConsistencyService, DistroDataProcessor &#123; @Override public boolean processData(DistroData distroData) &#123; DistroHttpData distroHttpData = (DistroHttpData) distroData; Datum&lt;Instances&gt; datum = (Datum&lt;Instances&gt;) distroHttpData.getDeserializedContent(); onPut(datum.key, datum.value); return true; &#125;&#125; 总结 client，可以去任意Server发心跳或者注册，因为到达Server会根据服务名计算hash，如果不是自己Server处理就会转发，最后只会是同一个Server处理 client,可以去任意Server订阅服务列表，因为 Server 和 Server 之间通过定时任务广播复制服务列表，服务列表最终一致性 client订阅的服务列表本地缓存每10秒刷新一次，这是拉模式，如果在10秒内有实例表更新了，Server 会推送新的数据过来，这是推模式 client 请求 Server 如果只填了一个 server-ip 会重试3次,如果填了多个，会重试所有你填的 server-ip todo 有时间重新画一下上面那副大图 todo 持久化实例使用了cp模式，用了 sofajraft 实现的，暂时没有时间看 todo 配置中心 todo 和安全有关 todo istio","categories":[{"name":"spring cloud alibaba","slug":"spring-cloud-alibaba","permalink":"https://xxnjdg.github.io/categories/spring-cloud-alibaba/"}],"tags":[{"name":"nacos","slug":"nacos","permalink":"https://xxnjdg.github.io/tags/nacos/"}]},{"title":"nacos注册中心集群笔记1","slug":"nacos-4","date":"2021-02-06T10:47:36.000Z","updated":"2021-02-14T10:33:41.210Z","comments":true,"path":"2021/02/06/nacos-4/","link":"","permalink":"https://xxnjdg.github.io/2021/02/06/nacos-4/","excerpt":"","text":"nacos 版本 1.4.1 ServerMemberManager集群管理类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889@Component(value = &quot;serverMemberManager&quot;)public class ServerMemberManager implements ApplicationListener&lt;WebServerInitializedEvent&gt; &#123; private final NacosAsyncRestTemplate asyncRestTemplate = HttpClientBeanHolder .getNacosAsyncRestTemplate(Loggers.CORE); /** * Cluster node list. * 集群节点列表 */ private volatile ConcurrentSkipListMap&lt;String, Member&gt; serverList; /** * Is this node in the cluster list. * 当前节点是否在集群中 */ private volatile boolean isInIpList = true; /** * port. * 当前节点端口 */ private int port; /** * Address information for the local node. * 当前节点地址 */ private String localAddress; /** * Addressing pattern instances. * 发现集群其他节点的方式 */ private MemberLookup lookup; /** * self member obj. * 本机 member */ private volatile Member self; /** * here is always the node information of the &quot;UP&quot; state. * 节点都是UP状态列表 */ private volatile Set&lt;String&gt; memberAddressInfos = new ConcurrentHashSet&lt;&gt;(); /** * Broadcast this node element information task. * 广播此节点元素信息任务。 */ private final MemberInfoReportTask infoReportTask = new MemberInfoReportTask(); public ServerMemberManager(ServletContext servletContext) throws Exception &#123; this.serverList = new ConcurrentSkipListMap&lt;&gt;(); EnvUtil.setContextPath(servletContext.getContextPath()); init(); &#125; protected void init() throws NacosException &#123; Loggers.CORE.info(&quot;Nacos-related cluster resource initialization&quot;); //初始化参数 this.port = EnvUtil.getProperty(&quot;server.port&quot;, Integer.class, 8848); this.localAddress = InetUtils.getSelfIP() + &quot;:&quot; + port; this.self = MemberUtil.singleParse(this.localAddress); this.self.setExtendVal(MemberMetaDataConstants.VERSION, VersionUtils.version); serverList.put(self.getAddress(), self); // register NodeChangeEvent publisher to NotifyManager registerClusterEvent(); // Initializes the lookup mode initAndStartLookup(); if (serverList.isEmpty()) &#123; throw new NacosException(NacosException.SERVER_ERROR, &quot;cannot get serverlist, so exit.&quot;); &#125; Loggers.CORE.info(&quot;The cluster resource is initialized&quot;); &#125; private void initAndStartLookup() throws NacosException &#123; //创建 MemberLookup this.lookup = LookupFactory.createLookUp(this); this.lookup.start(); &#125;&#125; 123456789101112131415161718192021222324252627282930313233public final class LookupFactory &#123; private static final String LOOKUP_MODE_TYPE = &quot;nacos.core.member.lookup.type&quot;; @SuppressWarnings(&quot;checkstyle:StaticVariableName&quot;) private static MemberLookup LOOK_UP = null; private static LookupType currentLookupType = null; /** * Create the target addressing pattern. * * @param memberManager &#123;@link ServerMemberManager&#125; * @return &#123;@link MemberLookup&#125; * @throws NacosException NacosException */ public static MemberLookup createLookUp(ServerMemberManager memberManager) throws NacosException &#123; if (!EnvUtil.getStandaloneMode()) &#123; //从 nacos.core.member.lookup.type 读取集群寻址模式 String lookupType = EnvUtil.getProperty(LOOKUP_MODE_TYPE); //默认 file，可以选择 address-server LookupType type = chooseLookup(lookupType); //创建 MemberLookup LOOK_UP = find(type); currentLookupType = type; &#125; else &#123; LOOK_UP = new StandaloneMemberLookup(); &#125; LOOK_UP.injectMemberManager(memberManager); Loggers.CLUSTER.info(&quot;Current addressing mode selection : &#123;&#125;&quot;, LOOK_UP.getClass().getSimpleName()); return LOOK_UP; &#125;&#125; FileConfigMemberLookup 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class FileConfigMemberLookup extends AbstractMemberLookup &#123; private FileWatcher watcher = new FileWatcher() &#123; @Override public void onChange(FileChangeEvent event) &#123; readClusterConfFromDisk(); &#125; @Override public boolean interest(String context) &#123; return StringUtils.contains(context, &quot;cluster.conf&quot;); &#125; &#125;; @Override public void start() throws NacosException &#123; if (start.compareAndSet(false, true)) &#123; readClusterConfFromDisk(); // Use the inotify mechanism to monitor file changes and automatically // trigger the reading of cluster.conf try &#123; //监听 cluster.conf 是否改变 WatchFileCenter.registerWatcher(EnvUtil.getConfPath(), watcher); &#125; catch (Throwable e) &#123; Loggers.CLUSTER.error(&quot;An exception occurred in the launch file monitor : &#123;&#125;&quot;, e.getMessage()); &#125; &#125; &#125; private void readClusterConfFromDisk() &#123; Collection&lt;Member&gt; tmpMembers = new ArrayList&lt;&gt;(); try &#123; //读取集群地址 List&lt;String&gt; tmp = EnvUtil.readClusterConf(); //封装成 Member tmpMembers = MemberUtil.readServerConf(tmp); &#125; catch (Throwable e) &#123; Loggers.CLUSTER .error(&quot;nacos-XXXX [serverlist] failed to get serverlist from disk!, error : &#123;&#125;&quot;, e.getMessage()); &#125; afterLookup(tmpMembers); &#125;&#125;public abstract class AbstractMemberLookup implements MemberLookup &#123; protected ServerMemberManager memberManager; protected AtomicBoolean start = new AtomicBoolean(false); @Override public void injectMemberManager(ServerMemberManager memberManager) &#123; this.memberManager = memberManager; &#125; @Override public void afterLookup(Collection&lt;Member&gt; members) &#123; //最后调用了 memberChange this.memberManager.memberChange(members); &#125; @Override public void destroy() throws NacosException &#123; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566@Component(value = &quot;serverMemberManager&quot;)public class ServerMemberManager implements ApplicationListener&lt;WebServerInitializedEvent&gt; &#123; synchronized boolean memberChange(Collection&lt;Member&gt; members) &#123; if (members == null || members.isEmpty()) &#123; return false; &#125; boolean isContainSelfIp = members.stream() .anyMatch(ipPortTmp -&gt; Objects.equals(localAddress, ipPortTmp.getAddress())); if (isContainSelfIp) &#123; isInIpList = true; &#125; else &#123; isInIpList = false; members.add(this.self); Loggers.CLUSTER.warn(&quot;[serverlist] self ip &#123;&#125; not in serverlist &#123;&#125;&quot;, self, members); &#125; // If the number of old and new clusters is different, the cluster information // must have changed; if the number of clusters is the same, then compare whether // there is a difference; if there is a difference, then the cluster node changes // are involved and all recipients need to be notified of the node change event boolean hasChange = members.size() != serverList.size(); ConcurrentSkipListMap&lt;String, Member&gt; tmpMap = new ConcurrentSkipListMap&lt;&gt;(); Set&lt;String&gt; tmpAddressInfo = new ConcurrentHashSet&lt;&gt;(); for (Member member : members) &#123; final String address = member.getAddress(); if (!serverList.containsKey(address)) &#123; hasChange = true; &#125; // Ensure that the node is created only once tmpMap.put(address, member); if (NodeState.UP.equals(member.getState())) &#123; tmpAddressInfo.add(address); &#125; &#125; //更新 serverList memberAddressInfos serverList = tmpMap; memberAddressInfos = tmpAddressInfo; Collection&lt;Member&gt; finalMembers = allMembers(); Loggers.CLUSTER.warn(&quot;[serverlist] updated to : &#123;&#125;&quot;, finalMembers); // Persist the current cluster node information to cluster.conf // &lt;important&gt; need to put the event publication into a synchronized block to ensure // that the event publication is sequential // 将当前集群节点信息持久化为cluster.conf &lt;important&gt;需要将事件发布放入同步块中，以确保事件发布是连续的 //如果节点发生了变化 if (hasChange) &#123; //同步到文件 MemberUtil.syncToFile(finalMembers); Event event = MembersChangeEvent.builder().members(finalMembers).build(); //发送事件 NotifyCenter.publishEvent(event); &#125; return hasChange; &#125;&#125; 只要继承 MemberChangeListener 类就能接受 MembersChangeEvent 事件，以下是 MemberChangeListener 子类 1234ProtocolManagerServerListManagerRaftPeerSetDistroMapper RaftPeerSet 和 ServerListManager 准备废弃了，不看 todo ProtocolManager 和持久化实例有关 重点看 DistroMapper 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Component(&quot;distroMapper&quot;)public class DistroMapper extends MemberChangeListener &#123; /** * List of service nodes, you must ensure that the order of healthyList is the same for all nodes. * 服务节点列表，必须确保所有节点的healthyList顺序相同。 */ private volatile List&lt;String&gt; healthyList = new ArrayList&lt;&gt;(); private final SwitchDomain switchDomain; private final ServerMemberManager memberManager; public DistroMapper(ServerMemberManager memberManager, SwitchDomain switchDomain) &#123; this.memberManager = memberManager; this.switchDomain = switchDomain; &#125; public List&lt;String&gt; getHealthyList() &#123; return healthyList; &#125; /** * init server list. */ @PostConstruct public void init() &#123; NotifyCenter.registerSubscriber(this); this.healthyList = MemberUtil.simpleMembers(memberManager.allMembers()); &#125; @Override public void onEvent(MembersChangeEvent event) &#123; // Here, the node list must be sorted to ensure that all nacos-server&#x27;s // node list is in the same order //过滤出是 NodeState.UP 或 NodeState.SUSPICIOUS 状态的节点，原因在 ServerMemberManager 定时任务简介 List&lt;String&gt; list = MemberUtil.simpleMembers(MemberUtil.selectTargetMembers(event.getMembers(), member -&gt; NodeState.UP.equals(member.getState()) || NodeState.SUSPICIOUS.equals(member.getState()))); //必须排序，确保所有服务节点列表顺序都是一样，原因后面 DistroFilter 讲解 Collections.sort(list); Collection&lt;String&gt; old = healthyList; healthyList = Collections.unmodifiableList(list); Loggers.SRV_LOG.info(&quot;[NACOS-DISTRO] healthy server list changed, old: &#123;&#125;, new: &#123;&#125;&quot;, old, healthyList); &#125;&#125; com.alibaba.nacos.core.cluster.lookup.FileConfigMemberLookup#start最后会监听 cluster.conf 是否改变， 如果修改了，会重新执行 com.alibaba.nacos.core.cluster.lookup.FileConfigMemberLookup#readClusterConfFromDisk 这些逻辑可以参考 com.alibaba.nacos.sys.file.WatchFileCenter#registerWatcher 这里有个问题就是，修改 cluster.conf 只有本地Server知道，其他Server是不知道的，所以其他Server也要一同修改 ServerMemberManager 定时任务com.alibaba.nacos.core.cluster.ServerMemberManager 继承了 ApplicationListener 初始化会接受事件，接下来我们看看代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495@Component(value = &quot;serverMemberManager&quot;)public class ServerMemberManager implements ApplicationListener&lt;WebServerInitializedEvent&gt; &#123; @Override public void onApplicationEvent(WebServerInitializedEvent event) &#123; getSelf().setState(NodeState.UP); if (!EnvUtil.getStandaloneMode()) &#123; //启动定时任务每2秒循环请求集群节点，主动检测其他节点是否能通信 GlobalExecutor.scheduleByCommon(this.infoReportTask, 5_000L); &#125; //设置 端口和地址 EnvUtil.setPort(event.getWebServer().getPort()); EnvUtil.setLocalAddress(this.localAddress); Loggers.CLUSTER.info(&quot;This node is ready to provide external services&quot;); &#125; class MemberInfoReportTask extends Task &#123; private final GenericType&lt;RestResult&lt;String&gt;&gt; reference = new GenericType&lt;RestResult&lt;String&gt;&gt;() &#123; &#125;; private int cursor = 0; @Override protected void executeBody() &#123; //获取除本Server其他节点 List&lt;Member&gt; members = ServerMemberManager.this.allMembersWithoutSelf(); if (members.isEmpty()) &#123; return; &#125; //轮询节点 this.cursor = (this.cursor + 1) % members.size(); Member target = members.get(cursor); Loggers.CLUSTER.debug(&quot;report the metadata to the node : &#123;&#125;&quot;, target.getAddress()); final String url = HttpUtils .buildUrl(false, target.getAddress(), EnvUtil.getContextPath(), Commons.NACOS_CORE_CONTEXT, &quot;/cluster/report&quot;); try &#123; asyncRestTemplate .post(url, Header.newInstance().addParam(Constants.NACOS_SERVER_HEADER, VersionUtils.version), Query.EMPTY, getSelf(), reference.getType(), new Callback&lt;String&gt;() &#123; @Override public void onReceive(RestResult&lt;String&gt; result) &#123; if (result.getCode() == HttpStatus.NOT_IMPLEMENTED.value() || result.getCode() == HttpStatus.NOT_FOUND.value()) &#123; //警告解释了 Loggers.CLUSTER .warn(&quot;&#123;&#125; version is too low, it is recommended to upgrade the version : &#123;&#125;&quot;, target, VersionUtils.version); return; &#125; if (result.ok()) &#123; //成功逻辑 MemberUtil.onSuccess(ServerMemberManager.this, target); &#125; else &#123; Loggers.CLUSTER .warn(&quot;failed to report new info to target node : &#123;&#125;, result : &#123;&#125;&quot;, target.getAddress(), result); //失败 MemberUtil.onFail(ServerMemberManager.this, target); &#125; &#125; @Override public void onError(Throwable throwable) &#123; Loggers.CLUSTER .error(&quot;failed to report new info to target node : &#123;&#125;, error : &#123;&#125;&quot;, target.getAddress(), ExceptionUtil.getAllExceptionMsg(throwable)); //失败 MemberUtil.onFail(ServerMemberManager.this, target, throwable); &#125; @Override public void onCancel() &#123; &#125; &#125;); &#125; catch (Throwable ex) &#123; Loggers.CLUSTER.error(&quot;failed to report new info to target node : &#123;&#125;, error : &#123;&#125;&quot;, target.getAddress(), ExceptionUtil.getAllExceptionMsg(ex)); &#125; &#125; @Override protected void after() &#123; //重新调度 GlobalExecutor.scheduleByCommon(this, 2_000L); &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738public class MemberUtil &#123; public static void onSuccess(final ServerMemberManager manager, final Member member) &#123; //请求成功，证明被请求Server能够正常通信 final NodeState old = member.getState(); //设置被请求Server状态 NodeState.UP manager.getMemberAddressInfos().add(member.getAddress()); member.setState(NodeState.UP); member.setFailAccessCnt(0); if (!Objects.equals(old, member.getState())) &#123; //如果状态不一致，发送事件 manager.notifyMemberChange(); &#125; &#125; public static void onFail(final ServerMemberManager manager, final Member member, Throwable ex) &#123; //请求失败，被请求节点不能通信 //移除被请求Server manager.getMemberAddressInfos().remove(member.getAddress()); final NodeState old = member.getState(); //设置状态 NodeState.SUSPICIOUS member.setState(NodeState.SUSPICIOUS); //失败次数加1 member.setFailAccessCnt(member.getFailAccessCnt() + 1); int maxFailAccessCnt = EnvUtil.getProperty(&quot;nacos.core.member.fail-access-cnt&quot;, Integer.class, 3); // If the number of consecutive failures to access the target node reaches // a maximum, or the link request is rejected, the state is directly down //失败次数超过 maxFailAccessCnt 设置状态 NodeState.DOWN if (member.getFailAccessCnt() &gt; maxFailAccessCnt || StringUtils .containsIgnoreCase(ex.getMessage(), TARGET_MEMBER_CONNECT_REFUSE_ERRMSG)) &#123; member.setState(NodeState.DOWN); &#125; if (!Objects.equals(old, member.getState())) &#123; //状态改变，发送事件 manager.notifyMemberChange(); &#125; &#125;&#125; 在 DistroMapper 接受事件中，可以看到过滤出 NodeState.UP 或 NodeState.SUSPICIOUS 状态的节点，nacos 把 NodeState.SUSPICIOUS 也认为可以正常通信 我猜测这么做，可能考虑到网络抖动带来的短暂通信失败，不代表节点真的挂了 DistroFiltercom.alibaba.nacos.naming.web.NamingConfig 初始化 DistroFilter 过滤器 功能和一致性hash负载均衡很相似，先计算服务名hash值，在和集群列表长度求余获取请求服务的index，这样客户端随便请求Server每次请求都能请求到同一个服务 逻辑简单，不贴代码了 ServiceManager1234567891011121314151617181920212223242526272829303132333435@Componentpublic class ServiceManager implements RecordListener&lt;Service&gt; &#123; /** * Init service maneger. */ @PostConstruct public void init() &#123; GlobalExecutor.scheduleServiceReporter(new ServiceReporter(), 60000, TimeUnit.MILLISECONDS); GlobalExecutor.submitServiceUpdateManager(new UpdatedServiceProcessor()); if (emptyServiceAutoClean) &#123; Loggers.SRV_LOG.info(&quot;open empty service auto clean job, initialDelay : &#123;&#125; ms, period : &#123;&#125; ms&quot;, cleanEmptyServiceDelay, cleanEmptyServicePeriod); // delay 60s, period 20s; // This task is not recommended to be performed frequently in order to avoid // the possibility that the service cache information may just be deleted // and then created due to the heartbeat mechanism GlobalExecutor.scheduleServiceAutoClean(new EmptyServiceAutoClean(), cleanEmptyServiceDelay, cleanEmptyServicePeriod); &#125; try &#123; Loggers.SRV_LOG.info(&quot;listen for service meta change&quot;); consistencyService.listen(KeyBuilder.SERVICE_META_KEY_PREFIX, this); &#125; catch (NacosException e) &#123; Loggers.SRV_LOG.error(&quot;listen for service meta change failed!&quot;); &#125; &#125;&#125; 这里涉及两个定时任务和1个线程 定时任务每60秒循环执行ServiceReporter,遍历当前服务所有Server,并计算相应 checksum ，把 checksum 封装成消息，广播发送给其他服务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@Componentpublic class ServiceManager implements RecordListener&lt;Service&gt; &#123; private class ServiceReporter implements Runnable &#123; @Override public void run() &#123; try &#123; //Map(namespace, Set(group::serviceName)). Map&lt;String, Set&lt;String&gt;&gt; allServiceNames = getAllServiceNames(); if (allServiceNames.size() &lt;= 0) &#123; //ignore return; &#125; for (String namespaceId : allServiceNames.keySet()) &#123; ServiceChecksum checksum = new ServiceChecksum(namespaceId); //遍历 serviceName for (String serviceName : allServiceNames.get(namespaceId)) &#123; //不属于当前节点serviceName不处理 if (!distroMapper.responsible(serviceName)) &#123; continue; &#125; //获取 Service Service service = getService(namespaceId, serviceName); if (service == null || service.isEmpty()) &#123; continue; &#125; //计算 Service Checksum service.recalculateChecksum(); //加入 checksum checksum.addItem(serviceName, service.getChecksum()); &#125; //构造 Message Message msg = new Message(); //设置 msg.setData(JacksonUtils.toJson(checksum)); Collection&lt;Member&gt; sameSiteServers = memberManager.allMembers(); if (sameSiteServers == null || sameSiteServers.size() &lt;= 0) &#123; return; &#125; //遍历其他 Server 节点 for (Member server : sameSiteServers) &#123; if (server.getAddress().equals(NetUtils.localServer())) &#123; continue; &#125; //发送 synchronizer.send(server.getAddress(), msg); &#125; &#125; &#125; catch (Exception e) &#123; Loggers.SRV_LOG.error(&quot;[DOMAIN-STATUS] Exception while sending service status&quot;, e); &#125; finally &#123; GlobalExecutor.scheduleServiceReporter(this, switchDomain.getServiceStatusSynchronizationPeriodMillis(), TimeUnit.MILLISECONDS); &#125; &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class ServiceStatusSynchronizer implements Synchronizer &#123; @Override public void send(final String serverIP, Message msg) &#123; if (serverIP == null) &#123; return; &#125; Map&lt;String, String&gt; params = new HashMap&lt;String, String&gt;(10); params.put(&quot;statuses&quot;, msg.getData()); params.put(&quot;clientIP&quot;, NetUtils.localServer()); String url = &quot;http://&quot; + serverIP + &quot;:&quot; + EnvUtil.getPort() + EnvUtil.getContextPath() + UtilsAndCommons.NACOS_NAMING_CONTEXT + &quot;/service/status&quot;; if (IPUtil.containsPort(serverIP)) &#123; url = &quot;http://&quot; + serverIP + EnvUtil.getContextPath() + UtilsAndCommons.NACOS_NAMING_CONTEXT + &quot;/service/status&quot;; &#125; try &#123; //Post 请求 /nacos/v1/ns/service/status HttpClient.asyncHttpPostLarge(url, null, JacksonUtils.toJson(params), new Callback&lt;String&gt;() &#123; @Override public void onReceive(RestResult&lt;String&gt; result) &#123; if (!result.ok()) &#123; Loggers.SRV_LOG.warn(&quot;[STATUS-SYNCHRONIZE] failed to request serviceStatus, remote server: &#123;&#125;&quot;, serverIP); &#125; &#125; @Override public void onError(Throwable throwable) &#123; Loggers.SRV_LOG.warn(&quot;[STATUS-SYNCHRONIZE] failed to request serviceStatus, remote server: &quot; + serverIP, throwable); &#125; @Override public void onCancel() &#123; &#125; &#125;); &#125; catch (Exception e) &#123; Loggers.SRV_LOG.warn(&quot;[STATUS-SYNCHRONIZE] failed to request serviceStatus, remote server: &quot; + serverIP, e); &#125; &#125;&#125; todo EmptyServiceAutoClean 好像和持久化实例有关 开启线程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113@Componentpublic class ServiceManager implements RecordListener&lt;Service&gt; &#123; private final LinkedBlockingDeque&lt;ServiceKey&gt; toBeUpdatedServicesQueue = new LinkedBlockingDeque&lt;&gt;(1024 * 1024); private class UpdatedServiceProcessor implements Runnable &#123; //get changed service from other server asynchronously @Override public void run() &#123; ServiceKey serviceKey = null; try &#123; while (true) &#123; try &#123; // serviceKey = toBeUpdatedServicesQueue.take(); &#125; catch (Exception e) &#123; Loggers.EVT_LOG.error(&quot;[UPDATE-DOMAIN] Exception while taking item from LinkedBlockingDeque.&quot;); &#125; if (serviceKey == null) &#123; continue; &#125; GlobalExecutor.submitServiceUpdate(new ServiceUpdater(serviceKey)); &#125; &#125; catch (Exception e) &#123; Loggers.EVT_LOG.error(&quot;[UPDATE-DOMAIN] Exception while update service: &#123;&#125;&quot;, serviceKey, e); &#125; &#125; &#125; private class ServiceUpdater implements Runnable &#123; String namespaceId; String serviceName; String serverIP; public ServiceUpdater(ServiceKey serviceKey) &#123; this.namespaceId = serviceKey.getNamespaceId(); this.serviceName = serviceKey.getServiceName(); this.serverIP = serviceKey.getServerIP(); &#125; @Override public void run() &#123; try &#123; updatedHealthStatus(namespaceId, serviceName, serverIP); &#125; catch (Exception e) &#123; Loggers.SRV_LOG .warn(&quot;[DOMAIN-UPDATER] Exception while update service: &#123;&#125; from &#123;&#125;, error: &#123;&#125;&quot;, serviceName, serverIP, e); &#125; &#125; &#125; public void updatedHealthStatus(String namespaceId, String serviceName, String serverIP) &#123; //向 源nacos Server ip 获取 serviceName 对应所有实例信息 Message msg = synchronizer.get(serverIP, UtilsAndCommons.assembleFullServiceName(namespaceId, serviceName)); JsonNode serviceJson = JacksonUtils.toObj(msg.getData()); ArrayNode ipList = (ArrayNode) serviceJson.get(&quot;ips&quot;); //key = 实例ip value = 健康状态 Map&lt;String, String&gt; ipsMap = new HashMap&lt;&gt;(ipList.size()); for (int i = 0; i &lt; ipList.size(); i++) &#123; String ip = ipList.get(i).asText(); String[] strings = ip.split(&quot;_&quot;); ipsMap.put(strings[0], strings[1]); &#125; Service service = getService(namespaceId, serviceName); if (service == null) &#123; return; &#125; boolean changed = false; List&lt;Instance&gt; instances = service.allIPs(); for (Instance instance : instances) &#123; //可能出现 ipsMap 比 instances 少，不会删除 instance，只会修改 instance 状态 boolean valid = Boolean.parseBoolean(ipsMap.get(instance.toIpAddr())); if (valid != instance.isHealthy()) &#123; //新实例和旧实例健康状态不一致 changed = true; instance.setHealthy(valid); Loggers.EVT_LOG.info(&quot;&#123;&#125; &#123;SYNC&#125; IP-&#123;&#125; : &#123;&#125;:&#123;&#125;@&#123;&#125;&quot;, serviceName, (instance.isHealthy() ? &quot;ENABLED&quot; : &quot;DISABLED&quot;), instance.getIp(), instance.getPort(), instance.getClusterName()); &#125; &#125; if (changed) &#123; //更新客户端服务发现实例信息 pushService.serviceChanged(service); if (Loggers.EVT_LOG.isDebugEnabled()) &#123; StringBuilder stringBuilder = new StringBuilder(); List&lt;Instance&gt; allIps = service.allIPs(); for (Instance instance : allIps) &#123; stringBuilder.append(instance.toIpAddr()).append(&quot;_&quot;).append(instance.isHealthy()).append(&quot;,&quot;); &#125; Loggers.EVT_LOG .debug(&quot;[HEALTH-STATUS-UPDATED] namespace: &#123;&#125;, service: &#123;&#125;, ips: &#123;&#125;&quot;, service.getNamespaceId(), service.getName(), stringBuilder.toString()); &#125; &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435public class ServiceStatusSynchronizer implements Synchronizer &#123; @Override public Message get(String serverIP, String key) &#123; if (serverIP == null) &#123; return null; &#125; Map&lt;String, String&gt; params = new HashMap&lt;&gt;(1); params.put(&quot;key&quot;, key); String result; try &#123; if (Loggers.SRV_LOG.isDebugEnabled()) &#123; Loggers.SRV_LOG.debug(&quot;[STATUS-SYNCHRONIZE] sync service status from: &#123;&#125;, service: &#123;&#125;&quot;, serverIP, key); &#125; // get 请求 /v1/ns/instance/statuses result = NamingProxy .reqApi(EnvUtil.getContextPath() + UtilsAndCommons.NACOS_NAMING_CONTEXT + &quot;/instance/&quot; + &quot;statuses&quot;, params, serverIP); &#125; catch (Exception e) &#123; Loggers.SRV_LOG.warn(&quot;[STATUS-SYNCHRONIZE] Failed to get service status from &quot; + serverIP, e); return null; &#125; if (result == null || result.equals(StringUtils.EMPTY)) &#123; return null; &#125; Message msg = new Message(); msg.setData(result); return msg; &#125;&#125; POST /v1/ns/service/status12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@RestController@RequestMapping(UtilsAndCommons.NACOS_NAMING_CONTEXT + &quot;/service&quot;)public class ServiceController &#123; @PostMapping(&quot;/status&quot;) public String serviceStatus(HttpServletRequest request) throws Exception &#123; String entity = IoUtils.toString(request.getInputStream(), &quot;UTF-8&quot;); String value = URLDecoder.decode(entity, &quot;UTF-8&quot;); JsonNode json = JacksonUtils.toObj(value); //format: service1@@checksum@@@service2@@checksum String statuses = json.get(&quot;statuses&quot;).asText(); String serverIp = json.get(&quot;clientIP&quot;).asText(); if (!memberManager.hasMember(serverIp)) &#123; throw new NacosException(NacosException.INVALID_PARAM, &quot;ip: &quot; + serverIp + &quot; is not in serverlist&quot;); &#125; try &#123; ServiceManager.ServiceChecksum checksums = JacksonUtils .toObj(statuses, ServiceManager.ServiceChecksum.class); if (checksums == null) &#123; Loggers.SRV_LOG.warn(&quot;[DOMAIN-STATUS] receive malformed data: null&quot;); return &quot;fail&quot;; &#125; for (Map.Entry&lt;String, String&gt; entry : checksums.serviceName2Checksum.entrySet()) &#123; if (entry == null || StringUtils.isEmpty(entry.getKey()) || StringUtils.isEmpty(entry.getValue())) &#123; continue; &#125; //服务名 String serviceName = entry.getKey(); //对应 checksum String checksum = entry.getValue(); Service service = serviceManager.getService(checksums.namespaceId, serviceName); //其他nacos Server 的 service 没有同步过来 if (service == null) &#123; continue; &#125; //计算 checksum service.recalculateChecksum(); //如果不一样 if (!checksum.equals(service.getChecksum())) &#123; if (Loggers.SRV_LOG.isDebugEnabled()) &#123; Loggers.SRV_LOG.debug(&quot;checksum of &#123;&#125; is not consistent, remote: &#123;&#125;, checksum: &#123;&#125;, local: &#123;&#125;&quot;, serviceName, serverIp, checksum, service.getChecksum()); &#125; //加入队列 serviceManager.addUpdatedServiceToQueue(checksums.namespaceId, serviceName, serverIp, checksum); &#125; &#125; &#125; catch (Exception e) &#123; Loggers.SRV_LOG.warn(&quot;[DOMAIN-STATUS] receive malformed data: &quot; + statuses, e); &#125; return &quot;ok&quot;; &#125;&#125;","categories":[{"name":"spring cloud alibaba","slug":"spring-cloud-alibaba","permalink":"https://xxnjdg.github.io/categories/spring-cloud-alibaba/"}],"tags":[{"name":"nacos","slug":"nacos","permalink":"https://xxnjdg.github.io/tags/nacos/"}]},{"title":"nacos注册中心服务发现笔记","slug":"nacos-3","date":"2021-02-04T08:15:54.000Z","updated":"2021-02-14T10:33:41.231Z","comments":true,"path":"2021/02/04/nacos-3/","link":"","permalink":"https://xxnjdg.github.io/2021/02/04/nacos-3/","excerpt":"","text":"nacos 版本 1.4.1 客户端服务发现todo 目前没搞清楚哪里调用 搜索了下spring-cloud-starter-alibaba-nacos-discovery包会调用 12ListView&lt;String&gt; services = namingService().getServicesOfServer(1,Integer.MAX_VALUE, group);List&lt;Instance&gt; instances = namingService().selectInstances(serviceId, group,true); 那就重点看下以下两个方法 1234567891011121314151617181920212223242526272829@SuppressWarnings(&quot;PMD.ServiceOrDaoClassShouldEndWithImplRule&quot;)public class NacosNamingService implements NamingService &#123; @Override public List&lt;Instance&gt; selectInstances(String serviceName, String groupName, boolean healthy) throws NacosException &#123; return selectInstances(serviceName, groupName, healthy, true); &#125; @Override public ListView&lt;String&gt; getServicesOfServer(int pageNo, int pageSize, String groupName) throws NacosException &#123; return getServicesOfServer(pageNo, pageSize, groupName, null); &#125; @Override public List&lt;Instance&gt; selectInstances(String serviceName, String groupName, List&lt;String&gt; clusters, boolean healthy, boolean subscribe) throws NacosException &#123; ServiceInfo serviceInfo; //true if (subscribe) &#123; serviceInfo = hostReactor.getServiceInfo(NamingUtils.getGroupedName(serviceName, groupName), StringUtils.join(clusters, &quot;,&quot;)); &#125; else &#123; serviceInfo = hostReactor .getServiceInfoDirectlyFromServer(NamingUtils.getGroupedName(serviceName, groupName), StringUtils.join(clusters, &quot;,&quot;)); &#125; return selectInstances(serviceInfo, healthy); &#125;&#125; https://github.com/alibaba/nacos/issues/3181 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194public class HostReactor implements Closeable &#123; //获取服务信息 public ServiceInfo getServiceInfo(final String serviceName, final String clusters) &#123; NAMING_LOGGER.debug(&quot;failover-mode: &quot; + failoverReactor.isFailoverSwitch()); //获取key String key = ServiceInfo.getKey(serviceName, clusters); //默认为fasle if (failoverReactor.isFailoverSwitch()) &#123; return failoverReactor.getService(key); &#125; //先获取服务信息 ServiceInfo serviceObj = getServiceInfo0(serviceName, clusters); //没有发现服务信息 if (null == serviceObj) &#123; serviceObj = new ServiceInfo(serviceName, clusters); //先放一个空的 ServiceInfo serviceInfoMap.put(serviceObj.getKey(), serviceObj); //加入进 updatingMap，更新标志位 updatingMap.put(serviceName, new Object()); //更新 updateServiceNow(serviceName, clusters); updatingMap.remove(serviceName); &#125; else if (updatingMap.containsKey(serviceName)) &#123; //更新中。。。 if (UPDATE_HOLD_INTERVAL &gt; 0) &#123; // hold a moment waiting for update finish synchronized (serviceObj) &#123; try &#123; //睡眠一会等待更新完成 serviceObj.wait(UPDATE_HOLD_INTERVAL); &#125; catch (InterruptedException e) &#123; NAMING_LOGGER .error(&quot;[getServiceInfo] serviceName:&quot; + serviceName + &quot;, clusters:&quot; + clusters, e); &#125; &#125; &#125; &#125; //开启拉模式 scheduleUpdateIfAbsent(serviceName, clusters); return serviceInfoMap.get(serviceObj.getKey()); &#125; public void updateService(String serviceName, String clusters) throws NacosException &#123; //获取旧 ServiceInfo ServiceInfo oldService = getServiceInfo0(serviceName, clusters); try &#123; //get 请求 /nacos/v1/ns/instance/list String result = serverProxy.queryList(serviceName, clusters, pushReceiver.getUdpPort(), false); if (StringUtils.isNotEmpty(result)) &#123; processServiceJson(result); &#125; &#125; finally &#123; if (oldService != null) &#123; synchronized (oldService) &#123; //唤醒等待的服务 oldService.notifyAll(); &#125; &#125; &#125; &#125; //更新 serviceInfo //发送 InstancesChangeEvent 时间 public ServiceInfo processServiceJson(String json) &#123; //新数据 ServiceInfo serviceInfo = JacksonUtils.toObj(json, ServiceInfo.class); //旧数据 ServiceInfo oldService = serviceInfoMap.get(serviceInfo.getKey()); if (pushEmptyProtection &amp;&amp; !serviceInfo.validate()) &#123; //empty or error push, just ignore return oldService; &#125; boolean changed = false; if (oldService != null) &#123; if (oldService.getLastRefTime() &gt; serviceInfo.getLastRefTime()) &#123; NAMING_LOGGER.warn(&quot;out of date data received, old-t: &quot; + oldService.getLastRefTime() + &quot;, new-t: &quot; + serviceInfo.getLastRefTime()); &#125; //put 进新数据 serviceInfoMap.put(serviceInfo.getKey(), serviceInfo); Map&lt;String, Instance&gt; oldHostMap = new HashMap&lt;String, Instance&gt;(oldService.getHosts().size()); for (Instance host : oldService.getHosts()) &#123; oldHostMap.put(host.toInetAddr(), host); &#125; Map&lt;String, Instance&gt; newHostMap = new HashMap&lt;String, Instance&gt;(serviceInfo.getHosts().size()); for (Instance host : serviceInfo.getHosts()) &#123; newHostMap.put(host.toInetAddr(), host); &#125; //修改数组 Set&lt;Instance&gt; modHosts = new HashSet&lt;Instance&gt;(); //新增数组 Set&lt;Instance&gt; newHosts = new HashSet&lt;Instance&gt;(); //删除数组 Set&lt;Instance&gt; remvHosts = new HashSet&lt;Instance&gt;(); List&lt;Map.Entry&lt;String, Instance&gt;&gt; newServiceHosts = new ArrayList&lt;Map.Entry&lt;String, Instance&gt;&gt;( newHostMap.entrySet()); for (Map.Entry&lt;String, Instance&gt; entry : newServiceHosts) &#123; Instance host = entry.getValue(); String key = entry.getKey(); if (oldHostMap.containsKey(key) &amp;&amp; !StringUtils .equals(host.toString(), oldHostMap.get(key).toString())) &#123; modHosts.add(host); continue; &#125; if (!oldHostMap.containsKey(key)) &#123; newHosts.add(host); &#125; &#125; for (Map.Entry&lt;String, Instance&gt; entry : oldHostMap.entrySet()) &#123; Instance host = entry.getValue(); String key = entry.getKey(); if (newHostMap.containsKey(key)) &#123; continue; &#125; if (!newHostMap.containsKey(key)) &#123; remvHosts.add(host); &#125; &#125; if (newHosts.size() &gt; 0) &#123; changed = true; NAMING_LOGGER.info(&quot;new ips(&quot; + newHosts.size() + &quot;) service: &quot; + serviceInfo.getKey() + &quot; -&gt; &quot; + JacksonUtils.toJson(newHosts)); &#125; if (remvHosts.size() &gt; 0) &#123; changed = true; NAMING_LOGGER.info(&quot;removed ips(&quot; + remvHosts.size() + &quot;) service: &quot; + serviceInfo.getKey() + &quot; -&gt; &quot; + JacksonUtils.toJson(remvHosts)); &#125; if (modHosts.size() &gt; 0) &#123; changed = true; updateBeatInfo(modHosts); NAMING_LOGGER.info(&quot;modified ips(&quot; + modHosts.size() + &quot;) service: &quot; + serviceInfo.getKey() + &quot; -&gt; &quot; + JacksonUtils.toJson(modHosts)); &#125; serviceInfo.setJsonFromServer(json); if (newHosts.size() &gt; 0 || remvHosts.size() &gt; 0 || modHosts.size() &gt; 0) &#123; //数据变动了 //发送事件 NotifyCenter.publishEvent(new InstancesChangeEvent(serviceInfo.getName(), serviceInfo.getGroupName(), serviceInfo.getClusters(), serviceInfo.getHosts())); //写到缓存文件中 DiskCache.write(serviceInfo, cacheDir); &#125; &#125; else &#123; changed = true; NAMING_LOGGER.info(&quot;init new ips(&quot; + serviceInfo.ipCount() + &quot;) service: &quot; + serviceInfo.getKey() + &quot; -&gt; &quot; + JacksonUtils.toJson(serviceInfo.getHosts())); serviceInfoMap.put(serviceInfo.getKey(), serviceInfo); NotifyCenter.publishEvent(new InstancesChangeEvent(serviceInfo.getName(), serviceInfo.getGroupName(), serviceInfo.getClusters(), serviceInfo.getHosts())); serviceInfo.setJsonFromServer(json); DiskCache.write(serviceInfo, cacheDir); &#125; MetricsMonitor.getServiceInfoMapSizeMonitor().set(serviceInfoMap.size()); if (changed) &#123; NAMING_LOGGER.info(&quot;current ips:(&quot; + serviceInfo.ipCount() + &quot;) service: &quot; + serviceInfo.getKey() + &quot; -&gt; &quot; + JacksonUtils.toJson(serviceInfo.getHosts())); &#125; return serviceInfo; &#125;&#125; get 请求 /nacos/v1/ns/instance/list 参数整理了下 12345678NetUtils.localIP() = System.getProperty(&quot;com.alibaba.nacos.client.naming.local.ip&quot;,InetAddress.getLocalHost().getHostAddress());params.put(CommonParams.NAMESPACE_ID, nacosDiscoveryProperties.getNamespace()/public);params.put(CommonParams.SERVICE_NAME, nacosDiscoveryProperties.getGroup() + @@ + nacosDiscoveryProperties.getService());params.put(&quot;clusters&quot;, &quot;&quot;);params.put(&quot;udpPort&quot;, String.valueOf(udpPort));params.put(&quot;clientIP&quot;, NetUtils.localIP());params.put(&quot;healthyOnly&quot;, String.valueOf(false)); 定时任务客户端拉取实例列表 这是客户端从服务端拉取实例列表，正常每10秒拉取一次 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104public class HostReactor implements Closeable &#123; public void scheduleUpdateIfAbsent(String serviceName, String clusters) &#123; if (futureMap.get(ServiceInfo.getKey(serviceName, clusters)) != null) &#123; return; &#125; synchronized (futureMap) &#123; if (futureMap.get(ServiceInfo.getKey(serviceName, clusters)) != null) &#123; return; &#125; ScheduledFuture&lt;?&gt; future = addTask(new UpdateTask(serviceName, clusters)); futureMap.put(ServiceInfo.getKey(serviceName, clusters), future); &#125; &#125; public class UpdateTask implements Runnable &#123; long lastRefTime = Long.MAX_VALUE; private final String clusters; private final String serviceName; /** * the fail situation. 1:can&#x27;t connect to server 2:serviceInfo&#x27;s hosts is empty */ private int failCount = 0; public UpdateTask(String serviceName, String clusters) &#123; this.serviceName = serviceName; this.clusters = clusters; &#125; private void incFailCount() &#123; int limit = 6; if (failCount == limit) &#123; return; &#125; failCount++; &#125; private void resetFailCount() &#123; failCount = 0; &#125; @Override public void run() &#123; long delayTime = DEFAULT_DELAY; try &#123; ServiceInfo serviceObj = serviceInfoMap.get(ServiceInfo.getKey(serviceName, clusters)); if (serviceObj == null) &#123; //服务为空，1秒后重新调度 updateService(serviceName, clusters); return; &#125; //获取的 ServiceInfo 可能就是上次拉服务更新的，那么就从服务端拉一次信息下来 //在重新调度延时期间，有可能服务端推服务生效，导致服务信息更新了，这是else情况 if (serviceObj.getLastRefTime() &lt;= lastRefTime) &#123; //先更新 updateService(serviceName, clusters); //获取最新 ServiceInfo serviceObj = serviceInfoMap.get(ServiceInfo.getKey(serviceName, clusters)); &#125; else &#123; // if serviceName already updated by push, we should not override it // since the push data may be different from pull through force push //主要刷新了Server对应PushClient.lastRefTime,不获取数据 //说通俗点就是告诉Server我还没挂，我一直订阅你 refreshOnly(serviceName, clusters); &#125; //记录下更新时间 lastRefTime = serviceObj.getLastRefTime(); if (!notifier.isSubscribed(serviceName, clusters) &amp;&amp; !futureMap .containsKey(ServiceInfo.getKey(serviceName, clusters))) &#123; // abort the update task NAMING_LOGGER.info(&quot;update task is stopped, service:&quot; + serviceName + &quot;, clusters:&quot; + clusters); return; &#125; //没有获取到实例 if (CollectionUtils.isEmpty(serviceObj.getHosts())) &#123; //增加 incFailCount(); return; &#125; //获取延迟时间 delayTime = serviceObj.getCacheMillis(); //重置 resetFailCount(); &#125; catch (Throwable e) &#123; incFailCount(); NAMING_LOGGER.warn(&quot;[NA] failed to update serviceName: &quot; + serviceName, e); &#125; finally &#123; //重新调度,正常10秒 //没获取到实例表，2，4，8，16，32，60逐级延迟，一成功旧变10秒 executor.schedule(this, Math.min(delayTime &lt;&lt; failCount, DEFAULT_DELAY * 60), TimeUnit.MILLISECONDS); &#125; &#125; &#125;&#125; 除了拉取实例列表外，在客户端拉取任务等待期间，正常10秒内，服务端会主动推实例列表过来，前提是实例列表有变法 我们在 GET 请求 /nacos/v1/ns/instance/list 后讲解推模式发送 这里讲解下推模式怎么接受 PushReceiver123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class PushReceiver implements Runnable, Closeable &#123;@Override public void run() &#123; while (!closed) &#123; try &#123; // byte[] is initialized with 0 full filled by default byte[] buffer = new byte[UDP_MSS]; DatagramPacket packet = new DatagramPacket(buffer, buffer.length); //等待接受，在这之前会睡眠 udpSocket.receive(packet); String json = new String(IoUtils.tryDecompress(packet.getData()), UTF_8).trim(); NAMING_LOGGER.info(&quot;received push data: &quot; + json + &quot; from &quot; + packet.getAddress().toString()); PushPacket pushPacket = JacksonUtils.toObj(json, PushPacket.class); String ack; if (&quot;dom&quot;.equals(pushPacket.type) || &quot;service&quot;.equals(pushPacket.type)) &#123; //正常都是走到这个分支 hostReactor.processServiceJson(pushPacket.data); // send ack to server ack = &quot;&#123;\\&quot;type\\&quot;: \\&quot;push-ack\\&quot;&quot; + &quot;, \\&quot;lastRefTime\\&quot;:\\&quot;&quot; + pushPacket.lastRefTime + &quot;\\&quot;, \\&quot;data\\&quot;:&quot; + &quot;\\&quot;\\&quot;&#125;&quot;; &#125; else if (&quot;dump&quot;.equals(pushPacket.type)) &#123; // dump data to server ack = &quot;&#123;\\&quot;type\\&quot;: \\&quot;dump-ack\\&quot;&quot; + &quot;, \\&quot;lastRefTime\\&quot;: \\&quot;&quot; + pushPacket.lastRefTime + &quot;\\&quot;, \\&quot;data\\&quot;:&quot; + &quot;\\&quot;&quot; + StringUtils.escapeJavaScript(JacksonUtils.toJson(hostReactor.getServiceInfoMap())) + &quot;\\&quot;&#125;&quot;; &#125; else &#123; // do nothing send ack only ack = &quot;&#123;\\&quot;type\\&quot;: \\&quot;unknown-ack\\&quot;&quot; + &quot;, \\&quot;lastRefTime\\&quot;:\\&quot;&quot; + pushPacket.lastRefTime + &quot;\\&quot;, \\&quot;data\\&quot;:&quot; + &quot;\\&quot;\\&quot;&#125;&quot;; &#125; //处理结束后，给Server发送ack udpSocket.send(new DatagramPacket(ack.getBytes(UTF_8), ack.getBytes(UTF_8).length, packet.getSocketAddress())); &#125; catch (Exception e) &#123; if (closed) &#123; return; &#125; NAMING_LOGGER.error(&quot;[NA] error while receiving push data&quot;, e); &#125; &#125; &#125;&#125; GET /v1/ns/instance/list123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205@RestController@RequestMapping(UtilsAndCommons.NACOS_NAMING_CONTEXT + &quot;/instance&quot;)public class InstanceController &#123; @GetMapping(&quot;/list&quot;) @Secured(parser = NamingResourceParser.class, action = ActionTypes.READ) public ObjectNode list(HttpServletRequest request) throws Exception &#123; String namespaceId = WebUtils.optional(request, CommonParams.NAMESPACE_ID, Constants.DEFAULT_NAMESPACE_ID); String serviceName = WebUtils.required(request, CommonParams.SERVICE_NAME); NamingUtils.checkServiceNameFormat(serviceName); String agent = WebUtils.getUserAgent(request); String clusters = WebUtils.optional(request, &quot;clusters&quot;, StringUtils.EMPTY); String clientIP = WebUtils.optional(request, &quot;clientIP&quot;, StringUtils.EMPTY); int udpPort = Integer.parseInt(WebUtils.optional(request, &quot;udpPort&quot;, &quot;0&quot;)); String env = WebUtils.optional(request, &quot;env&quot;, StringUtils.EMPTY); boolean isCheck = Boolean.parseBoolean(WebUtils.optional(request, &quot;isCheck&quot;, &quot;false&quot;)); String app = WebUtils.optional(request, &quot;app&quot;, StringUtils.EMPTY); String tenant = WebUtils.optional(request, &quot;tid&quot;, StringUtils.EMPTY); boolean healthyOnly = Boolean.parseBoolean(WebUtils.optional(request, &quot;healthyOnly&quot;, &quot;false&quot;)); return doSrvIpxt(namespaceId, serviceName, agent, clusters, clientIP, udpPort, env, isCheck, app, tenant, healthyOnly); &#125; public ObjectNode doSrvIpxt(String namespaceId, String serviceName, String agent, String clusters, String clientIP, int udpPort, String env, boolean isCheck, String app, String tid, boolean healthyOnly) throws Exception &#123; ClientInfo clientInfo = new ClientInfo(agent); ObjectNode result = JacksonUtils.createEmptyJsonNode(); //获取 Service Service service = serviceManager.getService(namespaceId, serviceName); //默认3秒 long cacheMillis = switchDomain.getDefaultCacheMillis(); // now try to enable the push //服务信息推服务 try &#123; //满足情况只有 client 调用 com.alibaba.nacos.naming.controllers.InstanceController.list if (udpPort &gt; 0 &amp;&amp; pushService.canEnablePush(agent)) &#123; //把参数封装成PushClient对象，加入进 clientMap pushService .addClient(namespaceId, serviceName, clusters, agent, new InetSocketAddress(clientIP, udpPort), pushDataSource, tid, app); //10秒 cacheMillis = switchDomain.getPushCacheMillis(serviceName); &#125; &#125; catch (Exception e) &#123; Loggers.SRV_LOG .error(&quot;[NACOS-API] failed to added push client &#123;&#125;, &#123;&#125;:&#123;&#125;&quot;, clientInfo, clientIP, udpPort, e); cacheMillis = switchDomain.getDefaultCacheMillis(); &#125; if (service == null) &#123; if (Loggers.SRV_LOG.isDebugEnabled()) &#123; Loggers.SRV_LOG.debug(&quot;no instance to serve for service: &#123;&#125;&quot;, serviceName); &#125; //Service 为空 ，直接返回 result.put(&quot;name&quot;, serviceName); result.put(&quot;clusters&quot;, clusters); result.put(&quot;cacheMillis&quot;, cacheMillis); result.replace(&quot;hosts&quot;, JacksonUtils.createEmptyArrayNode()); return result; &#125; checkIfDisabled(service); List&lt;Instance&gt; srvedIPs; //获取指定 clusters 实例，如果没有指定 clusters ，获取 Service 所有实例 srvedIPs = service.srvIPs(Arrays.asList(StringUtils.split(clusters, &quot;,&quot;))); // filter ips using selector: if (service.getSelector() != null &amp;&amp; StringUtils.isNotBlank(clientIP)) &#123; srvedIPs = service.getSelector().select(clientIP, srvedIPs); &#125; if (CollectionUtils.isEmpty(srvedIPs)) &#123; if (Loggers.SRV_LOG.isDebugEnabled()) &#123; Loggers.SRV_LOG.debug(&quot;no instance to serve for service: &#123;&#125;&quot;, serviceName); &#125; if (clientInfo.type == ClientInfo.ClientType.JAVA &amp;&amp; clientInfo.version.compareTo(VersionUtil.parseVersion(&quot;1.0.0&quot;)) &gt;= 0) &#123; result.put(&quot;dom&quot;, serviceName); &#125; else &#123; result.put(&quot;dom&quot;, NamingUtils.getServiceName(serviceName)); &#125; //没有获取到实例，没注册，或者自动删除删光了 result.put(&quot;name&quot;, serviceName); result.put(&quot;cacheMillis&quot;, cacheMillis); result.put(&quot;lastRefTime&quot;, System.currentTimeMillis()); result.put(&quot;checksum&quot;, service.getChecksum()); result.put(&quot;useSpecifiedURL&quot;, false); result.put(&quot;clusters&quot;, clusters); result.put(&quot;env&quot;, env); result.set(&quot;hosts&quot;, JacksonUtils.createEmptyArrayNode()); result.set(&quot;metadata&quot;, JacksonUtils.transferToJsonNode(service.getMetadata())); return result; &#125; Map&lt;Boolean, List&lt;Instance&gt;&gt; ipMap = new HashMap&lt;&gt;(2); ipMap.put(Boolean.TRUE, new ArrayList&lt;&gt;()); ipMap.put(Boolean.FALSE, new ArrayList&lt;&gt;()); //分成健康和不健康两个队列 for (Instance ip : srvedIPs) &#123; ipMap.get(ip.isHealthy()).add(ip); &#125; if (isCheck) &#123; result.put(&quot;reachProtectThreshold&quot;, false); &#125; double threshold = service.getProtectThreshold(); //健康的实例低于等于 threshold * 实例总数，threshold是个阈值，低于等于实例总数的百分之多少，就进入这个if if ((float) ipMap.get(Boolean.TRUE).size() / srvedIPs.size() &lt;= threshold) &#123; Loggers.SRV_LOG.warn(&quot;protect threshold reached, return all ips, service: &#123;&#125;&quot;, serviceName); if (isCheck) &#123; result.put(&quot;reachProtectThreshold&quot;, true); &#125; //不健康的实例列表加进健康实例列表 ipMap.get(Boolean.TRUE).addAll(ipMap.get(Boolean.FALSE)); //不健康列表清除 ipMap.get(Boolean.FALSE).clear(); &#125; if (isCheck) &#123; result.put(&quot;protectThreshold&quot;, service.getProtectThreshold()); result.put(&quot;reachLocalSiteCallThreshold&quot;, false); return JacksonUtils.createEmptyJsonNode(); &#125; ArrayNode hosts = JacksonUtils.createEmptyArrayNode(); for (Map.Entry&lt;Boolean, List&lt;Instance&gt;&gt; entry : ipMap.entrySet()) &#123; List&lt;Instance&gt; ips = entry.getValue(); if (healthyOnly &amp;&amp; !entry.getKey()) &#123; continue; &#125; for (Instance instance : ips) &#123; // remove disabled instance: if (!instance.isEnabled()) &#123; continue; &#125; ObjectNode ipObj = JacksonUtils.createEmptyJsonNode(); //封装 instance ipObj.put(&quot;ip&quot;, instance.getIp()); ipObj.put(&quot;port&quot;, instance.getPort()); // deprecated since nacos 1.0.0: ipObj.put(&quot;valid&quot;, entry.getKey()); ipObj.put(&quot;healthy&quot;, entry.getKey()); ipObj.put(&quot;marked&quot;, instance.isMarked()); ipObj.put(&quot;instanceId&quot;, instance.getInstanceId()); ipObj.set(&quot;metadata&quot;, JacksonUtils.transferToJsonNode(instance.getMetadata())); ipObj.put(&quot;enabled&quot;, instance.isEnabled()); ipObj.put(&quot;weight&quot;, instance.getWeight()); ipObj.put(&quot;clusterName&quot;, instance.getClusterName()); if (clientInfo.type == ClientInfo.ClientType.JAVA &amp;&amp; clientInfo.version.compareTo(VersionUtil.parseVersion(&quot;1.0.0&quot;)) &gt;= 0) &#123; ipObj.put(&quot;serviceName&quot;, instance.getServiceName()); &#125; else &#123; ipObj.put(&quot;serviceName&quot;, NamingUtils.getServiceName(instance.getServiceName())); &#125; ipObj.put(&quot;ephemeral&quot;, instance.isEphemeral()); hosts.add(ipObj); &#125; &#125; //构造剩下的值返回 result.replace(&quot;hosts&quot;, hosts); if (clientInfo.type == ClientInfo.ClientType.JAVA &amp;&amp; clientInfo.version.compareTo(VersionUtil.parseVersion(&quot;1.0.0&quot;)) &gt;= 0) &#123; result.put(&quot;dom&quot;, serviceName); &#125; else &#123; result.put(&quot;dom&quot;, NamingUtils.getServiceName(serviceName)); &#125; result.put(&quot;name&quot;, serviceName); result.put(&quot;cacheMillis&quot;, cacheMillis); result.put(&quot;lastRefTime&quot;, System.currentTimeMillis()); result.put(&quot;checksum&quot;, service.getChecksum()); result.put(&quot;useSpecifiedURL&quot;, false); result.put(&quot;clusters&quot;, clusters); result.put(&quot;env&quot;, env); result.replace(&quot;metadata&quot;, JacksonUtils.transferToJsonNode(service.getMetadata())); return result; &#125;&#125; 在实例注册，或者删除实例中,会出现这些代码，这个代码就是服务端主动把变化了的实例列表推送给客户端 1getPushService().serviceChanged(service); 客户端定期拉取任务中，会每10秒调用 GET /v1/ns/instance/list 请求，在这个请求中，我们看到调用了 PushService#addClient，把参数封装成PushClient对象加到 clientMap 集合中 问题是为什么定时任务要不断调用 addClient 加入到 clientMap 集合中呢？ 答案是因为，如果客户端不在订阅某个服务了，这个时候 PushClient.lastRefTime 时间就不会一直更新，推模式执行时 会判断当前时间 是否大于 PushClient.lastRefTime 加上10秒，如果是，就证明客户端不在订阅实例表了，我们不用把实例表推送给不订阅的客户端 接着把 PushClient 从 clientMap 集合中提走 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233public class PushService implements ApplicationContextAware, ApplicationListener&lt;ServiceChangeEvent&gt; &#123; public void addClient(String namespaceId, String serviceName, String clusters, String agent, InetSocketAddress socketAddr, DataSource dataSource, String tenant, String app) &#123; PushClient client = new PushClient(namespaceId, serviceName, clusters, agent, socketAddr, dataSource, tenant, app); addClient(client); &#125; public void addClient(PushClient client) &#123; // client is stored by key &#x27;serviceName&#x27; because notify event is driven by serviceName change String serviceKey = UtilsAndCommons.assembleFullServiceName(client.getNamespaceId(), client.getServiceName()); ConcurrentMap&lt;String, PushClient&gt; clients = clientMap.get(serviceKey); if (clients == null) &#123; clientMap.putIfAbsent(serviceKey, new ConcurrentHashMap&lt;&gt;(1024)); clients = clientMap.get(serviceKey); &#125; PushClient oldClient = clients.get(client.toString()); if (oldClient != null) &#123; oldClient.refresh(); &#125; else &#123; PushClient res = clients.putIfAbsent(client.toString(), client); if (res != null) &#123; Loggers.PUSH.warn(&quot;client: &#123;&#125; already associated with key &#123;&#125;&quot;, res.getAddrStr(), res.toString()); &#125; Loggers.PUSH.debug(&quot;client: &#123;&#125; added for serviceName: &#123;&#125;&quot;, client.getAddrStr(), client.getServiceName()); &#125; &#125; public void serviceChanged(Service service) &#123; // merge some change events to reduce the push frequency: //等下会看到onApplicationEvent方法定时任务是延时1秒后执行的，并且加紧 futureMap 中 //就是说在这1秒内假设有实例疯狂注册，也不会立刻执行，等延时1秒后， //在读取新数据，这里的新数据就包括了1秒内疯狂注册的新实例列表 //减少 client 推模式qps if (futureMap .containsKey(UtilsAndCommons.assembleFullServiceName(service.getNamespaceId(), service.getName()))) &#123; return; &#125; //发送事件 this.applicationContext.publishEvent(new ServiceChangeEvent(this, service)); &#125; @Override public void onApplicationEvent(ServiceChangeEvent event) &#123; Service service = event.getService(); String serviceName = service.getName(); String namespaceId = service.getNamespaceId(); Future future = GlobalExecutor.scheduleUdpSender(() -&gt; &#123; try &#123; Loggers.PUSH.info(serviceName + &quot; is changed, add it to push queue.&quot;); //获取 clients ConcurrentMap&lt;String, PushClient&gt; clients = clientMap .get(UtilsAndCommons.assembleFullServiceName(namespaceId, serviceName)); //如果为空不执行 if (MapUtils.isEmpty(clients)) &#123; return; &#125; Map&lt;String, Object&gt; cache = new HashMap&lt;&gt;(16); long lastRefTime = System.nanoTime(); for (PushClient client : clients.values()) &#123; //客户端注册时间大于当前时间的10秒，则不在执行 if (client.zombie()) &#123; Loggers.PUSH.debug(&quot;client is zombie: &quot; + client.toString()); clients.remove(client.toString()); Loggers.PUSH.debug(&quot;client is zombie: &quot; + client.toString()); continue; &#125; Receiver.AckEntry ackEntry; Loggers.PUSH.debug(&quot;push serviceName: &#123;&#125; to client: &#123;&#125;&quot;, serviceName, client.toString()); String key = getPushCacheKey(serviceName, client.getIp(), client.getAgent()); byte[] compressData = null; Map&lt;String, Object&gt; data = null; if (switchDomain.getDefaultPushCacheMillis() &gt;= 20000 &amp;&amp; cache.containsKey(key)) &#123; org.javatuples.Pair pair = (org.javatuples.Pair) cache.get(key); compressData = (byte[]) (pair.getValue0()); data = (Map&lt;String, Object&gt;) pair.getValue1(); Loggers.PUSH.debug(&quot;[PUSH-CACHE] cache hit: &#123;&#125;:&#123;&#125;&quot;, serviceName, client.getAddrStr()); &#125; //封装 ackEntry if (compressData != null) &#123; ackEntry = prepareAckEntry(client, compressData, data, lastRefTime); &#125; else &#123; ackEntry = prepareAckEntry(client, prepareHostsData(client), lastRefTime); if (ackEntry != null) &#123; //正常不会等于null ackEntry.origin.getData() 和 ackEntry.data 数据是一样的 //ackEntry.origin.getData() 是字节数组，ackEntry.data 是map集合 cache.put(key, new org.javatuples.Pair&lt;&gt;(ackEntry.origin.getData(), ackEntry.data)); &#125; &#125; Loggers.PUSH.info(&quot;serviceName: &#123;&#125; changed, schedule push for: &#123;&#125;, agent: &#123;&#125;, key: &#123;&#125;&quot;, client.getServiceName(), client.getAddrStr(), client.getAgent(), (ackEntry == null ? null : ackEntry.key)); //把实例表发送给订阅的客户端 udpPush(ackEntry); &#125; &#125; catch (Exception e) &#123; Loggers.PUSH.error(&quot;[NACOS-PUSH] failed to push serviceName: &#123;&#125; to client, error: &#123;&#125;&quot;, serviceName, e); &#125; finally &#123; futureMap.remove(UtilsAndCommons.assembleFullServiceName(namespaceId, serviceName)); &#125; &#125;, 1000, TimeUnit.MILLISECONDS); futureMap.put(UtilsAndCommons.assembleFullServiceName(namespaceId, serviceName), future); &#125; private static Receiver.AckEntry udpPush(Receiver.AckEntry ackEntry) &#123; if (ackEntry == null) &#123; Loggers.PUSH.error(&quot;[NACOS-PUSH] ackEntry is null.&quot;); return null; &#125; //重试1次，正常会发送1次，没收到ack就认为是失败了,那么就重试1次，如果还是失败了，会进入这个if if (ackEntry.getRetryTimes() &gt; MAX_RETRY_TIMES) &#123; Loggers.PUSH.warn(&quot;max re-push times reached, retry times &#123;&#125;, key: &#123;&#125;&quot;, ackEntry.retryTimes, ackEntry.key); ackMap.remove(ackEntry.key); udpSendTimeMap.remove(ackEntry.key); //统计 failedPush += 1; return ackEntry; &#125; try &#123; if (!ackMap.containsKey(ackEntry.key)) &#123; //统计 totalPush++; &#125; //put 进集合 ackMap.put(ackEntry.key, ackEntry); udpSendTimeMap.put(ackEntry.key, System.currentTimeMillis()); Loggers.PUSH.info(&quot;send udp packet: &quot; + ackEntry.key); //发送 udpSocket.send(ackEntry.origin); //重试加1 ackEntry.increaseRetryTime(); //延迟10秒 GlobalExecutor.scheduleRetransmitter(new Retransmitter(ackEntry), TimeUnit.NANOSECONDS.toMillis(ACK_TIMEOUT_NANOS), TimeUnit.MILLISECONDS); return ackEntry; &#125; catch (Exception e) &#123; //如果有异常，终止这次推送 Loggers.PUSH.error(&quot;[NACOS-PUSH] failed to push data: &#123;&#125; to client: &#123;&#125;, error: &#123;&#125;&quot;, ackEntry.data, ackEntry.origin.getAddress().getHostAddress(), e); ackMap.remove(ackEntry.key); udpSendTimeMap.remove(ackEntry.key); failedPush += 1; return null; &#125; &#125; public static class Retransmitter implements Runnable &#123; Receiver.AckEntry ackEntry; public Retransmitter(Receiver.AckEntry ackEntry) &#123; this.ackEntry = ackEntry; &#125; @Override public void run() &#123; //收到ack,后会从ackMap删除，不会在进入这个if中 if (ackMap.containsKey(ackEntry.key)) &#123; Loggers.PUSH.info(&quot;retry to push data, key: &quot; + ackEntry.key); udpPush(ackEntry); &#125; &#125; &#125; public static class Receiver implements Runnable &#123; @Override public void run() &#123; while (true) &#123; byte[] buffer = new byte[1024 * 64]; DatagramPacket packet = new DatagramPacket(buffer, buffer.length); try &#123; udpSocket.receive(packet); String json = new String(packet.getData(), 0, packet.getLength(), StandardCharsets.UTF_8).trim(); AckPacket ackPacket = JacksonUtils.toObj(json, AckPacket.class); InetSocketAddress socketAddress = (InetSocketAddress) packet.getSocketAddress(); String ip = socketAddress.getAddress().getHostAddress(); int port = socketAddress.getPort(); if (System.nanoTime() - ackPacket.lastRefTime &gt; ACK_TIMEOUT_NANOS) &#123; Loggers.PUSH.warn(&quot;ack takes too long from &#123;&#125; ack json: &#123;&#125;&quot;, packet.getSocketAddress(), json); &#125; String ackKey = getAckKey(ip, port, ackPacket.lastRefTime); //就是这里，删除 AckEntry ackEntry = ackMap.remove(ackKey); if (ackEntry == null) &#123; throw new IllegalStateException( &quot;unable to find ackEntry for key: &quot; + ackKey + &quot;, ack json: &quot; + json); &#125; long pushCost = System.currentTimeMillis() - udpSendTimeMap.get(ackKey); Loggers.PUSH .info(&quot;received ack: &#123;&#125; from: &#123;&#125;:&#123;&#125;, cost: &#123;&#125; ms, unacked: &#123;&#125;, total push: &#123;&#125;&quot;, json, ip, port, pushCost, ackMap.size(), totalPush); pushCostMap.put(ackKey, pushCost); udpSendTimeMap.remove(ackKey); &#125; catch (Throwable e) &#123; Loggers.PUSH.error(&quot;[NACOS-PUSH] error while receiving ack data&quot;, e); &#125; &#125; &#125; &#125;&#125; todo InstancesChangeEvent 没画出来，有空在画 InstancesChangeEvent 怎么使用? nacos源码有这个例子 com.alibaba.nacos.example.NamingExample 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Nacos naming example. * * @author nkorange */public class NamingExample &#123; public static void main(String[] args) throws NacosException &#123; Properties properties = new Properties(); properties.setProperty(&quot;serverAddr&quot;, System.getProperty(&quot;serverAddr&quot;)); properties.setProperty(&quot;namespace&quot;, System.getProperty(&quot;namespace&quot;)); //创建 NamingService NamingService naming = NamingFactory.createNamingService(properties); //注册实例 naming.registerInstance(&quot;nacos.test.3&quot;, &quot;11.11.11.11&quot;, 8888, &quot;TEST1&quot;); naming.registerInstance(&quot;nacos.test.3&quot;, &quot;2.2.2.2&quot;, 9999, &quot;DEFAULT&quot;); System.out.println(naming.getAllInstances(&quot;nacos.test.3&quot;)); naming.deregisterInstance(&quot;nacos.test.3&quot;, &quot;2.2.2.2&quot;, 9999, &quot;DEFAULT&quot;); System.out.println(naming.getAllInstances(&quot;nacos.test.3&quot;)); Executor executor = new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), new ThreadFactory() &#123; @Override public Thread newThread(Runnable r) &#123; Thread thread = new Thread(r); thread.setName(&quot;test-thread&quot;); return thread; &#125; &#125;); //就是这个，一旦收到 InstancesChangeEvent ，onEvent 就会被调用，观察者模式 naming.subscribe(&quot;nacos.test.3&quot;, new AbstractEventListener() &#123; //EventListener onEvent is sync to handle, If process too low in onEvent, maybe block other onEvent callback. //So you can override getExecutor() to async handle event. @Override public Executor getExecutor() &#123; return executor; &#125; @Override public void onEvent(Event event) &#123; System.out.println(((NamingEvent) event).getServiceName()); System.out.println(((NamingEvent) event).getInstances()); &#125; &#125;); &#125;&#125; NotifyCenter 简单介绍","categories":[{"name":"spring cloud alibaba","slug":"spring-cloud-alibaba","permalink":"https://xxnjdg.github.io/categories/spring-cloud-alibaba/"}],"tags":[{"name":"nacos","slug":"nacos","permalink":"https://xxnjdg.github.io/tags/nacos/"}]},{"title":"nacos注册中心AP模式注册笔记","slug":"nacos-2","date":"2021-02-02T13:39:48.000Z","updated":"2021-02-14T10:33:41.126Z","comments":true,"path":"2021/02/02/nacos-2/","link":"","permalink":"https://xxnjdg.github.io/2021/02/02/nacos-2/","excerpt":"","text":"nacos 版本 1.4.1 客户端非持久化注册，AP模式引入下面包，启动时会自动注册，默认非持久化注册，使用AP方式 1234567891011&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; NacosNamingService调用 com.alibaba.nacos.api.NacosFactory#createNamingService(java.util.Properties) 创建 NacosNamingService 12345678910111213141516171819202122232425262728293031public class NacosNamingService implements NamingService &#123; /** * Each Naming service should have different namespace. * 如果 nacosDiscoveryProperties.getNamespace() 没有修改，使用默认public */ private String namespace; //Properties 的 endpoint 属性获取 //地域的某个服务的入口域名，通过此域名可以动态地拿到服务端地址 //endpoint 和 serverList二选一，不能同时存在，都配了，endpoint 优先级高 private String endpoint; //Properties 的 serverAddr 属性获取 //Nacos Server 启动监听的ip地址和端口,可以写多个地址，用逗号分割 //endpoint 和 serverList二选一，不能同时存在，都配了，endpoint 优先级高 private String serverList; //缓存文件夹 private String cacheDir; //log名字 private String logName; private HostReactor hostReactor; //和发送心跳相关 private BeatReactor beatReactor; private NamingProxy serverProxy;&#125; NamingProxy12345678910111213141516171819202122232425262728293031323334353637383940414243public class NamingProxy implements Closeable &#123; //http通信工具 private final NacosRestTemplate nacosRestTemplate = NamingHttpClientManager.getInstance().getNacosRestTemplate(); private static final int DEFAULT_SERVER_PORT = 8848; private int serverPort = DEFAULT_SERVER_PORT; //namespace 名称 private final String namespaceId; //同 NacosNamingService.endpoint private final String endpoint; //如果 serverList 长度等于1，即单机模式 // nacosDomain就等于serverList private String nacosDomain; //NacosNamingService.serverList.split(&quot;,&quot;) 分割后数组 private List&lt;String&gt; serverList; //从 endpoint 获取的服务列表 private List&lt;String&gt; serversFromEndpoint = new ArrayList&lt;String&gt;(); //和获取token有关 private final SecurityProxy securityProxy; private long lastSrvRefTime = 0L; private final long vipSrvRefInterMillis = TimeUnit.SECONDS.toMillis(30); private final long securityInfoRefreshIntervalMills = TimeUnit.SECONDS.toMillis(5); //Properties private Properties properties; //定时任务线城池 private ScheduledExecutorService executorService; //单机模式最大重试，默认3 private int maxRetry;&#125; 定时任务线程名字 com.alibaba.nacos.client.naming.updater 每30秒循环执行一次，refreshSrvIfNeed() ,解析 endpoint 地址，动态请求获取服务地址，配了 serverList 无视这个任务 每5秒循环执行一次，securityProxy.login(getServerList()) 请求 /nacos/v1/auth/users/login 获取 accessToken，如果没有配置 spring.cloud.nacos.discovery.username 和 spring.cloud.nacos.discovery.password 不会去请求 HostReactor这个类和服务发现有关，在服务发现讲解 registerInstance创建 NacosNamingService 后会调用 public void registerInstance(String serviceName, String groupName, Instance instance) throws NacosException 注册一个实例 12345678910111213141516public class NacosNamingService implements NamingService &#123; @Override public void registerInstance(String serviceName, String groupName, Instance instance) throws NacosException &#123; NamingUtils.checkInstanceIsLegal(instance); //groupName + Constants.SERVICE_INFO_SPLITER + serviceName String groupedServiceName = NamingUtils.getGroupedName(serviceName, groupName); if (instance.isEphemeral()) &#123; //构造 beatInfo BeatInfo beatInfo = beatReactor.buildBeatInfo(groupedServiceName, instance); //2) 启动一个线程隔5秒发送心跳 beatReactor.addBeatInfo(groupedServiceName, beatInfo); &#125; //1) post 请求 /nacos/v1/ns/instance 注册 serverProxy.registerService(groupedServiceName, groupName, instance); &#125;&#125; 注册请求 /nacos/v1/ns/instance 会带上参数，整理了下，基本上就是根据 nacosDiscoveryProperties 构造以下参数 123456789101112131415161718192021params.put(CommonParams.NAMESPACE_ID, nacosDiscoveryProperties.getNamespace()/public);params.put(CommonParams.SERVICE_NAME, nacosDiscoveryProperties.getGroup() + @@ + nacosDiscoveryProperties.getService());params.put(CommonParams.GROUP_NAME, nacosDiscoveryProperties.getGroup());params.put(CommonParams.CLUSTER_NAME, nacosDiscoveryProperties.getClusterName());params.put(&quot;ip&quot;, nacosDiscoveryProperties.getIp());params.put(&quot;port&quot;, String.valueOf(nacosDiscoveryProperties.getPort()));params.put(&quot;weight&quot;, String.valueOf(nacosDiscoveryProperties.getWeight()));params.put(&quot;enable&quot;, String.valueOf(nacosDiscoveryProperties.isInstanceEnabled()));params.put(&quot;healthy&quot;, String.valueOf(instance.isHealthy() = true));params.put(&quot;ephemeral&quot;, String.valueOf(nacosDiscoveryProperties.isEphemeral()));params.put(&quot;metadata&quot;, JacksonUtils.toJson(nacosDiscoveryProperties.getMetadata()));如果token存在params.put(Constants.ACCESS_TOKEN, securityProxy.getAccessToken());如果 ak/sk存在String signData = getSignData(params.get(&quot;serviceName&quot;));String signature = SignUtil.sign(signData, sk);params.put(&quot;signature&quot;, signature);params.put(&quot;data&quot;, signData);params.put(&quot;ak&quot;, ak); 定时任务-发送心跳可以先跳去看服务端注册，因为发送心跳会延时5s执行，网络正常的情况下，先注册在发送心跳 123456789BeatInfo beatInfo = new BeatInfo();beatInfo.setServiceName(nacosDiscoveryProperties.getGroup() + @@ + nacosDiscoveryProperties.getService());beatInfo.setIp(nacosDiscoveryProperties.getIp());beatInfo.setPort(nacosDiscoveryProperties.getPort());beatInfo.setCluster(nacosDiscoveryProperties.getClusterName());beatInfo.setWeight(nacosDiscoveryProperties.getWeight());beatInfo.setMetadata(nacosDiscoveryProperties.getMetadata());beatInfo.setScheduled(false);beatInfo.setPeriod(instance.getInstanceHeartBeatInterval());//默认5秒 123456789101112131415public class BeatReactor implements Closeable &#123; public void addBeatInfo(String serviceName, BeatInfo beatInfo) &#123; NAMING_LOGGER.info(&quot;[BEAT] adding beat: &#123;&#125; to beat map.&quot;, beatInfo); String key = buildKey(serviceName, beatInfo.getIp(), beatInfo.getPort()); BeatInfo existBeat = null; //fix #1733 if ((existBeat = dom2Beat.remove(key)) != null) &#123; existBeat.setStopped(true); &#125; dom2Beat.put(key, beatInfo); //默认5秒循环执行一次 executorService.schedule(new BeatTask(beatInfo), beatInfo.getPeriod(), TimeUnit.MILLISECONDS); MetricsMonitor.getDom2BeatSizeMonitor().set(dom2Beat.size()); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class BeatTask implements Runnable &#123; @Override public void run() &#123; if (beatInfo.isStopped()) &#123; return; &#125; //5000 long nextTime = beatInfo.getPeriod(); try &#123; //发送心跳 // put 请求 /nacos/v1/ns/instance/beat，文章后面详细讲服务端心跳处理 JsonNode result = serverProxy.sendBeat(beatInfo, BeatReactor.this.lightBeatEnabled); long interval = result.get(&quot;clientBeatInterval&quot;).asLong(); boolean lightBeatEnabled = false; if (result.has(CommonParams.LIGHT_BEAT_ENABLED)) &#123; //发完心跳后设置为true lightBeatEnabled = result.get(CommonParams.LIGHT_BEAT_ENABLED).asBoolean(); &#125; BeatReactor.this.lightBeatEnabled = lightBeatEnabled; if (interval &gt; 0) &#123; nextTime = interval; &#125; int code = NamingResponseCode.OK; if (result.has(CommonParams.CODE)) &#123; code = result.get(CommonParams.CODE).asInt(); &#125; if (code == NamingResponseCode.RESOURCE_NOT_FOUND) &#123; //之前是发过心跳，但是心跳到了删除超时时间没发心跳，导致自动删除了 Instance instance = new Instance(); instance.setPort(beatInfo.getPort()); instance.setIp(beatInfo.getIp()); instance.setWeight(beatInfo.getWeight()); instance.setMetadata(beatInfo.getMetadata()); instance.setClusterName(beatInfo.getCluster()); instance.setServiceName(beatInfo.getServiceName()); instance.setInstanceId(instance.getInstanceId()); instance.setEphemeral(true); try &#123; //重新注册 serverProxy.registerService(beatInfo.getServiceName(), NamingUtils.getGroupName(beatInfo.getServiceName()), instance); &#125; catch (Exception ignore) &#123; &#125; &#125; &#125; catch (NacosException ex) &#123; NAMING_LOGGER.error(&quot;[CLIENT-BEAT] failed to send beat: &#123;&#125;, code: &#123;&#125;, msg: &#123;&#125;&quot;, JacksonUtils.toJson(beatInfo), ex.getErrCode(), ex.getErrMsg()); &#125; //重新调度 executorService.schedule(new BeatTask(beatInfo), nextTime, TimeUnit.MILLISECONDS); &#125; &#125; put 请求 /nacos/v1/ns/instance/beat，请求参数 123456789101112131415params.put(CommonParams.NAMESPACE_ID, nacosDiscoveryProperties.getNamespace()/public);params.put(CommonParams.SERVICE_NAME, nacosDiscoveryProperties.getGroup() + @@ + nacosDiscoveryProperties.getService());params.put(CommonParams.CLUSTER_NAME, nacosDiscoveryProperties.getClusterName());params.put(&quot;ip&quot;, nacosDiscoveryProperties.getIp());params.put(&quot;port&quot;, String.valueOf(nacosDiscoveryProperties.getPort()));bodyMap.put(&quot;beat&quot;, JacksonUtils.toJson(beatInfo));如果token存在params.put(Constants.ACCESS_TOKEN, securityProxy.getAccessToken());如果 ak/sk存在String signData = getSignData(params.get(&quot;serviceName&quot;));String signature = SignUtil.sign(signData, sk);params.put(&quot;signature&quot;, signature);params.put(&quot;data&quot;, signData);params.put(&quot;ak&quot;, ak); Service-Cluster-Instance 模型简单介绍123456789@Componentpublic class ServiceManager implements RecordListener&lt;Service&gt; &#123; /** * Map(namespace, Map(group::serviceName, Service)). * 管理多个 Service */ private final Map&lt;String, Map&lt;String, Service&gt;&gt; serviceMap = new ConcurrentHashMap&lt;&gt;();&#125; 1234@JsonInclude(Include.NON_NULL)public class Service extends com.alibaba.nacos.api.naming.pojo.Service implements Record, RecordListener&lt;Instances&gt; &#123; private Map&lt;String, Cluster&gt; clusterMap = new HashMap&lt;&gt;();&#125; 12345678public class Cluster extends com.alibaba.nacos.api.naming.pojo.Cluster implements Cloneable &#123; @JsonIgnore private Set&lt;Instance&gt; persistentInstances = new HashSet&lt;&gt;(); @JsonIgnore private Set&lt;Instance&gt; ephemeralInstances = new HashSet&lt;&gt;();&#125; consistencyService 简单介绍 服务端注册com.alibaba.nacos.naming.controllers.InstanceController#register 方法接受请求 1 根据传入的参数构造 com.alibaba.nacos.naming.core.Instance 1234567891011121314151617181920@RestController@RequestMapping(UtilsAndCommons.NACOS_NAMING_CONTEXT + &quot;/instance&quot;)public class InstanceController &#123; @CanDistro @PostMapping @Secured(parser = NamingResourceParser.class, action = ActionTypes.WRITE) public String register(HttpServletRequest request) throws Exception &#123; final String namespaceId = WebUtils .optional(request, CommonParams.NAMESPACE_ID, Constants.DEFAULT_NAMESPACE_ID); final String serviceName = WebUtils.required(request, CommonParams.SERVICE_NAME); NamingUtils.checkServiceNameFormat(serviceName); //根据接受到的参数构造 Instance final Instance instance = parseInstance(request); serviceManager.registerInstance(namespaceId, serviceName, instance); return &quot;ok&quot;; &#125;&#125; instance 的值整理了下 123456789101112instance.setPort(nacosDiscoveryProperties.getIp());instance.setIp(nacosDiscoveryProperties.getIp());instance.setEphemeral(String.valueOf(nacosDiscoveryProperties.isEphemeral()));instance.setClusterName(nacosDiscoveryProperties.getClusterName());instance.setWeight(Double.parseDouble(String.valueOf(nacosDiscoveryProperties.getWeight())));instance.setHealthy(String.valueOf(instance.isHealthy() = true));instance.setEnabled(String.valueOf(nacosDiscoveryProperties.isInstanceEnabled()));instance.setApp(&quot;DEFAULT&quot;);instance.setServiceName(groupName + @@ + serviceName);instance.setInstanceId(getIp() + &quot;#&quot; + getPort() + &quot;#&quot; + getClusterName() + &quot;#&quot; + getServiceName(););instance.setLastBeat(System.currentTimeMillis());instance.setMetadata(UtilsAndCommons.parseMetadata(JacksonUtils.toJson(nacosDiscoveryProperties.getMetadata()))); 12345678910111213141516171819@Componentpublic class ServiceManager implements RecordListener&lt;Service&gt; &#123; public void registerInstance(String namespaceId, String serviceName, Instance instance) throws NacosException &#123; //获取 Service，如果不存在创建服务 createEmptyService(namespaceId, serviceName, instance.isEphemeral()); //获取 Service Service service = getService(namespaceId, serviceName); if (service == null) &#123; throw new NacosException(NacosException.INVALID_PARAM, &quot;service not found, namespace: &quot; + namespaceId + &quot;, service: &quot; + serviceName); &#125; //加入实例 addInstance(namespaceId, serviceName, instance.isEphemeral(), instance); &#125;&#125; 2 获取 Service， namespace 和 group::serviceName 确定唯一 Service，如果没有则创建 Service，consistencyService 监听 Service，一旦 consistencyService 数据更新了，告诉 Service 更新 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Componentpublic class ServiceManager implements RecordListener&lt;Service&gt; &#123; public void createServiceIfAbsent(String namespaceId, String serviceName, boolean local, Cluster cluster) throws NacosException &#123; Service service = getService(namespaceId, serviceName); //如果没有 service 创建 if (service == null) &#123; Loggers.SRV_LOG.info(&quot;creating empty service &#123;&#125;:&#123;&#125;&quot;, namespaceId, serviceName); service = new Service(); //设置属性 service.setName(serviceName); service.setNamespaceId(namespaceId); service.setGroupName(NamingUtils.getGroupName(serviceName)); // now validate the service. if failed, exception will be thrown // 现在验证服务。 如果失败，将引发异常 service.setLastModifiedMillis(System.currentTimeMillis()); service.recalculateChecksum(); if (cluster != null) &#123; cluster.setService(service); service.getClusterMap().put(cluster.getName(), cluster); &#125; service.validate(); //重点方法 putServiceAndInit(service); if (!local) &#123; addOrReplaceService(service); &#125; &#125; &#125; private void putServiceAndInit(Service service) throws NacosException &#123; //放入 serviceMap putService(service); //初始化,后面定时任务讲解 service.init(); //consistencyService 监听 Service，一旦 consistencyService 数据更新了，告诉 Service 更新 //非持久化实例关注这个 consistencyService .listen(KeyBuilder.buildInstanceListKey(service.getNamespaceId(), service.getName(), true), service); //非持久化实例不关心这个 consistencyService .listen(KeyBuilder.buildInstanceListKey(service.getNamespaceId(), service.getName(), false), service); Loggers.SRV_LOG.info(&quot;[NEW-SERVICE] &#123;&#125;&quot;, service.toJson()); &#125;&#125; 3 service 下所有旧实例更新一些属性后和新实例合并一起，这里的旧实例，看ephemeral参数，true是非持久化实例，false是持久化实例，这里是非持久化实例， 把合并后实例列表更新进 consistencyService addIpAddresses方法，会获取Cluster，名字不同确定唯一 Cluster ，如果没有创建 com.alibaba.nacos.naming.core.Cluster 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394@Componentpublic class ServiceManager implements RecordListener&lt;Service&gt; &#123; public void addInstance(String namespaceId, String serviceName, boolean ephemeral, Instance... ips) throws NacosException &#123; //构造一个key,这个key 用于 consistencyService String key = KeyBuilder.buildInstanceListKey(namespaceId, serviceName, ephemeral); //获取 Service Service service = getService(namespaceId, serviceName); synchronized (service) &#123; //service 下所有旧实例更新一些属性后和新实例合并一起， //这里的旧实例，看ephemeral参数，true是非持久化实例，false是持久化实例 //这里是非持久化实例 List&lt;Instance&gt; instanceList = addIpAddresses(service, ephemeral, ips); Instances instances = new Instances(); instances.setInstanceList(instanceList); // 把合并后实例列表更新进 consistencyService consistencyService.put(key, instances); &#125; &#125; private List&lt;Instance&gt; addIpAddresses(Service service, boolean ephemeral, Instance... ips) throws NacosException &#123; return updateIpAddresses(service, UtilsAndCommons.UPDATE_INSTANCE_ACTION_ADD, ephemeral, ips); &#125; public List&lt;Instance&gt; updateIpAddresses(Service service, String action, boolean ephemeral, Instance... ips) throws NacosException &#123; //获取 Datum,存实例列表 Datum datum = consistencyService .get(KeyBuilder.buildInstanceListKey(service.getNamespaceId(), service.getName(), ephemeral)); //获取所有的非持久化实例 List&lt;Instance&gt; currentIPs = service.allIPs(ephemeral); Map&lt;String, Instance&gt; currentInstances = new HashMap&lt;&gt;(currentIPs.size()); Set&lt;String&gt; currentInstanceIds = Sets.newHashSet(); for (Instance instance : currentIPs) &#123; currentInstances.put(instance.toIpAddr(), instance); currentInstanceIds.add(instance.getInstanceId()); &#125; Map&lt;String, Instance&gt; instanceMap; if (datum != null &amp;&amp; null != datum.value) &#123; //Cluster 比 Datum 新？ //是的，Service 定时任务会修改 Cluster 下非持久化 Instance.healthy 属性，后面定时任务详细讲 //客户端发送心跳，服务端处理心跳会修改 Cluster 下非持久化 Instance.healthy 和 Instance.lastBeat 属性 instanceMap = setValid(((Instances) datum.value).getInstanceList(), currentInstances); &#125; else &#123; //这种情况就是没有注册过，要不就是实例没有心跳，自动剔除实例，最后一个都不剩 instanceMap = new HashMap&lt;&gt;(ips.length); &#125; for (Instance instance : ips) &#123; //如果 instance.getClusterName() 对应的 Cluster 没有创建 if (!service.getClusterMap().containsKey(instance.getClusterName())) &#123; Cluster cluster = new Cluster(instance.getClusterName(), service); cluster.init(); service.getClusterMap().put(instance.getClusterName(), cluster); Loggers.SRV_LOG .warn(&quot;cluster: &#123;&#125; not found, ip: &#123;&#125;, will create new cluster with default configuration.&quot;, instance.getClusterName(), instance.toJson()); &#125; if (UtilsAndCommons.UPDATE_INSTANCE_ACTION_REMOVE.equals(action)) &#123; //移除实例 instanceMap.remove(instance.getDatumKey()); &#125; else &#123; //ADD action Instance oldInstance = instanceMap.get(instance.getDatumKey()); //重新设置 instanceId if (oldInstance != null) &#123; instance.setInstanceId(oldInstance.getInstanceId()); &#125; else &#123; instance.setInstanceId(instance.generateInstanceId(currentInstanceIds)); &#125; instanceMap.put(instance.getDatumKey(), instance); &#125; &#125; if (instanceMap.size() &lt;= 0 &amp;&amp; UtilsAndCommons.UPDATE_INSTANCE_ACTION_ADD.equals(action)) &#123; throw new IllegalArgumentException( &quot;ip list can not be empty, service: &quot; + service.getName() + &quot;, ip list: &quot; + JacksonUtils .toJson(instanceMap.values())); &#125; return new ArrayList&lt;&gt;(instanceMap.values()); &#125;&#125; 4 实际就是DataStore存入合并后实例，回调监听的 Service 12345678910111213141516171819202122232425262728293031@DependsOn(&quot;ProtocolManager&quot;)@org.springframework.stereotype.Service(&quot;distroConsistencyService&quot;)public class DistroConsistencyServiceImpl implements EphemeralConsistencyService, DistroDataProcessor &#123; @Override public void put(String key, Record value) throws NacosException &#123; //将注册实例更新到内存注册表中 onPut(key, value); //复制新数据到服务集群中 distroProtocol.sync(new DistroKey(key, KeyBuilder.INSTANCE_LIST_KEY_PREFIX), DataOperation.CHANGE, globalConfig.getTaskDispatchPeriod() / 2); &#125; public void onPut(String key, Record value) &#123; if (KeyBuilder.matchEphemeralInstanceListKey(key)) &#123; Datum&lt;Instances&gt; datum = new Datum&lt;&gt;(); datum.value = (Instances) value; datum.key = key; datum.timestamp.incrementAndGet(); //存入内存注册表 dataStore.put(key, datum); &#125; if (!listeners.containsKey(key)) &#123; return; &#125; //回调监听的 Service，注意key值要相同 notifier.addTask(key, DataOperation.CHANGE); &#125;&#125; 5 Service被告知数据更新了，把新合并后实例更新到 Cluster 相应实例集合 从这里开始会出现以下方法，这个方法和服务发现有关，在服务发现讲解，暂时跳过 12getPushService().serviceChanged(this);getPushService().serviceChanged(service); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293@JsonInclude(Include.NON_NULL)public class Service extends com.alibaba.nacos.api.naming.pojo.Service implements Record, RecordListener&lt;Instances&gt; &#123; //当 Instances 更新了就会调用这个方法 @Override public void onChange(String key, Instances value) throws Exception &#123; Loggers.SRV_LOG.info(&quot;[NACOS-RAFT] datum is changed, key: &#123;&#125;, value: &#123;&#125;&quot;, key, value); for (Instance instance : value.getInstanceList()) &#123; if (instance == null) &#123; // Reject this abnormal instance list: throw new RuntimeException(&quot;got null instance &quot; + key); &#125; if (instance.getWeight() &gt; 10000.0D) &#123; instance.setWeight(10000.0D); &#125; if (instance.getWeight() &lt; 0.01D &amp;&amp; instance.getWeight() &gt; 0.0D) &#123; instance.setWeight(0.01D); &#125; &#125; //重点方法 updateIPs(value.getInstanceList(), KeyBuilder.matchEphemeralInstanceListKey(key)); recalculateChecksum(); &#125; public void updateIPs(Collection&lt;Instance&gt; instances, boolean ephemeral) &#123; Map&lt;String, List&lt;Instance&gt;&gt; ipMap = new HashMap&lt;&gt;(clusterMap.size()); for (String clusterName : clusterMap.keySet()) &#123; ipMap.put(clusterName, new ArrayList&lt;&gt;()); &#125; for (Instance instance : instances) &#123; try &#123; if (instance == null) &#123; Loggers.SRV_LOG.error(&quot;[NACOS-DOM] received malformed ip: null&quot;); continue; &#125; if (StringUtils.isEmpty(instance.getClusterName())) &#123; instance.setClusterName(UtilsAndCommons.DEFAULT_CLUSTER_NAME); &#125; //如果对应的 Cluster 不存在 if (!clusterMap.containsKey(instance.getClusterName())) &#123; Loggers.SRV_LOG .warn(&quot;cluster: &#123;&#125; not found, ip: &#123;&#125;, will create new cluster with default configuration.&quot;, instance.getClusterName(), instance.toJson()); //创建 Cluster Cluster cluster = new Cluster(instance.getClusterName(), this); //初始化，定时任务讲解 cluster.init(); getClusterMap().put(instance.getClusterName(), cluster); &#125; //Cluster 不存在时，重新创建集合 List&lt;Instance&gt; clusterIPs = ipMap.get(instance.getClusterName()); if (clusterIPs == null) &#123; clusterIPs = new LinkedList&lt;&gt;(); ipMap.put(instance.getClusterName(), clusterIPs); &#125; //把实例放入 clusterIPs clusterIPs.add(instance); &#125; catch (Exception e) &#123; Loggers.SRV_LOG.error(&quot;[NACOS-DOM] failed to process ip: &quot; + instance, e); &#125; &#125; for (Map.Entry&lt;String, List&lt;Instance&gt;&gt; entry : ipMap.entrySet()) &#123; //make every ip mine List&lt;Instance&gt; entryIPs = entry.getValue(); //新合并后实例更新到 Cluster 相应实例集合 clusterMap.get(entry.getKey()).updateIps(entryIPs, ephemeral); &#125; setLastModifiedMillis(System.currentTimeMillis()); getPushService().serviceChanged(this); StringBuilder stringBuilder = new StringBuilder(); for (Instance instance : allIPs()) &#123; stringBuilder.append(instance.toIpAddr()).append(&quot;_&quot;).append(instance.isHealthy()).append(&quot;,&quot;); &#125; Loggers.EVT_LOG.info(&quot;[IP-UPDATED] namespace: &#123;&#125;, service: &#123;&#125;, ips: &#123;&#125;&quot;, getNamespaceId(), getName(), stringBuilder.toString()); &#125;&#125; 6 更新 ephemeralInstances 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class Cluster extends com.alibaba.nacos.api.naming.pojo.Cluster implements Cloneable &#123; public void updateIps(List&lt;Instance&gt; ips, boolean ephemeral) &#123; Set&lt;Instance&gt; toUpdateInstances = ephemeral ? ephemeralInstances : persistentInstances; HashMap&lt;String, Instance&gt; oldIpMap = new HashMap&lt;&gt;(toUpdateInstances.size()); //从 toUpdateInstances 获取旧数据 for (Instance ip : toUpdateInstances) &#123; oldIpMap.put(ip.getDatumKey(), ip); &#125; //ips 和 老集合对比找出需要更新的实例集合 List&lt;Instance&gt; updatedIPs = updatedIps(ips, oldIpMap.values()); if (updatedIPs.size() &gt; 0) &#123; for (Instance ip : updatedIPs) &#123; Instance oldIP = oldIpMap.get(ip.getDatumKey()); // do not update the ip validation status of updated ips // because the checker has the most precise result // Only when ip is not marked, don&#x27;t we update the health status of IP: if (!ip.isMarked()) &#123; ip.setHealthy(oldIP.isHealthy()); &#125; if (ip.isHealthy() != oldIP.isHealthy()) &#123; // ip validation status updated Loggers.EVT_LOG.info(&quot;&#123;&#125; &#123;SYNC&#125; IP-&#123;&#125; &#123;&#125;:&#123;&#125;@&#123;&#125;&quot;, getService().getName(), (ip.isHealthy() ? &quot;ENABLED&quot; : &quot;DISABLED&quot;), ip.getIp(), ip.getPort(), getName()); &#125; if (ip.getWeight() != oldIP.getWeight()) &#123; // ip validation status updated Loggers.EVT_LOG.info(&quot;&#123;&#125; &#123;SYNC&#125; &#123;IP-UPDATED&#125; &#123;&#125;-&gt;&#123;&#125;&quot;, getService().getName(), oldIP.toString(), ip.toString()); &#125; &#125; &#125; //新加入的实例 List&lt;Instance&gt; newIPs = subtract(ips, oldIpMap.values()); if (newIPs.size() &gt; 0) &#123; Loggers.EVT_LOG .info(&quot;&#123;&#125; &#123;SYNC&#125; &#123;IP-NEW&#125; cluster: &#123;&#125;, new ips size: &#123;&#125;, content: &#123;&#125;&quot;, getService().getName(), getName(), newIPs.size(), newIPs.toString()); for (Instance ip : newIPs) &#123; //初始化 HealthCheckStatus HealthCheckStatus.reset(ip); &#125; &#125; List&lt;Instance&gt; deadIPs = subtract(oldIpMap.values(), ips); if (deadIPs.size() &gt; 0) &#123; Loggers.EVT_LOG .info(&quot;&#123;&#125; &#123;SYNC&#125; &#123;IP-DEAD&#125; cluster: &#123;&#125;, dead ips size: &#123;&#125;, content: &#123;&#125;&quot;, getService().getName(), getName(), deadIPs.size(), deadIPs.toString()); for (Instance ip : deadIPs) &#123; HealthCheckStatus.remv(ip); &#125; &#125; toUpdateInstances = new HashSet&lt;&gt;(ips); if (ephemeral) &#123; //更新 toUpdateInstances ephemeralInstances = toUpdateInstances; &#125; else &#123; persistentInstances = toUpdateInstances; &#125; &#125;&#125; 定时任务在前面服务注册的时候创建了定时任务 1234567891011@JsonInclude(Include.NON_NULL)public class Service extends com.alibaba.nacos.api.naming.pojo.Service implements Record, RecordListener&lt;Instances&gt; &#123; public void init() &#123; //起一个调度任务 HealthCheckReactor.scheduleCheck(clientBeatCheckTask); for (Map.Entry&lt;String, Cluster&gt; entry : clusterMap.entrySet()) &#123; entry.getValue().setService(this); entry.getValue().init(); &#125; &#125;&#125; 每5秒循环执行 clientBeatCheckTask 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class ClientBeatCheckTask implements Runnable &#123; @Override public void run() &#123; try &#123; //单机模式返回 true if (!getDistroMapper().responsible(service.getName())) &#123; return; &#125; //默认为true if (!getSwitchDomain().isHealthCheckEnabled()) &#123; return; &#125; //获取非持久化实例 List&lt;Instance&gt; instances = service.allIPs(true); // first set health status of instances: for (Instance instance : instances) &#123; //目前时间 大于 实例最后更新时间 加 实例超时时间（默认15秒） if (System.currentTimeMillis() - instance.getLastBeat() &gt; instance.getInstanceHeartBeatTimeOut()) &#123; //默认 false if (!instance.isMarked()) &#123; if (instance.isHealthy()) &#123; //设置状态 instance.setHealthy(false); Loggers.EVT_LOG .info(&quot;&#123;POS&#125; &#123;IP-DISABLED&#125; valid: &#123;&#125;:&#123;&#125;@&#123;&#125;@&#123;&#125;, region: &#123;&#125;, msg: client timeout after &#123;&#125;, last beat: &#123;&#125;&quot;, instance.getIp(), instance.getPort(), instance.getClusterName(), service.getName(), UtilsAndCommons.LOCALHOST_SITE, instance.getInstanceHeartBeatTimeOut(), instance.getLastBeat()); getPushService().serviceChanged(service); //没有使用 ApplicationUtils.publishEvent(new InstanceHeartbeatTimeoutEvent(this, instance)); &#125; &#125; &#125; &#125; if (!getGlobalConfig().isExpireInstance()) &#123; return; &#125; // then remove obsolete instances: for (Instance instance : instances) &#123; //默认 false if (instance.isMarked()) &#123; continue; &#125; //当前时间 大于 实例最后更新时间 加 实例删除超时时间(默认30秒) if (System.currentTimeMillis() - instance.getLastBeat() &gt; instance.getIpDeleteTimeout()) &#123; // delete instance Loggers.SRV_LOG.info(&quot;[AUTO-DELETE-IP] service: &#123;&#125;, ip: &#123;&#125;&quot;, service.getName(), JacksonUtils.toJson(instance)); // delete 请求 /nacos/v1/ns/instance 删除实例，服务端处理文章后面详细讲 deleteIp(instance); &#125; &#125; &#125; catch (Exception e) &#123; Loggers.SRV_LOG.warn(&quot;Exception while processing client beat time out.&quot;, e); &#125; &#125;&#125; cluster.init(); 1234567891011public class Cluster extends com.alibaba.nacos.api.naming.pojo.Cluster implements Cloneable &#123; public void init() &#123; if (inited) &#123; return; &#125; checkTask = new HealthCheckTask(this); HealthCheckReactor.scheduleCheck(checkTask); inited = true; &#125;&#125; todo 这个任务主要检测持久化实例有没有挂，是服务端主动探测的 服务端删除实例Server 定时任务在删除实例超时后会主动删除实例，delete 请求 /nacos/v1/ns/instance 1234567891011121314151617181920212223242526272829303132333435363738@RestController@RequestMapping(UtilsAndCommons.NACOS_NAMING_CONTEXT + &quot;/instance&quot;)public class InstanceController &#123; @CanDistro @DeleteMapping @Secured(parser = NamingResourceParser.class, action = ActionTypes.WRITE) public String deregister(HttpServletRequest request) throws Exception &#123; Instance instance = getIpAddress(request); String namespaceId = WebUtils.optional(request, CommonParams.NAMESPACE_ID, Constants.DEFAULT_NAMESPACE_ID); String serviceName = WebUtils.required(request, CommonParams.SERVICE_NAME); NamingUtils.checkServiceNameFormat(serviceName); Service service = serviceManager.getService(namespaceId, serviceName); if (service == null) &#123; Loggers.SRV_LOG.warn(&quot;remove instance from non-exist service: &#123;&#125;&quot;, serviceName); return &quot;ok&quot;; &#125; //移除实例 serviceManager.removeInstance(namespaceId, serviceName, instance.isEphemeral(), instance); return &quot;ok&quot;; &#125;&#125;@Componentpublic class ServiceManager implements RecordListener&lt;Service&gt; &#123; private void removeInstance(String namespaceId, String serviceName, boolean ephemeral, Service service, Instance... ips) throws NacosException &#123; String key = KeyBuilder.buildInstanceListKey(namespaceId, serviceName, ephemeral); List&lt;Instance&gt; instanceList = substractIpAddresses(service, ephemeral, ips); Instances instances = new Instances(); instances.setInstanceList(instanceList); consistencyService.put(key, instances); &#125;&#125; 逻辑和服务注册差不多 服务端心跳处理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889@RestController@RequestMapping(UtilsAndCommons.NACOS_NAMING_CONTEXT + &quot;/instance&quot;)public class InstanceController &#123;@CanDistro @PutMapping(&quot;/beat&quot;) @Secured(parser = NamingResourceParser.class, action = ActionTypes.WRITE) public ObjectNode beat(HttpServletRequest request) throws Exception &#123; //开始解析参数 ObjectNode result = JacksonUtils.createEmptyJsonNode(); //默认5秒 result.put(SwitchEntry.CLIENT_BEAT_INTERVAL, switchDomain.getClientBeatInterval()); //第一次发心跳不会为空,第二次开始后面会一直为空 String beat = WebUtils.optional(request, &quot;beat&quot;, StringUtils.EMPTY); RsInfo clientBeat = null; if (StringUtils.isNotBlank(beat)) &#123; clientBeat = JacksonUtils.toObj(beat, RsInfo.class); &#125; String clusterName = WebUtils .optional(request, CommonParams.CLUSTER_NAME, UtilsAndCommons.DEFAULT_CLUSTER_NAME); String ip = WebUtils.optional(request, &quot;ip&quot;, StringUtils.EMPTY); int port = Integer.parseInt(WebUtils.optional(request, &quot;port&quot;, &quot;0&quot;)); if (clientBeat != null) &#123; if (StringUtils.isNotBlank(clientBeat.getCluster())) &#123; clusterName = clientBeat.getCluster(); &#125; else &#123; // fix #2533 clientBeat.setCluster(clusterName); &#125; ip = clientBeat.getIp(); port = clientBeat.getPort(); &#125; String namespaceId = WebUtils.optional(request, CommonParams.NAMESPACE_ID, Constants.DEFAULT_NAMESPACE_ID); String serviceName = WebUtils.required(request, CommonParams.SERVICE_NAME); NamingUtils.checkServiceNameFormat(serviceName); Loggers.SRV_LOG.debug(&quot;[CLIENT-BEAT] full arguments: beat: &#123;&#125;, serviceName: &#123;&#125;&quot;, clientBeat, serviceName); //用解析后的参数找到实例 Instance instance = serviceManager.getInstance(namespaceId, serviceName, clusterName, ip, port); if (instance == null) &#123; //实例为空，clientBeat也为空，之前是发过心跳，但是心跳到了删除超时时间没发心跳，导致自动删除了 if (clientBeat == null) &#123; result.put(CommonParams.CODE, NamingResponseCode.RESOURCE_NOT_FOUND); return result; &#125; Loggers.SRV_LOG.warn(&quot;[CLIENT-BEAT] The instance has been removed for health mechanism, &quot; + &quot;perform data compensation operations, beat: &#123;&#125;, serviceName: &#123;&#125;&quot;, clientBeat, serviceName); //实例为空，客户端没来得及注册，但是心跳先发了 instance = new Instance(); instance.setPort(clientBeat.getPort()); instance.setIp(clientBeat.getIp()); instance.setWeight(clientBeat.getWeight()); instance.setMetadata(clientBeat.getMetadata()); instance.setClusterName(clusterName); instance.setServiceName(serviceName); instance.setInstanceId(instance.getInstanceId()); instance.setEphemeral(clientBeat.isEphemeral()); //注册 serviceManager.registerInstance(namespaceId, serviceName, instance); &#125; Service service = serviceManager.getService(namespaceId, serviceName); if (service == null) &#123; throw new NacosException(NacosException.SERVER_ERROR, &quot;service not found: &quot; + serviceName + &quot;@&quot; + namespaceId); &#125; //不是第一次发心跳 if (clientBeat == null) &#123; clientBeat = new RsInfo(); clientBeat.setIp(ip); clientBeat.setPort(port); clientBeat.setCluster(clusterName); &#125; service.processClientBeat(clientBeat); result.put(CommonParams.CODE, NamingResponseCode.OK); if (instance.containsMetadata(PreservedMetadataKeys.HEART_BEAT_INTERVAL)) &#123; result.put(SwitchEntry.CLIENT_BEAT_INTERVAL, instance.getInstanceHeartBeatInterval()); &#125; //设置为 true 返回 result.put(SwitchEntry.LIGHT_BEAT_ENABLED, switchDomain.isLightBeatEnabled()); return result; &#125;&#125; 最后交给定时任务执行，设置属性 1234567891011121314151617181920212223242526272829303132333435363738public class ClientBeatProcessor implements Runnable &#123; @Override public void run() &#123; Service service = this.service; if (Loggers.EVT_LOG.isDebugEnabled()) &#123; Loggers.EVT_LOG.debug(&quot;[CLIENT-BEAT] processing beat: &#123;&#125;&quot;, rsInfo.toString()); &#125; String ip = rsInfo.getIp(); String clusterName = rsInfo.getCluster(); int port = rsInfo.getPort(); Cluster cluster = service.getClusterMap().get(clusterName); //获取非持久化实例 List&lt;Instance&gt; instances = cluster.allIPs(true); for (Instance instance : instances) &#123; if (instance.getIp().equals(ip) &amp;&amp; instance.getPort() == port) &#123; if (Loggers.EVT_LOG.isDebugEnabled()) &#123; Loggers.EVT_LOG.debug(&quot;[CLIENT-BEAT] refresh beat: &#123;&#125;&quot;, rsInfo.toString()); &#125; //设置最后心跳更新时间 instance.setLastBeat(System.currentTimeMillis()); if (!instance.isMarked()) &#123; //之前设置过false if (!instance.isHealthy()) &#123; //重新设置为ture instance.setHealthy(true); Loggers.EVT_LOG .info(&quot;service: &#123;&#125; &#123;POS&#125; &#123;IP-ENABLED&#125; valid: &#123;&#125;:&#123;&#125;@&#123;&#125;, region: &#123;&#125;, msg: client beat ok&quot;, cluster.getService().getName(), ip, port, cluster.getName(), UtilsAndCommons.LOCALHOST_SITE); getPushService().serviceChanged(service); &#125; &#125; &#125; &#125; &#125;&#125;","categories":[{"name":"spring cloud alibaba","slug":"spring-cloud-alibaba","permalink":"https://xxnjdg.github.io/categories/spring-cloud-alibaba/"}],"tags":[{"name":"nacos","slug":"nacos","permalink":"https://xxnjdg.github.io/tags/nacos/"}]},{"title":"spring-cloud-gateway源码笔记1","slug":"spring-cloud-gateway-one","date":"2021-01-26T09:29:15.000Z","updated":"2021-01-27T10:14:02.557Z","comments":true,"path":"2021/01/26/spring-cloud-gateway-one/","link":"","permalink":"https://xxnjdg.github.io/2021/01/26/spring-cloud-gateway-one/","excerpt":"","text":"克隆代码1git clone https://github.com/spring-cloud/spring-cloud-gateway.git 用idea打开后发现master分支pom.xml文件parent找不到版本，在阿里云仓库找不到，不知道什么原因 看了下github当前tag版本是v3.0.0 在idea切换v3.0.0的tag 还是有错误，是插件没指定版本，这个暂时不管他了，因为切到了tag,没有指定分支，所以这时候要new 一个自己的分支出来 项目要想使用gateway就需要加以下依赖 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 源码在如下的位置，它依赖了自身 spring-cloud-gateway-server 模块 接下来重点看下 spring-cloud-gateway-server 模块 spring-cloud-gateway-server自动配置 能确定的是在初始化 GatewayAutoConfiguration 配置之前有以下4个配置,这4个配置也没法办确定顺序，按顺序看好了，GatewayAutoConfiguration 配置之后的配置不属于这个项目 12345org.springframework.cloud.gateway.config.GatewayClassPathWarningAutoConfigurationorg.springframework.cloud.gateway.config.GatewayRedisAutoConfigurationorg.springframework.cloud.gateway.discovery.GatewayDiscoveryClientAutoConfigurationorg.springframework.cloud.gateway.config.GatewayReactiveLoadBalancerClientAutoConfigurationorg.springframework.cloud.gateway.config.GatewayAutoConfiguration 剩下的没法确定顺序，按顺序看就好了 12345org.springframework.cloud.gateway.config.GatewayResilience4JCircuitBreakerAutoConfigurationorg.springframework.cloud.gateway.config.GatewayNoLoadBalancerClientAutoConfigurationorg.springframework.cloud.gateway.config.GatewayMetricsAutoConfigurationorg.springframework.cloud.gateway.config.SimpleUrlHandlerMappingGlobalCorsAutoConfigurationorg.springframework.cloud.gateway.config.GatewayReactiveOAuth2AutoConfiguration DiscoveryClientRouteDefinitionLocatorReactiveLoadBalancerClientFilterRedisRateLimiter fromIterable PredicateArgsEvent","categories":[{"name":"spring cloud","slug":"spring-cloud","permalink":"https://xxnjdg.github.io/categories/spring-cloud/"}],"tags":[{"name":"spring-cloud-gateway","slug":"spring-cloud-gateway","permalink":"https://xxnjdg.github.io/tags/spring-cloud-gateway/"}]},{"title":"nacos学习笔记-1","slug":"nacos-1","date":"2021-01-25T13:39:48.000Z","updated":"2021-02-14T10:24:03.468Z","comments":true,"path":"2021/01/25/nacos-1/","link":"","permalink":"https://xxnjdg.github.io/2021/01/25/nacos-1/","excerpt":"","text":"下载与运行https://github.com/alibaba/nacos/releases 如果下载慢，可复制下面链接到迅雷等下载器下载 https://github.com.cnpmjs.org/alibaba/nacos/releases/download/1.4.1/nacos-server-1.4.1.zip 进入bin目录执行命令，以单机模式启动nacos 12345678910111213141516171819ddd@DESKTOP-OKBQRKQ MINGW64 /d/nacos-server-1.4.1/nacos/bin$ ./startup.cmd -m standalone&quot;nacos is starting with standalone&quot; ,--. ,--.&#x27;| ,--,: : | Nacos 1.4.1,`--.&#x27;`| &#x27; : ,---. Running in stand alone mode, All function modules| : : | | &#x27; ,&#x27;\\ .--.--. Port: 8848: | \\ | : ,--.--. ,---. / / | / / &#x27; Pid: 13260| : &#x27; &#x27;; | / \\ / \\. ; ,. :| : /`./ Console: http://192.168.56.1:8848/nacos/index.html&#x27; &#x27; ;. ;.--. .-. | / / &#x27;&#x27; | |: :| : ;_| | | \\ | \\__\\/: . .. &#x27; / &#x27; | .; : \\ \\ `. https://nacos.io&#x27; : | ; .&#x27; ,&quot; .--.; |&#x27; ; :__| : | `----. \\| | &#x27;`--&#x27; / / ,. |&#x27; | &#x27;.&#x27;|\\ \\ / / /`--&#x27; /&#x27; : | ; : .&#x27; \\ : : `----&#x27; &#x27;--&#x27;. /; |.&#x27; | , .-./\\ \\ / `--&#x27;---&#x27;&#x27;---&#x27; `--`---&#x27; `----&#x27; 浏览器输入 http://localhost:8848/nacos/ 访问nacos 账号和密码默认是 nacos 注册一个服务进nacos完整代码示例 添加依赖 1234567891011&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 修改 application.yml 1234567891011server: port: 27991spring: # 服务名 application: name: nacos-example-one # 注册中心nacos地址 cloud: nacos: discovery: server-addr: 192.168.100.14:8848 启动服务后，打开控制台，看到自己注册的服务名，表示成功注册 客户端配置属性 配置项 key 默认值 说明 服务端地址 spring.cloud.nacos.discovery.server-addr localhost:8848 Nacos Server 启动监听的ip地址和端口,可以写多个地址，用逗号分割 用户名 spring.cloud.nacos.discovery.username 要打开才生效 密码 spring.cloud.nacos.discovery.password 要打开才生效 接入点 spring.cloud.nacos.discovery.endpoint 地域的某个服务的入口域名，通过此域名可以动态地拿到服务端地址 命名空间 spring.cloud.nacos.discovery.namespace 常用场景之一是不同环境的注册的区分隔离，例如开发测试环境和生产环境的资源（如配置、服务）隔离等，clinet端不填，服务端默认给public nacos watch spring.cloud.nacos.discovery.watch-delay 30000ms todo 是否开启Nacos Watch spring.cloud.nacos.discovery.watch.enabled true 可以设置成false来关闭 watch 日志文件名 spring.cloud.nacos.discovery.log-name 服务名 spring.cloud.nacos.discovery.service ${spring.application.name} 注册的服务名 权重 spring.cloud.nacos.discovery.weight 1.0 取值范围 1 到 100，数值越大，权重越大 集群 spring.cloud.nacos.discovery.cluster-name DEFAULT Nacos集群名称 组 spring.cloud.nacos.discovery.group DEFAULT_GROUP Nacos组名称 服务发现列表缓存预加载 spring.cloud.nacos.discovery.naming-load-cache-at-start false 为true时会先从缓存文件读取服务列表，客户端向服务端请求到的服务列表会写入到缓存文件，等请求到服务列表和当前的列表不同时会重新写入到缓存文件中 元数据 spring.cloud.nacos.discovery.metadata {preserved.register.source=SPRING_CLOUD} 使用Map格式配置，用户可以根据自己的需要自定义一些和服务相关的元数据信息 是否启动注册 spring.cloud.nacos.discovery.register-enabled true 为true服务启动时会自动注册进注册中心 注册客户端的IP地址 spring.cloud.nacos.discovery.ip 分情况 优先级最高，如果没有配置此项，此时网卡名配了，看网卡名说明，否则用inetUtils.findFirstNonLoopbackHostInfo().getIpAddress()生成 网卡名 spring.cloud.nacos.discovery.network-interface 当IP未配置时，注册的IP为此网卡所对应的IP地址，如果此项也未配置，则默认取第一块网卡的地址 注册的客户端端口 spring.cloud.nacos.discovery.port ${server.port} 默认情况下不用配置，会自动探测 todo spring.cloud.nacos.discovery.secure false AccessKey spring.cloud.nacos.discovery.access-key 当要上阿里云时，阿里云上面的一个云账号名 SecretKey spring.cloud.nacos.discovery.secret-key 当要上阿里云时，阿里云上面的一个云账号密码 客户端心跳间隔 spring.cloud.nacos.discovery.heart-beat-interval 如果不填，服务端会指定默认为5s,客户端5s发送心跳 客户端心跳超时时间 spring.cloud.nacos.discovery.heart-beat-timeout 如果不填，服务端会指定默认为15s,超过15s没有发送心跳，服务端修改客户端状态为不健康，但不会剔除客户端 客户端ip删除时间 spring.cloud.nacos.discovery.ip-delete-timeout 如果不填，服务端会指定默认为30s,超过30s没有发送心跳，服务自动剔除客户端 实例是否启动 spring.cloud.nacos.discovery.instance-enabled true 实例是否启动 是否是非持久化实例 spring.cloud.nacos.discovery.ephemeral true true为非持久化实例，非持久化实例会定期上传心跳，告诉服务端他是健康的 服务端配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181#*************** Spring Boot Related Configurations ***************#### Default web context path:server.servlet.contextPath=/nacos### Default web server port:server.port=8848#*************** Network Related Configurations ***************#### If prefer hostname over ip for Nacos server addresses in cluster.conf:# nacos.inetutils.prefer-hostname-over-ip=false### Specify local server&#x27;s IP:# nacos.inetutils.ip-address=#*************** Config Module Related Configurations ***************#### If use MySQL as datasource:# spring.datasource.platform=mysql### Count of DB:# db.num=1### Connect URL of DB:# db.url.0=jdbc:mysql://127.0.0.1:3306/nacos?characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useUnicode=true&amp;useSSL=false&amp;serverTimezone=UTC# db.user.0=nacos# db.password.0=nacos### Connection pool configuration: hikariCPdb.pool.config.connectionTimeout=30000db.pool.config.validationTimeout=10000db.pool.config.maximumPoolSize=20db.pool.config.minimumIdle=2#*************** Naming Module Related Configurations ***************#### Data dispatch task execution period in milliseconds:### 同步实例信息到其他Server时间间隔# nacos.naming.distro.taskDispatchPeriod=200### Data count of batch sync task:# nacos.naming.distro.batchSyncKeyCount=1000### Retry delay in milliseconds if sync task failed:### 同步实例信息到其他Server任务失败后，任务重试间隔# nacos.naming.distro.syncRetryDelay=5000### If enable data warmup. If set to false, the server would accept request without local data preparation:# nacos.naming.data.warmup=true### If enable the instance auto expiration, kind like of health check of instance:### 为true超过实例删除超时时间会自动剔除实例，false只是会设置实例不健康，不会剔除实例# nacos.naming.expireInstance=truenacos.naming.empty-service.auto-clean=truenacos.naming.empty-service.clean.initial-delay-ms=50000nacos.naming.empty-service.clean.period-time-ms=30000#*************** CMDB Module Related Configurations ***************#### The interval to dump external CMDB in seconds:# nacos.cmdb.dumpTaskInterval=3600### The interval of polling data change event in seconds:# nacos.cmdb.eventTaskInterval=10### The interval of loading labels in seconds:# nacos.cmdb.labelTaskInterval=300### If turn on data loading task:# nacos.cmdb.loadDataAtStart=false#*************** Metrics Related Configurations ***************#### Metrics for prometheus#management.endpoints.web.exposure.include=*### Metrics for elastic searchmanagement.metrics.export.elastic.enabled=false#management.metrics.export.elastic.host=http://localhost:9200### Metrics for influxmanagement.metrics.export.influx.enabled=false#management.metrics.export.influx.db=springboot#management.metrics.export.influx.uri=http://localhost:8086#management.metrics.export.influx.auto-create-db=true#management.metrics.export.influx.consistency=one#management.metrics.export.influx.compressed=true#*************** Access Log Related Configurations ***************#### If turn on the access log:server.tomcat.accesslog.enabled=true### The access log pattern:server.tomcat.accesslog.pattern=%h %l %u %t &quot;%r&quot; %s %b %D %&#123;User-Agent&#125;i %&#123;Request-Source&#125;i### The directory of access log:server.tomcat.basedir=#*************** Access Control Related Configurations ***************#### If enable spring security, this option is deprecated in 1.2.0:#spring.security.enabled=false### The ignore urls of auth, is deprecated in 1.2.0:nacos.security.ignore.urls=/,/error,/**/*.css,/**/*.js,/**/*.html,/**/*.map,/**/*.svg,/**/*.png,/**/*.ico,/console-ui/public/**,/v1/auth/**,/v1/console/health/**,/actuator/**,/v1/console/server/**### The auth system to use, currently only &#x27;nacos&#x27; is supported:nacos.core.auth.system.type=nacos### If turn on auth system:### 开启安全功能nacos.core.auth.enabled=false### The token expiration in seconds:### token 过期时间nacos.core.auth.default.token.expire.seconds=18000### The default token:nacos.core.auth.default.token.secret.key=SecretKey012345678901234567890123456789012345678901234567890123456789### Turn on/off caching of auth information. By turning on this switch, the update of auth information would have a 15 seconds delay.nacos.core.auth.caching.enabled=true### Since 1.4.1, Turn on/off white auth for user-agent: nacos-server, only for upgrade from old version.### 这是bug，要设置为falsenacos.core.auth.enable.userAgentAuthWhite=true### Since 1.4.1, worked when nacos.core.auth.enabled=true and nacos.core.auth.enable.userAgentAuthWhite=false.### The two properties is the white list for auth and used by identity the request from other server.### nacos.core.auth.enabled 为true时要设置nacos.core.auth.server.identity.key=nacos.core.auth.server.identity.value=#*************** Istio Related Configurations ***************#### If turn on the MCP server:nacos.istio.mcp.server.enabled=false###*************** Add from 1.3.0 ***************####*************** Core Related Configurations ***************#### set the WorkerID manually# nacos.core.snowflake.worker-id=### Member-MetaData# nacos.core.member.meta.site=# nacos.core.member.meta.adweight=# nacos.core.member.meta.weight=### MemberLookup### Addressing pattern category, If set, the priority is highest# nacos.core.member.lookup.type=[file,address-server]## Set the cluster list with a configuration file or command-line argument# nacos.member.list=192.168.16.101:8847?raft_port=8807,192.168.16.101?raft_port=8808,192.168.16.101:8849?raft_port=8809## for AddressServerMemberLookup# Maximum number of retries to query the address server upon initialization# nacos.core.address-server.retry=5## Server domain name address of [address-server] mode# address.server.domain=jmenv.tbsite.net## Server port of [address-server] mode# address.server.port=8080## Request address of [address-server] mode# address.server.url=/nacos/serverlist#*************** JRaft Related Configurations ***************#### Sets the Raft cluster election timeout, default value is 5 second# nacos.core.protocol.raft.data.election_timeout_ms=5000### Sets the amount of time the Raft snapshot will execute periodically, default is 30 minute# nacos.core.protocol.raft.data.snapshot_interval_secs=30### raft internal worker threads# nacos.core.protocol.raft.data.core_thread_num=8### Number of threads required for raft business request processing# nacos.core.protocol.raft.data.cli_service_thread_num=4### raft linear read strategy. Safe linear reads are used by default, that is, the Leader tenure is confirmed by heartbeat# nacos.core.protocol.raft.data.read_index_type=ReadOnlySafe### rpc request timeout, default 5 seconds# nacos.core.protocol.raft.data.rpc_request_timeout_ms=5000 spring-cloud-starter-alibaba-nacos-discovery12345678com.alibaba.cloud.nacos.discovery.reactive.NacosReactiveDiscoveryClientConfiguration,\\com.alibaba.cloud.nacos.endpoint.NacosDiscoveryEndpointAutoConfiguration,\\com.alibaba.cloud.nacos.discovery.configclient.NacosConfigServerAutoConfiguration,\\com.alibaba.cloud.nacos.NacosServiceAutoConfigurationcom.alibaba.cloud.nacos.discovery.NacosDiscoveryAutoConfiguration,\\com.alibaba.cloud.nacos.registry.NacosServiceRegistryAutoConfiguration,\\com.alibaba.cloud.nacos.discovery.NacosDiscoveryClientConfiguration,\\com.alibaba.cloud.nacos.ribbon.RibbonNacosAutoConfiguration,\\ 重点关注这个 NacosAutoServiceRegistration 类 NacosAutoServiceRegistration 继承 AbstractAutoServiceRegistration，这个类实现 ApplicationListener在spring容器初始化时会发送 WebServerInitializedEvent 信号，那么以下的 onApplicationEvent 就会执行 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class NacosAutoServiceRegistration extends AbstractAutoServiceRegistration&lt;Registration&gt; &#123; &#125;public abstract class AbstractAutoServiceRegistration&lt;R extends Registration&gt; implements AutoServiceRegistration, ApplicationContextAware, ApplicationListener&lt;WebServerInitializedEvent&gt; &#123; public void onApplicationEvent(WebServerInitializedEvent event) &#123; this.bind(event); &#125; @Deprecated public void bind(WebServerInitializedEvent event) &#123; ApplicationContext context = event.getApplicationContext(); if (!(context instanceof ConfigurableWebServerApplicationContext) || !&quot;management&quot;.equals(((ConfigurableWebServerApplicationContext)context).getServerNamespace())) &#123; this.port.compareAndSet(0, event.getWebServer().getPort()); this.start(); &#125; &#125; public void start() &#123; if (!this.isEnabled()) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Discovery Lifecycle disabled. Not starting&quot;); &#125; &#125; else &#123; if (!this.running.get()) &#123; this.context.publishEvent(new InstancePreRegisteredEvent(this, this.getRegistration())); this.register(); if (this.shouldRegisterManagement()) &#123; this.registerManagement(); &#125; this.context.publishEvent(new InstanceRegisteredEvent(this, this.getConfiguration())); this.running.compareAndSet(false, true); &#125; &#125; &#125; protected void register() &#123; this.serviceRegistry.register(this.getRegistration()); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class NacosServiceRegistry implements ServiceRegistry&lt;Registration&gt; &#123; // 注册实例进 nacos @Override public void register(Registration registration) &#123; if (StringUtils.isEmpty(registration.getServiceId())) &#123; log.warn(&quot;No service to register for nacos client...&quot;); return; &#125; //获取 NamingService NamingService namingService = namingService(); String serviceId = registration.getServiceId(); String group = nacosDiscoveryProperties.getGroup(); Instance instance = getNacosInstanceFromRegistration(registration); try &#123; //这个方法注册进 nacos namingService.registerInstance(serviceId, group, instance); log.info(&quot;nacos registry, &#123;&#125; &#123;&#125; &#123;&#125;:&#123;&#125; register finished&quot;, group, serviceId, instance.getIp(), instance.getPort()); &#125; catch (Exception e) &#123; log.error(&quot;nacos registry, &#123;&#125; register failed...&#123;&#125;,&quot;, serviceId, registration.toString(), e); // rethrow a RuntimeException if the registration is failed. // issue : https://github.com/alibaba/spring-cloud-alibaba/issues/1132 rethrowRuntimeException(e); &#125; &#125; private NamingService namingService() &#123; return nacosServiceManager .getNamingService(nacosDiscoveryProperties.getNacosProperties()); &#125;&#125;public class NacosServiceManager &#123; private NamingService namingService; //获取 NamingService，如果没有就创建 public NamingService getNamingService(Properties properties) &#123; if (Objects.isNull(this.namingService)) &#123; buildNamingService(properties); &#125; return namingService; &#125; //双重检测 private NamingService buildNamingService(Properties properties) &#123; if (Objects.isNull(namingService)) &#123; synchronized (NacosServiceManager.class) &#123; if (Objects.isNull(namingService)) &#123; namingService = createNewNamingService(properties); &#125; &#125; &#125; return namingService; &#125; private NamingService createNewNamingService(Properties properties) &#123; try &#123; //创建 createNamingService return createNamingService(properties); &#125; catch (NacosException e) &#123; throw new RuntimeException(e); &#125; &#125;&#125;","categories":[{"name":"spring cloud alibaba","slug":"spring-cloud-alibaba","permalink":"https://xxnjdg.github.io/categories/spring-cloud-alibaba/"}],"tags":[{"name":"nacos","slug":"nacos","permalink":"https://xxnjdg.github.io/tags/nacos/"}]},{"title":"sentinel学习使用","slug":"sentinel-1","date":"2021-01-24T16:18:26.000Z","updated":"2021-01-25T10:18:57.686Z","comments":true,"path":"2021/01/25/sentinel-1/","link":"","permalink":"https://xxnjdg.github.io/2021/01/25/sentinel-1/","excerpt":"","text":"参考2020新版视频含SpringCloud Hoxton和SpringCloud alibaba 下载和运行https://github.com/alibaba/Sentinel/releases 如果下载慢，可复制下面链接到迅雷等下载器下载 https://github.com.cnpmjs.org/alibaba/Sentinel/releases/download/v1.8.0/sentinel-dashboard-1.8.0.jar 执行下面命令 1java -jar sentinel-dashboard-1.8.0.jar 打开浏览器输入默认地址 http://localhost:8080/ 账号和密码默认都是 sentinel 登陆后界面 因为 Sentinel 控制台是基于 Spring Boot 实现，所以我们可以通过启动时的命令行参数，自定义配置 –server.port：自定义服务器端口。默认为 8080 端口。 源码解析Sentinel 的数据统计StatisticNode 统计节点保留三种实时统计指标： 秒级统计 分钟级统计 线程数 Sentinel使用滑动窗口实时记录和统计资源统计信息。 {@link ArrayMetric}后面的滑动窗口基础结构是{@code LeapArray}。","categories":[{"name":"spring cloud alibaba","slug":"spring-cloud-alibaba","permalink":"https://xxnjdg.github.io/categories/spring-cloud-alibaba/"}],"tags":[{"name":"sentinel","slug":"sentinel","permalink":"https://xxnjdg.github.io/tags/sentinel/"}]},{"title":"the-art-of-java-concurrency-programming","slug":"the-art-of-java-concurrency-programming","date":"2020-12-26T16:48:37.000Z","updated":"2020-12-27T07:59:11.377Z","comments":true,"path":"2020/12/27/the-art-of-java-concurrency-programming/","link":"","permalink":"https://xxnjdg.github.io/2020/12/27/the-art-of-java-concurrency-programming/","excerpt":"","text":"第1章 并发编程的挑战上下文切换的问题 线程有创建和上下文切换的开销，如何解决？ 减少上下文切换的方法有无锁并发编程、CAS算法、使用最少线程和使用协程。 死锁的问题 现在我们介绍避免死锁的几个常见方法。·避免一个线程同时获取多个锁。 ·避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 ·尝试使用定时锁，使用lock.tryLock（timeout）来替代使用内部锁机制。 ·对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。 以及受限于硬件和软件的资源限制问题 对于硬件资源限制，可以考虑使用集群并行执行程序。既然单机的资源有限制，那么就让程序在多机上运行 多使用JDK并发包提供的并发容器和工具类来解决并发问题，因为这些类都已经通过了充分的测试和优化，均可解决了本章提到的几个挑战 第2章 Java并发机制的底层实现原理2.1 volatile的应用有volatile变量修饰的共享变量进行写操作的时候会多出第二行汇编代码，Lock前缀的指令在多核处理器下会引发了两件事情[1]。1）将当前处理器缓存行的数据写回到系统内存。2）这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 2.2 synchronized的实现原理与应用先来看下利用synchronized实现同步的基础：Java中的每一个对象都可以作为锁。具体表现为以下3种形式。·对于普通同步方法，锁是当前实例对象。·对于静态同步方法，锁是当前类的Class对象。·对于同步方法块，锁是Synchonized括号里配置的对象 JVM基于进入和退出Monitor对象来实现方法同步和代码块同步，但两者的实现细节不一样。代码块同步是使用monitorenter和monitorexit指令实现的，而方法同步是使用另外一种方式实现的，细节在JVM规范里并没有详细说明。但是，方法的同步同样可以使用这两个指令来实现 monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。任何对象都有一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。 Java SE 1.6为了减少获得锁和释放锁带来的性能消耗，引入了“偏向锁”和“轻量级锁”，在Java SE 1.6中，锁一共有4种状态，级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率，下文会详细分析。 2.3 原子操作的实现原理在Java中可以通过锁和循环CAS的方式来实现原子操作。JVM中的CAS操作正是利用了处理器提供的CMPXCHG指令实现的CAS实现原子操作的三大问题ABA问题，循环时间长开销大，以及只能保证一个共享变量的原子操作。 第3章 Java内存模型Java的并发采用的是共享内存模型","categories":[{"name":"并发编程","slug":"并发编程","permalink":"https://xxnjdg.github.io/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"并发编程","slug":"并发编程","permalink":"https://xxnjdg.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-12-26T06:56:30.821Z","updated":"2021-02-14T10:38:27.585Z","comments":true,"path":"2020/12/26/hello-world/","link":"","permalink":"https://xxnjdg.github.io/2020/12/26/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 参考使用主题 Butterfly https://www.cnblogs.com/thanksblog/p/12900165.htmlhttps://blog.csdn.net/xjm850552586/article/details/84101345 文件头部 123456789101112---title: testdate: 2020-12-26 16:03:32categories: - 分类1tags:- 标签1cover: /2020/12/26/test/1.png # 封面图sticky: 1 # 顶置---![ss](test/1.png)","categories":[],"tags":[]}],"categories":[{"name":"spring cloud alibaba","slug":"spring-cloud-alibaba","permalink":"https://xxnjdg.github.io/categories/spring-cloud-alibaba/"},{"name":"cache","slug":"cache","permalink":"https://xxnjdg.github.io/categories/cache/"},{"name":"mq","slug":"mq","permalink":"https://xxnjdg.github.io/categories/mq/"},{"name":"spring cloud","slug":"spring-cloud","permalink":"https://xxnjdg.github.io/categories/spring-cloud/"},{"name":"并发编程","slug":"并发编程","permalink":"https://xxnjdg.github.io/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://xxnjdg.github.io/tags/elasticsearch/"},{"name":"redis","slug":"redis","permalink":"https://xxnjdg.github.io/tags/redis/"},{"name":"rocketmq","slug":"rocketmq","permalink":"https://xxnjdg.github.io/tags/rocketmq/"},{"name":"nacos","slug":"nacos","permalink":"https://xxnjdg.github.io/tags/nacos/"},{"name":"spring-cloud-gateway","slug":"spring-cloud-gateway","permalink":"https://xxnjdg.github.io/tags/spring-cloud-gateway/"},{"name":"sentinel","slug":"sentinel","permalink":"https://xxnjdg.github.io/tags/sentinel/"},{"name":"并发编程","slug":"并发编程","permalink":"https://xxnjdg.github.io/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"}]}